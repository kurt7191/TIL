# 빅데이터 분석 6주차 강의



### 인공신경망



인공신경망 단점 :  black boxes = 설명력이 떨어진다.



##### 뉴런



뉴런끼리는 화학적 전기를 통해서 신호를 보낸다.



##### 인공신경망



인공신경망의 뉴런은 노드(node) 라고 사용함

활성화 함수

- 결합함수
  - 내가받은 신호를 결합해서 하나의 값으로 만드는 함수
- 전환함수 (S자 형태)
  - 만들어진 값을 그냥 보내는게 아니라 S자 형태의 커브로 틀어서 보내는 게 전환함수 (**비선형적인 반응을 가능하게 함)
  - 임곗값(THRESHOLD) 근처의 입력값에 대해서는 작은 입력 차이에도 큰 출력값의 차이가 보인다. 하지만 임곗값과 거리가 먼 입력값에 대해서는 큰 입력 차이에도 출력 값이 별반 차이가 나지 않는다.



활성화 함수 구조



INPUT 입력값들을 받으면 INPUT 입력값들이 WEIGHTED SUM 이 된다.

그것이 전환함수를 통해서 출력으로 바뀌어서 결과값이 나온다.



##### MULTI LAYER PERCEPTRON



입력층, 은닉층, 출력층으로 이루어짐



입력변수들이 있으면 각 값의 범위가 RANGE 가 다 다를 것임. 그래서 통일 해줘야 해서 MIN_MAX_SCALING 으로 전환한다.

아크에 모두 WEIGHT가 있다. 따라서 입력이 들어가면 WEIGHT 부여받고 SUM 을 하게 된다. (WEIGHTED SUM) 



이 값을 TRANSFORM 해서 출력이 만들어진다. 만들어진 출력이 또다시 INPUT 이 되서 입력값으로 들어간다.



마지막으로 나온 값을 전환 변화를 통해 타겟값을 예측한다.



##### 피드 포워드(FEED-FORWARD) 신경망



인공신경망의 구조를 피드 포워드라고 한다. 

피드 포워드의 의미는 신호가 입력->은닉->출력 으로 한 방향으로 흘러간다는 의미.

전통적인 인공신경망은 피드 포워드 구조이다.



- 입력층

  - 각 노드는 보통 -1 ~1 사이 할당된 필드에 연결되어 있음
  - 범주형 변수는 다수의 입력 노드에 연결가능

- 은닉층

  - 은닉층이 많을수록 또는 은닉층의 노드의 개수가 많을수록 복잡한 패턴을 인식할 수 있다. 그러나 과적합의 위험이 있다.

- 출력층

  - 은닉층의 모든 노드들과 연결이 되어 있다.

  

##### 피드 포워드의 구조



1. 입력과 출력만으로 이루어진 구조

   - 은닉층이 없으면 logistic 회귀분석과 동일한 구조

2. 입력, 은닉, 출력

3. 입력, 더 많은 은닉노드, 출력 1

   - 은닉노드가 많아지면 복잡한 패턴을 더 학습할 수 있다. 그러나 과적합의 위험도 가지게 된다.

4. 입력, 은닉, 출력이 3개

   - 범주형 종속변수를 예측을 할 때, 타겟 데이터의 클래스가 3개이면 3개의 출력노드를 정의한다. 각 노드들이 각 클래스에 속할 확률을 산출할 수 있도록 학습을 시킨다.

   

   

   

   ### 신경망의 학습

   

   훈련 데이터를 이용하는 신경망에서의 학습은 ***각 아크에 있는 weight들을 정하는 과정***임.

   

   ##### 과정

   학습 초기에는 weight 들을 랜덤하게 부여한다.

   만일 첫 번째 학습을 통해서 weight 을 정했다면 그 weight 값을 first generation 이라고 부른다. 두 번째는 second generation.

   각 반복이 끝날 때마다 각 generation 의 weight set 이 만들어진다.

   만들어진 weight set을 통해서 계속해서 훈련을 시킨다. 즉 weight 를 수정한다.

   

   중요한점은 맨 마지막에 만들어진 weight set이 가장 좋은 성능을 내는 weight 셋이 아니라 중간쯤에 있는 weight set 이 더 좋은 성능을 내는 weight set이다. 그 이윤 과적합 때문

   

   ##### 목표

   

   가능하면 훈련데이터 셋의 종속 변수를 잘 맞출 수 있는 weight set 을 세우는 게 목표임

   

   신경망을 학습할 때 쓰는 알고리즘 Back propagation

   

   #### Back propagation

   

   weight 의 조정을 출력 쪽부터 역순으로 내려온다는 뜻

   

   세 개의 단계로 이루어짐

   

   1. 신경망에 데이터를 넣어서 훈련된 예시를 넣어서 기존의 weight 를 가지고 출력을 계산한다.
   2. 그 후 우리는 정답을 알고 있기 때문에 실제 데이터 값과 예측값과의 오차를 계산한다.
   3. 이 오차를 줄이기 위해서 weight 를 수정한다.

   

   이러한 알고리즘을 ***일반화 델타 규칙***이라고 부름

   일반화 델타 규칙은 미분(기울기)을 이용한다.

   

   일반화 델타 규칙은 중요한 파라미터 두 개를 가진다. 

   1. 모멘텀(Momentum) => 기존의 움직이던 방향을 고려함
      - weight를 조정하려던 방향, 그것과 새로운 훈련데이터를 가지고 구한 새로운 weight 방향이 존재.
      - 기존의 방향과 새로운 방향을 어느정도 반영할거냐를 정하는게 모멘텀
      - 즉, 기존의 방향을 어느 정도 유지를 할 것인가?(등산 예시, 경사가 있는 방향으로 1m 가다가 새로운 경사가 나왔을 때, 방향을 어떻게 할 것인가?)
   2. 학습률(learning_rate)
      - 정해진 방향으로 얼마나 이동을 할 것인가? 
      - 처음에는 크게 가다가 학습이 진행될수록 조금씩

   

   ##### optimizer

   

   AdaGrad : 확습률을 줄여줌

   Adam : 모멘텀 + AdaGrad

   

   

   미분을 이용한 학습의 문제 중 하나는 국소 최적값으로 수렴하는 것, 하짐나 최고의 성능이 아닐 수 있음. 즉, 최적의 weight set 으로만 이루어진게 아닐 수 있음

   

   

   ##### 미니배치

   

   훈련 데이터가 많은 경우 weight를 조정하는데 시간이 많이 걸린다.

   전체 데이터 중에 일부를 골라서 그것에 대해서 loss function 값을 구하고 그다음에 weight를 조정한다. 

   그리고 또 일부를 골라서 같은 작업을 반복

   

   10000개의 데이터 중에서 100개의 데이터를 미니배치 방식으로 돌리면 100번 돌려야 10000개의 데이터셋 모두 사용. 이때 값 100을 1 epoch라고 부름

   

   이후 강의 jupyter로

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   