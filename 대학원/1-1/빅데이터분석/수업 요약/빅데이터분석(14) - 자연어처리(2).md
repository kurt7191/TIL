# 빅데이터분석(14) - 자연어처리(2)



> 토픽 모델링 개요와 실습
>
> 영어 문장들에 대한 토픽 모델링 예제
>
> 국어 문장들에 대한 토픽 모델링 예제



## 토픽 모델링



문서 집합에서 주제를 찾아내는 기술

특정 주제와 관련된 문서에서는 특정 단어가 자주 등장할 것이라는 직관에 기반한다.



예를 들어, 경제가 주제이면, 경제 부분에서 자주 출현하는 단어들이 등장할 것이라는 생각



토픽 모델링에 자주 사용하는 기법

1. 잠재 의미 분석(Latent Semantic Analysis) => LSA 는 오래된 기법
2. 잠재 디리클레 할당(Latent Dirichlet Allocation) -> LD A 는 최근 사용 기법







<HR>



### LSA



DTM 을 만들어봤는데 DTM 을 분해를 하면 3개로 나눌 수 있다.

1. 단어 - 토픽 행렬
2. 토픽 - 중요도 행렬
3. 토픽 - 문서 행렬



이렇게 3 개로 나누는 방법이 LSA



<HR>



### LDA

### 

1. 사용자가 토픽의 개수를 사전에 지정함
2. 모든 단어들을 토픽 중 하나에 할당
3. 모든 문서의 모든 단어에 대해 단어 W의 가정에 의거, P(t|d), p(w|t)에 따라 토픽을 재할당하는 것을 반복함 (그 문서일 때, 그 토픽일 확률, 그 토픽일 때 그 단어가 나올 확률을 이용해서 토픽을 재할당)
   - 가정 : 자신만 잘못된 토픽에 할당되어 있고 다른 모든 단어는 올바른 토픽에 할당되어 있다.
4. 이를 변화가 없을 때까지 반복해서 토픽에 단어들을 할당하는 작업을 진행한다.



##### 문서 - 단어 - 토픽



문서는 토픽의 혼합체(mixture of topics)

=> 하나의 토픽으로만 이루어질수도 있지만 일반적으로 하나의 문서는 여러 개의 토픽의 혼합체임



토픽은 단어의 혼합체(mixture of words)

=> 토픽별로 나오는 단어들이 다를 것이다

=> 그렇지만 하나의 단어는 여러 토픽에 중복될 수 있다.





하나의 문서에 어떤 토픽들이 출현하는지, 어떤 토픽이 어느 정도 비율을 차지하는지 계산할 수 있다.

(하나의 문서에 여러 개의 토픽이 출현할 수 있음. 그러면 각각의 토픽은 이 문서에서 어느 정도 비율?

토픽별로 나오는 단어들이 다를텐데, 문서에서 나오는 단어들이 어떤 토픽에 많이 할당되어있는지 계산하면 각 토픽의 크기를 계산할 수 있고, 다음으로 문서에서 토픽이 차지하는 비중을 계산할 수 있다.



이 뿐만 아니라, 각 토픽별 어떤 단어의 비중이 높은지도 계산할 수 있다.)



<hr>

### 토픽 모델링의 활용



1. 연구 동향을 분석할 때 토픽 모델링을 사용할 수 있다.
2. 제품 간의 비교 가능 (리뷰를 이용하여) -> 각 브랜드별 어떤 토픽들이 다루어지는지 분석



<hr>

### 실제 파이썬에서 토픽 모델링 활용



```python
# -*- coding: utf-8 -*-
"""
Created on Thu May 13 18:21:03 2021

@author: USER
"""

import pandas as pd

news_df = pd.read_csv('C:\\study\\workspace_python\\pdsample\\datasets\\mydata\\naver_news.csv')



from pykospacing import spacing

news_df['title_n']=news_df['title'].apply(lambda x: spacing(x))


# 숫자 제거

import re

p=re.compile("[0-9]+")
news_df['title_n']=news_df['title_n'].apply(lambda x: p.sub(" ",x))


# 문장부호, 특수문자 제거

p=re.compile("\W+")
news_df['title_n']=news_df['title_n'].apply(lambda x: p.sub(" ",x))



# 명사만 추출 
from konlpy.tag import Okt  
okt=Okt()  

news_df['title_n']=news_df['title_n'].apply(lambda x: okt.nouns(x))

# Stop word 제거

def rem_n_stopwords(sentence):
    stopwords=['기사','동영상','조','전','만','중','월','명','위', '억','억원','액','연','년','이','거','것','저','또','도','나','더','그','일','말','제','수','안','줄','게','좀','듯','님','점','등','고']
    sent_stop=[]
    for token in sentence:
        if token not in stopwords:
            sent_stop.append(token)
    return(sent_stop)

news_df['title_n']=news_df['title_n'].apply(lambda x: rem_n_stopwords(x))


news_df.to_csv('naver_news_result.csv',index=False, encoding='utf-8-sig')


# Word Frequency 

total_word=[]
for i in range(len(news_df)):
    total_word.extend(news_df.loc[i,"title_n"])

pd.Series(total_word).value_counts().head(15)


# 한글 폰트 사용
from matplotlib import font_manager, rc

font_name = font_manager.FontProperties(fname="c:/Windows/Fonts/malgun.ttf").get_name()
rc('font', family=font_name)

pd.Series(total_word).value_counts().head(15).plot(kind='bar')
pd.Series(total_word).value_counts().head(15).plot(kind='barh')
pd.Series(total_word).value_counts().head(15).plot(kind='barh').invert_yaxis()

# word cloud

# pip install wordcloud
from wordcloud import WordCloud
import matplotlib.pyplot as plt
font_path='c:/Windows/Fonts/'
cloud = WordCloud(font_path=font_path+'NanumGothic.ttf',width=800, height=600).generate(" ".join(total_word))
plt.figure(figsize=(20, 15))
plt.imshow(cloud)
plt.axis('off')

# Topic Modeling
from gensim import corpora
dictionary = corpora.Dictionary(news_df['title_n'])
corpus = [dictionary.doc2bow(text) for text in news_df['title_n']]
print(corpus[1]) # 수행된 결과에서 두번째 뉴스 출력. 첫번째 문서의 인덱스는 0
print(dictionary[1])
len(dictionary)

import gensim
NUM_TOPICS = 10 #10개의 토픽, k=10
ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = NUM_TOPICS, id2word=dictionary, passes=15)
topics = ldamodel.print_topics(num_words=5)
for topic in topics:
    print(topic)
print(ldamodel.print_topics())

# 문서별 토픽보기
for i, topic_list in enumerate(ldamodel[corpus]):
    if i==5:
        break
    print(i,'번째 문서의 topic 비율은',topic_list)

doc=ldamodel[corpus][0]
print(doc)
doc = sorted(doc, key=lambda x: (x[1]), reverse=True)
print(doc)

def make_topictable_per_doc(ldamodel,corpus):
    topic_table=pd.DataFrame()
    for i, topic_list in enumerate(ldamodel[corpus]):
        doc=topic_list
        doc=sorted(doc,key=lambda x: (x[1]),reverse=True)
        for j, (topic_num, prop_topic) in enumerate(doc): #  몇 번 토픽인지와 비중을 나눠서 저장한다.
            if j == 0:  # 정렬을 한 상태이므로 가장 앞에 있는 것이 가장 비중이 높은 토픽
                topic_table = topic_table.append(pd.Series([int(topic_num), round(prop_topic,4), topic_list]), ignore_index=True)
                # 가장 비중이 높은 토픽과, 가장 비중이 높은 토픽의 비중과, 전체 토픽의 비중을 저장한다.
            else:
                break
    return(topic_table)
    
topictable = make_topictable_per_doc(ldamodel, corpus)
topictable = topictable.reset_index() # 문서 번호을 의미하는 열(column)로 사용하기 위해서 인덱스 열을 하나 더 만든다.
topictable.columns = ['문서 번호', '가장 비중이 높은 토픽', '가장 높은 토픽의 비중', '각 토픽의 비중']
topictable[:10]

## sklearn에서 LDA
 
from sklearn.feature_extraction.text import CountVectorizer

# the vectorizer object will be used to transform text to vector form
vectorizer = CountVectorizer(max_df=0.9, min_df=2)

# apply transformation
news_df['title_n_str']=[' '.join(token) for token in news_df['title_n']]
tf = vectorizer.fit_transform(news_df['title_n_str']).toarray()

# tf_feature_names tells us what word each column in the matric represents
tf_feature_names = vectorizer.get_feature_names()


from sklearn.decomposition import LatentDirichletAllocation

number_of_topics = 10

model = LatentDirichletAllocation(n_components=number_of_topics, random_state=0)

model.fit(tf)

def display_topics(model, feature_names, no_top_words):
    topic_dict = {}
    for topic_idx, topic in enumerate(model.components_):
        topic_dict["Topic %d words" % (topic_idx)]= ['{}'.format(feature_names[i])
                        for i in topic.argsort()[:-no_top_words - 1:-1]]
        topic_dict["Topic %d weights" % (topic_idx)]= ['{:.1f}'.format(topic[i])
                        for i in topic.argsort()[:-no_top_words - 1:-1]]
    return pd.DataFrame(topic_dict)

no_top_words = 10
topic_dictionary=display_topics(model, tf_feature_names, no_top_words)


```



<hr>


















