# 데이터 크롤링(2)



### 크롤링 허용 여부 확인하기



<크롤링 주소>/robots.txt 를 입력

ex) www.naver.com/robots.txt



robots.txt 파일이 검색이 안되면 크롤링이 허락되어 있는 경우며

만일 파일이 검색이 되면 아래의 표를 참조하여 허용여부를 판별해야 한다.

| 표시            | 허용여부                  |
| --------------- | ------------------------- |
| User-agent:*    | 모든 접근 허용            |
| Disallow:       |                           |
| 또는            |                           |
| User-agent:*    |                           |
| Allow:/         |                           |
| User-agent:*    | 모든 접근 금지            |
| Disallow:/      |                           |
| User-agent:*    | 특정 디렉토리만 접근 금지 |
| Disallow:/user/ |                           |



### children



어떤 태그의 모든 자식 태그들을 확인할 때 사용.

반환이 iterator



```python
#children

print(bs_obj.ul.children)
a = bs_obj.ul.children
for i in a:
    print(i)
    
print('=========================')
for sub in bs_obj.find_all('ul')[1].children:
    print(sub)
print('=========================')
```



이때 bs_obj.ul은 find와 같은 효과를 가지기 때문에 한 개만 가져온다. 즉 첫 번째 ul 만을 가져온다. 따라서 첫 번째 ul 의 자식 태그들만 가져오기 대문에 find_all 로 바꿔서 해주면 다른 ul 의 자식 태그들도 불러올 수 있다.



### next_siblings

### previous_siblings



반환이 generator

따라서 값들을 next 함수를 이용해서 가져와야 한다.



```python
print(bs_obj.find('a').next_siblings)

ele = bs_obj.find('a').next_siblings
next(ele)

print(bs_obj.find('a').previous_siblings)
ele = bs_obj.find('a').previous_siblings
next(ele)
```



generator는 반복문 x, `next()` 함수로 값 가져오기.



### 정규식을 이용한 데이터 접근



```python
import re

print(bs_obj.find('a',{'href':'https://www.naver.com/'}))

print(bs_obj.find('a',{'href': re.compile('.+com')}))
print(bs_obj.find('a',{'href':re.compile('.*naver.*')}))
print(bs_obj.find('a',{'href':re.compile('naver')}))
```



find 함수에서 특정 속성 값을 가지고 있는 특정 태그를 가져올 때 딕셔너리를 사용함을 살펴봤다.

이 딕셔너리의 value 값에 파이썬 정규식을 이용해서 특정 속성을 가지고 있는 태그를 꺼낼 수 있다.



### 람다식을 이용한 데이터 접근



```python
print(bs_obj.find(href=lambda val : val))
print(bs_obj.find(href=lambda val : val and 'naver' in val))
print('===========================================')

print(bs_obj.find(class_ = lambda val : val))
print(bs_obj.find(class_ = lambda val : val and 'sns' in val))
```



### GET 방식 request



```python
def getDownload( url, param = None, retries = 3 ):
    resp = None
    try:
        resp = requests.get( url, params = param )
        resp.raise_for_status()
    except requests.exceptions.HTTPError as e:
        if 500 <= resp.status_code < 600 and retries > 0:
            print( 'Retries : {0}'.format( retries ) )
            return getDownload( url, param, retries - 1 )
        else:
            print( resp.status_code )
            print( resp.reason )
            print( resp.request.headers )
    return resp

```



### class 속성의 값이 2개 이상일 때



```python
import requests
import re
header = {'user-agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}
url = requests.get('https://news.naver.com/', headers = header)
print(url)
html = bs4.BeautifulSoup(url.text,'lxml')
# print(html)

newsnow = html.find('ul',{'class':'mlist2 no_bg'})
# print(newsnow)

for new_li in newsnow.find_all('li'):
    
    new_strong = new_li.find('strong')
    print(re.findall('[a-z가-힣]+', new_strong.text))

```



406 에러가 뜨기 때문에 앞서 배운 user-agent를 복사해서 가져온다.

그리고 특정 속성을 가지고 있는 특정 태그를 가져오기 위해서 딕셔너리를 이용한다.

그리고 가져온 ul 에서 모든 li 정보를 가져오고 싶기 때문에 find_all 을 사용한다.



html 을 살펴보면 <strong> 태그에 text가 적혀있음을 확인할 수 있다.

따라서 strong 을 find 해주면 strong 정보를 모두 가져온다.

이때 한글과 영어만 가져오기 위해서 정규식을 사용한다.

정규식을 compile 을 먼저 해주고 단어를 치는 경우도 있지만, 

함수로 한번에 처리할 수 있다. new_strong 의 text 부분을 가져오고 싶기 때문에 

new_strong.text 을 해주고 정규식을 적용시켜준다.



### 