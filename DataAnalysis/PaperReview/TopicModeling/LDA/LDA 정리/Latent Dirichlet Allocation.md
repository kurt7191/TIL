# Latent Dirichlet Allocation from YouTube



## LDA PART

- LDA 2 PART
  1. LDA 가 사람들이 문서를 어떤 방식으로 생성할 것인가에 가정하는 모델 부분
  2. 그 모델에 의해서 실질적으로 토픽들에 해당하는 그 토픽 모델링의 파라미터를 추정하는 추론 부분



## LDA 의 배경



LDA의 첫 번째 배경

1. Documents exhibit multiple topics



![스크린샷 2022-03-03 오후 8.08.59](/Users/jang-eunjun/Desktop/TIL/DataAnalysis/PaperReview/TopicModeling/LDA/스크린샷 2022-03-03 오후 8.08.59.png)



1. 개별적인 문서는 여러 개의 토픽들을 포함하고 있다.(Documents exhibit multiple topics)

   1. **각각의 토픽들은 단어들의 분포(distribution) 이다.**

      즉 각각의 토픽들에 해당하는 단어 사용 빈도수가 존재할 것이고, 이를 통해서 내림차순으로 정렬할 수 있을 것. 

      이렇게 되면 각각의 토픽마다 사용하는 단어의 분포가 다르게 되고 토픽별로 단어 사용 분포를 표현할 수 있을 것이다.

   2. 각각의 doc들은 corpus-wide-topics 의 mixture 다. 문서가 다르다고 해서 문서 집합의 corpus에서의 토픽의 비중(단어들의 비중)은 변하지 않는다. => 전체 문서에 대해서 봤더니 ~% ~% ~%의 토픽으로 섞여있다고 생각할 수 있다.

   3. 단어들은 각 토픽에서부터 sampling 해서 생성이 된다.

      - 예를 들어서 토픽 A,B,C 가 존재한다고 가정.
      - 토픽 A,B,C 가 문서에 나올 확률을 30, 50, 20으로 가정.
      - 백 개의 단어가 존재하는 문서를 생성한다고 했을 때, A토픽이 30개 단어 차지, B가 50개 차지, C가 20개 차지한다고 생각한다.
      - A에서 뽑아야지! 했으면 A토픽의 단어 분포(확률)를 통해서 단어를 뽑아온다.(sampling) 즉 토픽의 단어 확률 분포에 근거해서 단어를 뽑아온다.

   

   

   > 여기까지 LDA 모델이 문서를 어떤 방식으로 생성할지에 대해서 살펴봤다.
   >
   > 다음으로 LDA 모델이 직면헌 issue 에 대해서 살펴볼 것.

   

   <hr>
   
   

   ![lda_issue](/Users/jang-eunjun/Desktop/TIL/DataAnalysis/PaperReview/TopicModeling/LDA/lda_issue.png)

   LDA 첫 번째 가정에 대한 **issue** 그리고 issue 를 해결하기 위한 **inference** : 

   1. 우리는 실질적으로 단어들이 무엇인지에 대한 documents에 대해서만 알고 있다.

   2. 토픽들에 해당하는 단어들의 분포가 어떻게 되느냐를 알 수가 없다.
   3. 문서내에서 토픽들의 비중이 어떻게 되느냐를 알 수가 없다.
   4. 또한 첫 번째 단어, 두 번째 단어, 세 번째 단어 등등, n번째 단어에 대해서 어떤 토픽의 단어 분포를 통해서 뽑아낼 것인지 모른다.

   

   > 정리하면,
   >
   > Q1 - Topic 내의 단어 분포를 어떻게 알 수 있는가?
   >
   > Q2 - Documents 내의 Topic 분포를 어떻게 알 수 있는가?
   >
   > Q3 - word를 생성할 때, n번째 단어는 어떤 Topic에 근거해서 생성했는가?
   >
   > 
   >
   > 이 세 개의 Question 에 대해서 알 수가 없다.

   

   이 세 가지에 대한 질문을 모르고 있기 때문 이 3가지 질문의 답을 찾아 나가는 게 LDA 의 inference 부분이다.

   

   **LDA inference purpose:**

   

   Documents 가 주어졌을 때 구해야 하는 것들

   1. Topic 의 단어 비율(분포) (전체 corpus level에서의 토픽에 해당하는 단어들의 비중)
   2. doc이 가지고 있는 토픽의 비중 (document level에서 토픽의 비중)
   3. 각각의 단어들이 어떤 토픽에서 나왔는지 (단어 level에서 그 단어가 어느 토픽에서 뽑혔는지)

   

   >  P(topics, proportion, assignments | documents)

   

   이게 바로 LDA의 추론 목적이다.

   

   이로써 LDA의 큰 두 부분인 1. LDA 가 문서를 어떻게 생성할 것인가? 2.LDA 가 파라미터를 추론한다는 게 무엇이고 추론의 목적은 무엇인가?

   에 대해서 대략적으로 살펴봤다.

   

   <hr>

   ![다운로드](/Users/jang-eunjun/Desktop/TIL/DataAnalysis/PaperReview/TopicModeling/LDA/다운로드.png)

   1. assumtion 이라는 가정을 encoding
   2. 다시 결합 확률 분포로 factorize
   3. data를 통해서 알고리즘을 추정하는 과정

   

   굉장히 중요한 그림.

   뒤에서 더 살펴보지만 간단하게 일단 살펴보자.

   

   1. plate(주황색 네모,D) : D는 documents 를 의미하고 plate는 반복을 의미.

      - documents 마다 plate 안에 있는 내용을 반복하겠다.
      -  N(D 안에 있는) : N은 word를 의미하고 마찬가지로 반복을 의미.
      - 그림상 D안에 N 이 있기 때문에, 이중 포문을 의미.
      - D와 N을 종합하면 document 마다 반복을 할텐데, 그 문서 안에서 단어마다 그 과정이 반복이 되는 것.
      - K(D 바깥에 있는) : K는 토픽의 수이고 마찬가지로 반복을 의미. 토픽마다 k번을 반복한다는 의미.

   2. 하이퍼 파라미터𝛂, 𝛃. (𝛂 와 𝛃가 어떤 분포를 따르냐면 Dirichlet distribution 을 따른다.)

      - 그래서 Latent dirichlet allocation 이다.
      - 토픽이 잠재되어 있을 것이고, 그 잠재된 토픽은 dirichlet 분포로부터 추출이 된다.(나왔을 것이다.)
      - 단어들은 이 토픽들에 할당이 될 것이다.

   3. 𝜽_d : 각각의 document별로 토픽이 얼마의 비중으로 섞여있는가를 알려주는 값. 즉, 각각의 documents의 토픽의 비중

      

      - |            | Topic1 | Topic2 | Topic3 |
        | :--------: | :----: | :----: | :----: |
        | Documennt1 |  0.3   |  0.5   |  0.2   |
        | Document2  |  0.1   |  0.1   |  0.8   |

        

   4. 𝜙_k : 토픽들에 대한 단어들의 distribution.

      - k번째 토픽의 단어들의 비중 혹은 분포.  

      - |      | Topic1 | Topic2 | Topic3 | Topic_K |
        | :--: | ------ | ------ | ------ | ------- |
        |  W1  | 0.1    | ..     | ..     | ..      |
        |  W2  | 0.3    | ..     | ..     | ..      |
        |  W3  | 0.4    | ..     | ..     | ..      |
        |  W4  | 0.1    | ..     | ..     | ..      |
        | W_N  | 0.1    | ..     | ..     | ..      |

        여기서 k를 1로 하면 첫 번째 토픽의 단어 분포를 의미한다.

   5. Z_d,n : (samll) d번째 문서에서 (small) n번째 단어는 어느 토픽에서 추출되었는가?

   6. 결국은 관측하는 것은 W_d,n : 실제d번째 n번째 단어가 뭐냐를 관측하는 것.

     

   

   > 이를 정리하자면

   

   Node(그림에서 원을 의미) 는 확률 변수를 의미

   edge는 dependency 를 의미

   

   1. 특정 document 가 토픽을 얼마의 비중으로 가지고 있느냐?(𝜽_d) : 하이퍼 파라미터 𝛂 에 영향을 받는다. ->
   2. 특정한 document의 토픽의 비중(𝜽_d) 은 해당하는 특정 개별 문서의 몇 번째 단어는 무슨 토픽에서 뽑혔는지에 (Z_d,n) 영향을 준다. -> 
   3. 그 단어는 어떤 토픽에서 왔느냐(Z_d,n), 그 토픽에서의 단어 비중(𝜙_k) 두 가지를 가지고 word를 선택하게 된다.

   

   >  추가로 : 각각의 토픽들에 대해서 단어들이 얼마만큼의 비중을 가지고 있는가(𝜙_k) 는 𝛃 파라미터에 의해서 영향을 받는다.

   

   

   ##### 위의 과정을 결합 확률 분포로 살펴보기

   이를 전부 결합 확률 분포로 살펴보면 아래 그림과 같다.

   

   ![스크린샷 2022-03-03 오후 9.24.58](/Users/jang-eunjun/Desktop/TIL/DataAnalysis/PaperReview/TopicModeling/LDA/스크린샷 2022-03-03 오후 9.24.58.png)

   P(𝜙1:K, 𝜽1:D, Z1:D, W1:D ) => 모든 토픽들에 대해서, 모든 문서에 대해서, 모든 문서에 대해서, 모든 문서에 대해서 

   

   - 토픽에서의 단어 비중(𝜙_k)
   - 그 단어는 어떤 토픽에서 왔느냐(Z_d,n)
   - 특정한 document의 토픽의 비중(𝜽_d)
   - 특정 document 가 토픽을 얼마의 비중으로 가지고 있느냐?(𝜽_d)

   

   => 𝛃가 주어졌을 때 모든 토픽들에 대해서 𝜙_i 의 확률 p(𝜙_i | 𝛃) 을 모두 𝚷 (product) 하고

   => 모든 문서들에 대해서 𝛂가 주어졌을  document 에 대한 𝜽_d의 확률 p(𝜽_d|𝛂) 을 모두 𝚷 (product) 한다.

   

   특정 document의 토픽의 distribution 혹은 비중이 주어졌을 때 모든 단어에 대해서 개별적인 단어는 어느 토픽에서 뽑아야 하느냐의 확률을 p(Z_d,n | 𝜽_d) 정의하고 

   

   특정 d문서에서 해당하는 단어의 토픽 할당(Z_d,n)과 실제 토픽의 단어 distribution(𝜙1:K) 을 보고 최종적으로 어떤 단어가 나오는지(w_d,n) 조건부절로 계산하는데, 문서에 있는 모든 단어에 대해서 다 독립적으로 발생했다고 가정을 하고 모두 product를 하면, 

   **이게 우리가 최대화를 해야 하는 목적함수!! (그림에 있음.)**

   

   <hr>
   
   
   
   

   1. Draw each topic 𝜙k ~ Dir(𝛃) for i ∈ {1,...., K}

      - 𝜙k 의 분포를 𝛃 하이퍼파라미터를 갖는 Dirichlet 분포로부터 추정을 한다. 디리슐레 분포로부터 𝜙k가 추정이 됐다고 가정한 것.
      - 어떤 주제 k가 있을 때, 그 k의 단어 w들의 생성 및 확률이 랜덤이 아니라 디리슐레 분포를 따른다는 것. 𝛃 하이퍼파라미터를 가지는 디리슐레 분포로부터 토픽의 단어 비중을 추정하겠다. 혹은 각각의 토픽의 단어 분포를 𝛃 하이퍼파라미터를 갖는 디리슐레 분포로부터 추정하겠다.

   2. For each document

      - Draw topic proportions  𝜽_d ~ Dir(𝛂) : 𝛂 하이퍼파라미터를 가지는 Dirichlet 분포로부터 특정 document의 토픽의 비중 𝜽_d 을 추론한다.
      - ![스크린샷 2022-03-03 오후 9.52.31](/Users/jang-eunjun/Desktop/TIL/DataAnalysis/PaperReview/TopicModeling/LDA/스크린샷 2022-03-03 오후 9.52.31.png)
      - 문서마다 토픽의 비중을 할당하겠다 ! (알파 파라미터를 가지는 디리슐레 분포를 통해서!)
      - For each word
        - **Draw Zd,n ~ Multi(𝜽_d)**
        - 토픽의 분포가 주어져있다.
        - Z_d,n 이라는 것을 𝜽_d를 하이퍼 파라미터를 가지는 다항 분포로부터 추정한다.
        - 30, 50,20 일때 1,2,3 번째 단어를 어떤 토픽에서 뽑지? 를 다항 분포로부터 결정한다.
        - ![스크린샷 2022-03-03 오후 10.00.22](/Users/jang-eunjun/Desktop/스크린샷 2022-03-03 오후 10.00.22.png)
        - Draw W_d,n ~ Multi(𝜙_zd,n,n)
        - ![스크린샷 2022-03-04 오전 7.23.35](/Users/jang-eunjun/Desktop/TIL/DataAnalysis/PaperReview/TopicModeling/LDA/스크린샷 2022-03-04 오전 7.23.35.png)

      

      

      > issue 발생 : 사람이 글을 쓰는 process 와는 굉장히 다르다. 
      >
      > LDA 모델이 글을 쓸 때, 각 단어는 독립적익 적용된다. 사람은 글을 쓸 때 그 이전 단어에 영향을 받는다. 즉 시퀀스의 순서에 영향을 받는다.
      >
      > 단어의 독립성이 LDA 의 단점이다.

      

      

      <hr>

      태동 선배님 리뷰

      

      

      - 단어 차원 모델링

      코퍼스 -> 토픽을 사전에 정의할 수 있으면

      단어들을 토픽에 할당한다

      

      - 문서 차원 모델링

      문서의 토픽의 비중

      

      LSA 에서 차원을 줄일수도 있고  document 수도 줄일 수 있다.

      차원을 줄여도 데이터의 구조를 해치지 않으려는게 LSA 의 목표.

      

      bow 가 발전이 되어 word2vec, doc2vec 으로 발전했다.

      근데, lsa가 bow기반으로 되어 있어서 좀 아쉽다.

      

      plsa

      

      다큐먼트 -> 컨셉 -> 단어
      
      다큐먼트 <- 컨셉 -> 단어
      
      
      
      
      
      
      
      

   

   

   

   

   

   

   

   

   

   

   
   
   
   
   
   
   
   
   
   
   



