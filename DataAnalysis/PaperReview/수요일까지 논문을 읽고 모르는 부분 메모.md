수요일까지 논문을 읽고 모르는 부분 메모



1. 논문의 데이터 셋 설명 중에 데이터 셋 두 개를 소개 하는데 하나는 93000건이 있는 데이터고 하나는 6243 + 692 개가 있는 데이터이다. 

   내가 이해하기로는 93000건의 데이터를 이용해서 논문에서 제안한 방식의 모델을 학습하고, 학습에 사용하지 않은 target-dependent Twitter 데이터로 모델의 성능을 평가하려고 하는 거 같은데 내가 이해한게 맞을까요.

2. Glove 임베딩 방식과 LDA2vec 방식을 대략적으로나마 이해하고 싶습니다.

3. 논문에서 weighted function 을 소개하는데, SIF 계산 식 (Equation(4))에서 tf_ic 값이 이해가 가지 않습니다.

   "tf_ic denote the corpus wide term frequencies" 라고 하는데, 이게 정확히 어떤 의미인지 모르겠습니다.

   그리고 논문에서 이렇게 weighted functions 들을 알려주고 있는데 어떻게 임베딩된 벡터에 적용을 하는지 이해가 가지 않습니다. 

   제가 이해하기로는 그 밑에 vector aggreation functions 을 사용하기 위해서 weighted function 값들을 구한걸로 보이는데 이게 맞을까요? 

   그리고 vector aggregation function 을 사용하는 이유도 모르겠는데, vector aggregation functions 을 사용하는 이유가 벡터를 좀더 dense 하게 만들려고 사용하는 거라고 이해해도 될까요?

   그러니까 결국 vector aggregation functions 을 적용한 임베딩 단어가 논문에서 말하는

   "weighted word embedding" 인가요? 이건 아닌 거 같거든요 ㅠㅠ..

   

   4. 벡터 집계 함수 중에서 center-based aggregation 방식과 Delta rule 이 이해가 가지 않습니다.
   
5. architecture 에서 convolution layer 에서 일어나는 계산과 관련해서, 제가 이해한게 맞는지 확인해야될 거 같습니다. 첫 번째 식은 80개의 필터중 하나의 필터 w_i 필터가 matrix를 수직으로 계산해가는 식을 그려낸거 같습니다. 그래서 o_1은 1x1 벡터 한 개로 생각했습니다.
   
   그래서 두 번째 식에 c_i가 해당 필터를 사용해서 계산이 완료된 길이가 있는 수직 벡터로 생각했는데 맞을까요? (근데 이렇게 생각하면 식을 이해하는데 뭔가 설명이 안됩니다.)
   

   

   

   

   

   
