# Latent Dirichlet Allocation



LDA가 실제로 파라미터들을 inference 하는 부분을 보겠다.





<hr>



## LDA 추정



LDA 가 추정해야하는 것들 3가지



1. 특정 document 의 n번째 단어가 어느 토픽으로부터 왔니 (Z_d,n)
2. 각각의 doc에 대한 토픽의 비중 혹은 비율 (𝜽_d)
3. 각각의 토픽은 어떤 단어들에 대한 분포를 가지고 있는가? (𝝓_k)



##### 결국 우리가 구해야 하는 사후 확률은 p(𝜽, 𝝓,Z_d,n |w)

이거에 대한 식은 ppt에 나와있음 (이따가 저장해서 올리기)

식의 분자는 결합확률로서 구할 수 있는데, 분모는 구할 수 없다.(모든 파이, 모든 세타 모든 z에 대해서 구함)



따라서 이 값을 근사해서 구하는 방법론을 사용한다.

개념적으로 가장 이해하기 쉽고 직관적인 Collapsed Gibbs sampling 으로 추정 방법을 살펴볼 것.



<hr>



## Dirichlet Distribution



1. 이항분포(베르누이 분포)

2. 다항분포

3. 베타분포

   - 동전이 두 개 있다고 가정했을 때, 우리는 동전의 앞면과 뒷면이 나올 확률 각각이 1/2 라는 것을 잘 알고있다.

   - 우리가 어떤 동전 A를 던졌을 때, 앞면이 8번 뒷면이 2번 나왔다고 가정해보자. 우리는 이 관측값을 통해서 본래 이 동전의 확률 p를 추정하고 싶을 수 있다.

   - ##### 관측값들로 부터 가장 성공확률이 그럴듯한 분포를 추정해볼 수 있다. (동전을 미리 10번을 던져봤더니)

   - ##### 즉, 실제의 확률이 나올 distribution 을 추정할 수 있다.

   - ![스크린샷 2022-03-04 오전 8.01.45](/Users/jang-eunjun/Desktop/TIL/DataAnalysis/PaperReview/LDA/스크린샷 2022-03-04 오전 8.01.45.png)

   - 위의 예시에서 𝜶, 𝜷 가 각각 8,2 라고 가정했을 때 B(𝜶, 𝜷) 는 다음과 같다.

   - ![스크린샷 2022-03-04 오전 8.04.07](/Users/jang-eunjun/Desktop/TIL/DataAnalysis/PaperReview/LDA/스크린샷 2022-03-04 오전 8.04.07.png)

   - ![스크린샷 2022-03-04 오전 8.02.19](/Users/jang-eunjun/Desktop/TIL/DataAnalysis/PaperReview/LDA/스크린샷 2022-03-04 오전 8.02.19.png)

   - distribution over distribution : binomial 성공 확률 p에 대한 분포를 그린게 Beta 분포 (과거에 관측한 데이터를 기반으로 해서)

     

4. Dirichlet distribution

   - Beta 분포를 다항 분포로 확장시킨게 dirichlet 분포
   - 토픽의 개수가 지금까지 몇 번 등장했는지 보고서는, 결국엔 해당하는 토픽들이 얼마만큼의 확률을 가질 수 있을건인지 추정할 수 있다. (정확하게 beta 분포를 확장한 것,) 밑의 식은 Beta 분포의 식을 확장한 Dirichlet distribution 식이다
   - ![스크린샷 2022-03-04 오전 8.14.30](/Users/jang-eunjun/Desktop/TIL/DataAnalysis/PaperReview/LDA/스크린샷 2022-03-04 오전 8.14.30.png)
   - Beta 분포를 일반화해서 확장한게 Dirichlet distributiion 이다.
     - beta분포는 binomial 에 해당하는 성공 확률에 대한 distribution 
     - Dirichlet 분포는 다항 분포의 성공 확률들에 대한 distribution 이다.

5. Dirichlet distribution의 중요한 properties

   - 1. ##### 조건부 확률도 Dirichlet distribution 이다.

        1. ![스크린샷 2022-03-04 오전 8.23.14](/Users/jang-eunjun/Desktop/TIL/DataAnalysis/PaperReview/LDA/스크린샷 2022-03-04 오전 8.23.14.png)
        2. 각각의 범주들에 대한 관측치를 실질적으로 카운트하고 해당하는 확률을 업데이트 하면 그것도 역시 Dirichlet distribution 에 의해서 정해진다.
        3. ![스크린샷 2022-03-04 오전 8.23.45](/Users/jang-eunjun/Desktop/TIL/DataAnalysis/PaperReview/LDA/스크린샷 2022-03-04 오전 8.23.45.png)
        4. 본래의 사후 확률에 실제 관측 개념만 넣어서 새로운 식을 만든다. 
        5. 각각의 범주들에 대한 관측치를 실질적으로 카운트하고 해당하는 확률을 업데이트 하면 그것도 dirichlet distribution 에 의해서 정해진다.
        6. X1 .... xk 는 실제 관측 데이터들, pi 는 i단어일 때의 실제 확률. 위의 조건부 확률 식에서 관측치에 대한 정보만 추가된다.
        7. ![스크린샷 2022-03-04 오전 10.31.06](/Users/jang-eunjun/Desktop/TIL/DataAnalysis/PaperReview/LDA/스크린샷 2022-03-04 오전 10.31.06.png)

     2. ##### 𝜶 값들은 평균에 대한 shape 가 sparsity 를 조정하게 된다.

        1. 만일 𝜶가 (1,1,1) 이라고 했을 때, 실제 관측값이 1번 1번 1번 나왔다는 이야기.

        2. 이 경우는 실제 값이 랜덤하게 나올 경우가 크다. (P1 = P2 = P3 = 1/3)

        3. 만일 𝜶가 (2,2,2) 라고 했을 때, 실제 관측값이 2번, 2번, 2번 나왔다는 이야기.

        4. 이 경우는 확률이 점점 고정적이게 되고 값이 랜덤하게 나올 확률이 적어진다.

        5. 예를 들어서 3면인 주사위가 있다고 가정해보자, 각각의 면이 3번 던졌는데 1번씩 나왔다면 𝜶 = (1,1,1) 로 표현이 된다.

        6. 이 경우 시행 횟수가 적기 때문에 다음에도 똑같이 1,1,1이 나올 보장이 적다.

        7. 하지만 𝜶 = 2,2,2 가 나왔다고 가정해보자. 그러면 1,1,1 보다는 다음에도 똑같은 결과가 반복될 확률이 커진다.

        8. 더 나아가 10,10,10 이 나오게 되면 더 커진다.

           

        9. ![스크린샷 2022-03-04 오전 8.35.38](/Users/jang-eunjun/Desktop/TIL/DataAnalysis/PaperReview/LDA/스크린샷 2022-03-04 오전 8.35.38.png)

        

     3. 근데 𝜶 = 2,10,2 는 p2가 나올 확률이 크다는 것이고 2,2,10이면 p3가 나올 확률이 크다는 것.

     4. 근데 𝜶 < 1작으면!?

        - 그 관측치에 대해서는 균등할 확률이 안나올 것, 극단적인 확률 분폭라 나올 상황이 많아짐을 의미한다.

        - 그림으로 치면 10,10,10 과 정반대의 그림

        - if 각각의 𝜶 = 100, k = 5. 와 𝜶 = 10, 그리고 𝜶 = 1 일 때를 비교하면 값이 클수록 균등하게 나온다.

        - 0.1이되면 특정 범주만 독단적이게 많이 나오는 경우가 많다.

        - 토픽 모델링 관점에서 보면 𝜶를 크게 하면 각각의 개별적인 문서에는 토픽들이 굉장히 균질하게 분포하며, 작게 하면 특정한 문서에서는 특정 토픽들이 집중적이게 나타날 것이다. (아래의 그림을 보고 설명)

        -  ![스크린샷 2022-03-04 오전 9.30.33](/Users/jang-eunjun/Desktop/TIL/DataAnalysis/PaperReview/LDA/스크린샷 2022-03-04 오전 9.30.33.png)

        - ![스크린샷 2022-03-04 오전 9.33.18](/Users/jang-eunjun/Desktop/TIL/DataAnalysis/PaperReview/LDA/스크린샷 2022-03-04 오전 9.33.18.png)

        - ![스크린샷 2022-03-04 오전 9.31.12](/Users/jang-eunjun/Desktop/TIL/DataAnalysis/PaperReview/LDA/스크린샷 2022-03-04 오전 9.31.12.png)

        - ![스크린샷 2022-03-04 오전 9.31.20](/Users/jang-eunjun/Desktop/TIL/DataAnalysis/PaperReview/LDA/스크린샷 2022-03-04 오전 9.31.20.png)

        - ![스크린샷 2022-03-04 오전 9.34.52](/Users/jang-eunjun/Desktop/TIL/DataAnalysis/PaperReview/LDA/스크린샷 2022-03-04 오전 9.34.52.png)

        - 관례적으로 LDA 에서 𝜶 를 통해서 각 문서의 토픽 비율을 찾을 때 𝜶 < 0 의 값을 사용한다. 그 이유는 각 특정 문서는 주제들이 균질하게 분포되어 있는 게 아니라 특정 토픽들 몇 개만 있기 때문이다.

        - ##### 반면에 𝜷는 1에 가깝게 잡아서 문서의 단어의 비율을 균질하게 만든다. => 𝜶,𝜷는 하이퍼 파라미터다. (값의 리미트 why)

   

   결국 밑의 식의 값이 최대화 하는게 LDA 추론의 목적.

   (하이퍼 파라미터값 4개를 사용.)

   

   ![스크린샷 2022-03-03 오후 9.24.58](/Users/jang-eunjun/Desktop/TIL/DataAnalysis/PaperReview/LDA/스크린샷 2022-03-03 오후 9.24.58.png)

   

