# Top2Vec_Distributed Representations of Topics



HDBSCAN, 

Umap,

mutual information



# ABSTRACT



토픽 모델링은 잠재된 의미 구조 (주제) 를 documents 들의 collection 으로부터 찾는다.

지금까지 보통 LSA 와 LDA 를 사용해서 토픽 모델링을 진행했다.

널리 사용되고 사랑받는 기법들이긴 하지만 몇 가지 문제점들을 안고있다.

이 용법들은 최적의 성능을 내기 위해서 lemmatization, stop-word list, stemming 등등의 전처리를 진행한다.

또한 BOW(bag of words) 방식의 단어 representation 을 사용하기 때문에 표현이 단어의 순서나 의미를 제대로 담고있지 못하다.

그래서 단어나 문서의 Distributed representation 들이 단어나 문장의 의미를 남아낼 수 있기 때문에 인기 있었던 것이다.

논문의 연구원들은 공동 docs들과 단어 의미 임베딩을 이용해서 "토픽 벡터" 를 찾는 **top2vec** 을 제안한다.

top2vec은 사전 전처리 작업 즉, stemming, lemmatization, stop-word 등등을 할 필요가 없으며 자동으로 토픽들의 개수를 찾는다.

top2vec을 통해 도출된 토픽 벡터들은 의미론적 유사성을 나타내는 문서 및 단어 벡터와 함께 결합되어 있다.

논문 연구원들은 top2vec이 확률 기반의 토픽 모델들보다 더 유익하고(정보를 제공하고) 더 대표성이 있다고 주장한다.



## Introduction



대량의 text 들을 요약하고 조직하고 찾는건 nlp 에서 흔한 task 이다.

토픽 모델링은 규모가 큰 데이터 셋에서 사람에 의해서 읽혀질 수 없고 정렬되어질 수 없을 때 사용되는 모델 기법이다.



문서라고 불리는 많은 text들의 구성으로 이루어진 corpus 가 주어지면,(즉 corpus 가 주어지면)

토픽 모델은 문서 안에 있는 잠재된 의미론적 구조 또는 토픽을 찾는다.

그러면 토픽 모델링은 큰 규모의 docs의 높은 수준의 요약을 할 수 있다.

(관심있는 문서를 검색하고 비슷한 docs들끼리 그룹화한다.)



##### what is topic

토픽(topic) 이 무엇인지에 대한 이야기는 계속해서 진행 중이다. (분절된 가치들 science, politics 등등)

하지만 우리가 다루는 토픽은 위의 것들이 아니다. 왜냐하면 이러한 type 혹은 정의의 토픽들은 sub-topic들로 분화될 수 있기 때문이다.

추가적으로 "politics" 와 같은 토픽들은 다른 토픽들 예컨데 "health" 와 같은 토픽과 겹친다. 왜냐하면 그들은 공통된 sub-topic들을 공유하기 때문이다.

(politics 와 health 는 sub topic 으로 "health care" 를 공유한다.)



이러한 것들 중에 아무거나.. 중에서, 그들(토픽)의 combination 이나 variations 들은 weighted 된 단어들의 독특한 set으로 묘사될 수 있다.

논문은 토픽을 토픽에 대해서 표현하기 위해 사용된 weighted 된 단어들의 무한히 많은 조합들로 연속적이라고 가정한다.

ex) weighted word1 + weighted word2 + weighted word3 + weighted word4 + ... weighted word5 ...weighted word..i





또한 이것 뿐만 아니라 논문은 document 가 그 자체의 topic 을 가지고 있다고 가정한다.



> ##### 가정
>
> 1. 토픽은 weighted word 들의 연속체이다.
> 2. 어떤 document 는 그 자체의 토픽을 가지고 있다.



따라서 논문에서 말하는 토픽을 찾는다고 함은, 문서의 정보를  가장 잘 나타내는 단어들의 weighted sets을 찾는 것이다.



그후 논문 Introduction 에서는 관련된 작업들과 토픽의 분산 표현에 대해서 소개한다.

그리고 section 2에서는 top2vec 모델에 대해서 살펴보고 section3 에서는 토픽 정보 이득을 설명하고 논문의 실험을 요약한다. 그리고 section4에서 결론을 내린다.



##### Traditional Topic Modeling Methods



-LDA 와 PLSA

토픽 모델링과 관련된 작업인 전통적인 토픽 모델링들에 대해서 살펴보자.

전통적으로 corpus 에 대한 토픽 모델링은 LDA를 통해 이루어져왔다.

이 모델은 생성적인 확률 모델(generative probability model) 로 각각의 documents 들을 topic 들의 혼합으로 보며, 각각의 topic 들은 word들의 분포로 본다.

LDA 는 확률적인 잠재적 의미 분석인 PLSA 를 일반화한다, document-topic(특정 document d의 topic 분포) 과 topic-word(특정 토픽 k의 word 분포) 분포에 Dirichlet 분포를 추가해서 잠재적 의미 분석인 PLSA 를 일반화한다.



-LDA, PLSA 약점

LDA와 PLSA 는 연속된 토픽 공간을 t topics 들로 이산화하고 model documents 들을 t topics들의 혼합으로 본다.

##### 이 모델들은 토픽들의 숫자 t가 알려져있다고 가정한다. (LDA 를 만들 때 임의의 토픽의 숫자를 사용자가 직접 정해준다.)

토픽을 이산화 하는 것은 documents 와 words들의 관계를 모델링 하기 위해서 필수적이다.

이게 이 모델(LDA) 의 가장 큰 약점이다.

주제의 수 t 또는 그것을 추정하는 방법은 거의 알려지지 않았다. 특히 크고 친숙하지 않은 데이터셋에서는 더욱 그렇다.



-stop words 제거 (전처리 작업)

위와 같은 방법으로 만들어진 토픽들은 단어 확률 분포이다.

토픽에서 가장 높은 확률을 가지고 있는 단어들은 "the", "and" 와 같은 것들이다. (다른 언어도 마찬가지)

이런 common-word 들은 stop-words 라고 불린다.

Stop-words 들은 토픽을 해석 가능하게끔 하기 위해서 제거 되기도 한다. 그리고 유익한 토픽 단어들을 추출한다.

stop word를 찾아서 제거하는 일은 언어와 corpus 특이적이기 때문에 하찮은 일이 아니라 중요한 일이다.

(강아지에 관한 글이 있을 때, 강아지가 나온 빈도수가 많을 것이기 때문에 이를 stop-word로 간주하고 제거할 것이다. 왜냐하면 정보가 많이 담겨있다고 생각하지 않기 때문이다.)





LDA 와 LSA 는 입력값으로 bow 방식으로 표현된 단어 표현을 사용한다.

bow 방식의 표현은 단어의 의미를 무시한다.



> Canada - Canadian
>
> big - large



Canada 와 Canadian 은 의미상으로 유사함에도 불구하고 완전 다른 단어로 표현된다. (bow의 비슷함이 없다.)

Stemming과 lemmatizaion 전처리가 이런 문제를 해결하기 위해서 사용되는데, big 이나 large 같이 둘은 의미상으로 유사함에도 불구하고 stem단어를 공유하지 않았기 때문에 그 둘의 유사함을 인지하지 못한다.



확률적인 생산 모델(LDA, PLSA) 의 목적은 본래의 문서 단어 분포를 최소한의 오류로 재현 할 수 있는 토픽을 찾는 것이다. (특정 토픽의 단어 분포를 이용해서 doc의 n번째 단어를 생성한다. 이때 토픽도 문서 내의 토픽 분포에 의해서 결정된다.)

하지만, 모든 텍스트들의 큰 부분은 주제적으로 고려될 수 없을, 정보 가치가 없는 단어들을 포함한다. 문서 내 단어들을 정보가 있는지 없는지 구분해야 하는데, LSA, LDA 와 같은 모델들은 이런 구분을 할 수 없다. 왜냐하면 그들의 목적은 단순하게 문서의 단어 분포를 재현하는데 맞춰져 있기 때문이다.

따라서 토픽 내에서 높은 확률분포를 가지고 있는 단어들은 부득이하게 사용자가 직관적으로 주제 단어라고 생각하는 것과 일치할 수 없다.



##### Distributed Representation of Words and Documents



- word 와 documents 에 대한 vector representation.

- 분포 가설 (distributed hyphothesis) 비슷한 context 에서는 비슷한 의미 단어들이 사용된다.
- word2vec (전통적인 방법들과 비교해서 언어적인 업무들에서 최첨단의 방법)
- PMI(point wise mutual information) 방식을 이용한 GLOVE 방식 등.
- Document 에 대한 vector 를 얻기.



##### Distributed Representation of Topics



Semantic space(의미 공간) 에서의 distance 는 **의미론적 연관성** 을 표현한다.

단어의 의미론적 임베딩을 위한 많은 시도들이 있었고, word2vec이 그 중에 하나다.

doc2vec 모델은 같은 공간에서 함께 임베딩된 word와 document vector 들을 배울 수 있다.



함께 임베드된 document와 word vector들은 학습될 수 있다 word vector 의미론적으로  가까운 document vector처럼.

doc2vec 에서 공동으로 포함된 doc과 word 벡터는 doc 벡터가 의미론적으로 유사한 word 벡터와 가깝도록 생성된다.

이 속성을 통해서 word 벡터를 통해서 유사한 문서를 query 할 수 있게 된다. (doc 벡터가 유사한 word 벡터와 가깝게 학습이 되기 때문에.)

그리고 어떤 단어가 문서와 가장 유사한지 그리고 가장 대표적인지 파악할 수 있다.

##### 그래서 doc 벡터와 가장 유사한 word 벡터는 doc 토픽을 가장 대표할 가능성이 높다.



이렇게 document 와 word를 함께 임베딩하는 건 의미론적(semantic) 임베딩과 같다.

왜냐하면 임베딩된 공간의 거리는 document와 words 간의 의미론적 유사도를 측정하기 때문이다. 이를 쉽게 말하면 doc과 word를 동시 임베딩하게 되면 임베딩된 각각의 벡터간의 차이는 의미론적 유사도를 의미한다.



전통적인 BOW 기반의 topic modeling 과는 다르게, 의미론적(semantic) 임베딩은 word와 documents 같은 의미론적 유사도를 측정하는데 의미가 있다. (그 doc과 많이 유사한 단어는 토픽으로 간주될 수 있는건가?)

논문(연구원들)은 의미론적 공간은 그 스스로 연속적인 토픽 표현이라고 주장한다.

space에서의 점들은 토픽을 의미하고, 이 각각의 토픽 point 들은  각각 그것들의 가장 가까운 단어들에 의해서 요약된 토픽들이다. (nearest words)

(space 의 점들 주변은 그 점과 유사한 단어들의 분포되어 있는건가?)

##### doc의 밀집된 지역은 유사한 토픽들을 가진 많은 문서들로 여겨진다. => 이게 뭔말이지



연구원들은 이 가정을 document vectors 들의 밀집된 지역으로부터 계산된 분산 토픽 벡터인 top2vec을 주장하기 위해서 사용한다.

top2vec에서 발견되는 밀집된 지역의 숫자는 눈에 띄는 주제의 숫자로 가정된다.

(keyword = dense area,  dense area의 개수가 토픽의 숫자.. 기존의 LDA 처럼 하이퍼파라미터가 아니라 모델이 토픽의 개수를 정해준다.)



자. 이제 dense로부터 주제 벡터가 도출이 되는데, doc의 밀집 지역의 중심이 주제 벡터로 계산된다. 

dense area는 매우 유서한 문서들의 영역이다. 

또 dense area 의 중심 혹은 토픽 벡터는 그 dense area 를 가장 대표하는 평균 document 로 여겨질 수 있다.

(Leverage = 활용하다, 자주나오네)

각 토픽의 가장 대표적인 word를 찾기 위해서 의미론적 임베딩을 활용(leverage, 단어 복습^^)한다. by 토픽 벡터와 가장 가까운 word 벡터들을 찾음으로써 



top2vec 모델은 임베드된 토픽, doc, word 벡터들을 만든다. 그리고 그것들간의 거리는 의미론적 유사성을 나타낸다.

그 전의 LDA, pLSA와 다르게 토픽을 찾는데 있어서, top2vec 은 전처리 작업들(lemmatization, stop-words, stemming) 이 필요하지 않다. (다른 모델에 비한 이점)



이 토픽 벡터는 유사한 doc을 찾을 때 사용될 수 있으며 word는 유사한 토픽을 찾는데 사용될 수 있다.

word2vec 과 같이 설명되어지는 벡터의 똑같은 선형(나는 shape을 설명하고 싶은거라 생각했다.) 은 단어들, 문서들 그리고 토픽 벡터들 사이에서 사용될 수 있다.



토픽 벡터들은  각각의 **문서 벡터의 최근접 토픽 벡터** 를 기반으로 **토픽 사이즈** 들이 계산되어지는걸 허용한다.

또한 주제 벡터에서 주제 감소를 수행하여 유사한 토픽들을 계층적으로 그룹화하고 발견된 토픽들의 개수를 줄인다.



top2vec 모델들과 확률적인 생산 모델의 가장 큰 차이점은 각 모델이 주제를 어떻게 모델링 하는지이다.

LDA와 plsa는 최소한의 오류로 본래의 doc word 분포를 재현하는데 사용되는 단어의 분포로 토픽을 모델링한다.

이는 종종 토픽 안에서 높은 확률을 가지고 있는 주제에 적합하지 않은 단어들을 취한다.

왜냐하면 그들은 모든 text들의 큰 비중으로 이루어져 있기 때문이다.

대조적으로 의미론적 임베딩 top2vec의 토픽 벡터는 문서들 사이에서 공유되어지는 눈에띄는 토픽을 표현한다.

**토픽 벡터** 와 가장 가까운 주변 단어들은 토픽을 묘사하고 그 단어들은 가장 문서를 잘 나타낸다.

이는 동시 doc, word 학습 task 때문이다. 이것은 어떤 word가 doc을 가장 잘 나타내는지 예측하기 위한 공동 문서와 단어 포함 학습 작업 때문이다. 따라서 **토픽 벡터** 가 가장 유익한 단어에 가장 가깝다.



## Model Description



문서의 토픽을 추출하기 위해서 특정 속성들을 가지고 있는 함께 임베드된 doc 벡터와 word 벡터가 필요하다.

특히 우리는 doc 벡터와 word 벡터간의 거리를 임베딩 하는 게 필요하다. 그리고 word 벡터들은 의미론적 연관성을 나타낸다.

##### 의미론적으로 유사한 doc들은 의미론적 공간에서 서로 근접하게 위치된다. 그리고 당연히 의미론적으로 유사하지 않은 doc 들끼리는 멀게 위치한다.

##### 추가적으로 단어들은 그들이 가장 잘 표현할 수 있는 doc들에 근접하게 위치한다.

##### 특정 속성들을 가지면서 함께 임베드된 doc 벡터와 word 벡터들을 사용해서 토픽 벡터는 계산될 수 있다.



> 1. 의미론적으로 유사한 doc들끼리는 의미 유사성 공간에서 근접하게 나타난다. 유사하지 않은 doc들끼리는 멀게 나타난다.
> 2. word 벡터는 본인이 가장 잘 표현할 수 있는 doc에 가깝게 나타난다.
> 3. doc벡터, word 벡터를 이용해서 (물론 같이 embedding 작업을 거친) 토픽 벡터를 계산할 수 있다.



##### 논문 저자들은 윤곽이 그려진 속성들과 함께한 의미론적 공간은 연속적인 토픽의 표현이라고 주장한다.



![스크린샷 2022-03-06 오후 4.49.24](/Users/jang-eunjun/Desktop/TIL/DataAnalysis/PaperReview/Top2Vec_Distributed Representations of Topics/스크린샷 2022-03-06 오후 4.49.24.png)



> 보라색 point : doc
>
> 초록색 point : word
>
> 유사한 doc들끼리는 근접하게 나타나고 유사하지 않은 doc들끼리는 멀리 나타난다.
>
> word는 본인이 가장 잘 표현하는 doc들과 근접하게 나타난다.





여기서 같이 embedding 된 doc 벡터와 word 벡터를 얻기 위해서 논문 저자들은 doc2vec 을 사용한다.



doc2vec 의 두 가지 버전이 존재



1. the Paragraph Vector with Ditstributed Memory(DM)
   - context word 사용
   - context witndow 와 함께 target word 를 예측하기 위해서 doc vector 를 사용.
2. DIstributed Bag of Words(DBOW)
   - doc 안에서 context window 안에 있는 단어들을 예측하기 위해서 doc 벡터 사용



> DBOW 가 더 간단한 모델임에도 불구하고 성능이 더 좋게 나타났다.
>
> 따라서 논문은 doc2vec 의 버전 중 하나인 DBOW 모델을 사용한다.



doc2vec 의 DBOW 아키텍처는 word2vec 의 skip-gram model(window 내에서 주변 단어를 예측하는 형식) 과 굉장히 유사하다.

유일한 차이점은 doc2vec DBOW 는 window 내의 주변 단어들을 사용하는 대신에 window 내에서 주변 단어를 예측하는데 사용되어지는 doc 벡터를 사용한다.



##### word2vec 과 doc2vec



1. word2vec

   - CBOW

     - window 사이즈 지정

     - target word 주변의 window 사이즈 내의 단어들(맥락 단어)을 이용해서 target 단어를 예측한다.

     - 즉, input 단어로 맥락 단어들을 사용한다.

     - 일반적으로 input 단어에 대한 가중치 W의 각 행을 해당 단어의 분산 표현으로 사용한다.

     - 오차 역전파를 이용해서 입력 단어들에 대한 가중치들을 업데이트 한다.

     - cbow 학습 과정은 다음 사실을 전제한다.

       ex) we guzzle beer, we guzzle wine

       똑같은 맥락에서 drink 말고 guzzle 이 사용이 됐는데, 이럴 때 우리는 guzzle 과 drink 가 비슷한 단어라는 걸 알 수 있다.

       즉, 주변 맥락에 따라서 어떤 단어가 나올 수 있는데, 똑같은 맥락에서 어떤 단어들이복수 사용 가능하면, 그 단어들  끼리는 유사성이 있음을 알 수 있다.

   - SKIP-GRAM (논문은 사용)

     - CBOW 방식과 반대 방식을 이용한다.(맥락과 타깃을 역전시킨 모델)
     - 타깃 단어로부터 맥락 단어들을 예측한다.
     - 어떤 유사한 단어들끼리는 주변 맥락 단어들이 유사하게 등장할 것이다를 전제하고 학습한다.

2. doc2vec

   - paragraph 에 id 를 붙인다.
   - PV-DM, PV-DBOW 방식으로 나눔.
   - ![doc2vec 2](/Users/jang-eunjun/Desktop/TIL/DataAnalysis/PaperReview/Top2Vec_Distributed Representations of Topics/doc2vec 2.png)
   - ![doc2vec](/Users/jang-eunjun/Desktop/TIL/DataAnalysis/PaperReview/Top2Vec_Distributed Representations of Topics/doc2vec.png)
   - "the cat sat on the mat"
   - 이는 각 단어가 자신과 유사한 문서를 유인하는 동시에 자신과 다른 문서를 폐기하는 것으로 해석할 수 있다. 
   - 그 결과 문서가 문서를 가장 잘 설명하는 단어와 가장 가깝고 문서와 다른 단어와는 거리가 먼 의미 공간이 형성된다.
   - 비슷한 단어들로 같은 지역으로 끌어당겨질 것이기 때문에 이 공간에서는 비슷한 문서들이 서로 가까이 있을 것이다.
   - 서로 다른 문서들은 다른 단어들에 의해 의미 공간의 다른 영역으로 이끌리기 때문에 멀리 떨어져 있을 것이다.





##### 연구원들은 word2vec 과 doc2vec으로 생성된 의미론적 공간이 연속적인 토픽들의 표현이라고 주장한다.





d차원 공간의 point를 n개의 단어에 대한 확률 분포로 변환하는 방법.

이는 word2vec 에 의해서 만들어진 학습된 벡터 공간을 관찰하면 명확해질 수 있다.

이 모델은 학습되는 모든 단어 n을 위한 d차원의 context 단어 벡터들을 포함하는 행렬 W_n,d 를 학습한다.

연구원들은 이 d차원 임베딩 공간을 W_n,d 행렬에 의해서 정의되어진 연속적인 토픽 표현이라고 주장한다.



임베딩 공간으로부터 d차원 벡터에 적용될 때, 행렬 w_n,d 는 n차원 벡터를 생성하는 선형 변환으로 볼 수 있다.

d차원 공간으로 단어들이 표현되고 각 point 는 단어의 d차원 표현이라고 할 수 있으며 각각의 힘으로 측정된다.

행렬 "W Δ_n,d"는 내장 공간으로부터 d차원 벡터에 적용될 때 n차원 벡터를 생성하는 선형 변환으로 볼 수 있다.



하지만 모델이 실제 배우는 것은 d차원의 공간에서 전체 단어에 대해서 point 를 어떻게 확률 분포적으로 변화시키는지이다.

따라서 d 차원 내에서의 어떤 벡터든지 n word vocabulary 전체에 대해서 소프트 맥스 함수를 사용해서 확률 분포적으로 변환될 수 있다.

따라서 d차원의 point들은 다른 토픽들로 표현될 수 있다.



각각의 단어 벡터들은 각각의 단어의 가장 큰 확률을 가지는 d차원 공간의 토픽과 일치한다.

ex) cat 단어의 벡터 표현은 소프트맥스 함수로 넣었을 때, cat의 확률이 가장 큰 벡터 값이다.



일반적으로 d차원을 가지는 point(단어 vector 로 이해함) 는 가장 가까이 있는 word 벡터들을 사용해서 의미론적으로 가장 잘 설명할 수 있다. 왜냐하면 어휘 내의 n개의 단어들에 대한 해당 주제 분포에서 가장 높은 확률을 가지는 단어들이기 때문이다.



### Find Number of Topics



의미론적 임베딩은 연속적인 토픽 표현을 학습하는데 이점이 있다.

Doc, word 는 의미적 공간에서 표현된다.

이 의미론적 공간에서는 doc 벡터는 doc의 토픽으로 표현된다고 할 수 있다.

그 doc 벡터와 가장 가까운  word 벡터들이 doc 토픽을 의미론적으로 가장 잘 표현한다고 할 수 있음.





문서들의 밀집된 공간은 꽤 많이 유사한 문서들의 공간으로 해석될 수 있따.

이 문서들의 밀집된 지역은 문서들의 공통 주제로 나타난다. 

이 doc벡터들이 doc의 토픽으로 나타날 수 있기 때문에, doc벡터들의 중심 혹은 평균이 계산될 수 있다. 이 중심은 doc들의 밀집 지역을 가장 대표하는 토픽 벡터다.

이 토픽 벡터와 가장 가까운 word 벡터들이 의미론적으로 토픽을 가장 잘 표현한 단어다.

##### Doc 벡터들의 밀집지역의 숫자는 눈에 띄는 토픽들의 숫자와 동일하다



#### Low Dimensional Document Embedding



차원 축소를 시켜서 더 효과적이고 정확하게 군집화를 시킬 수 있다.

- Umap -> 주요 차원 축소 방법 (다른 영역에서도 최적의 성능을 나타낸다고 나온다.)

- t-sne -> 다른 차원 축소 방법



논문에서는 umap 방법 선택 (전역적이고 지역적인 구조를 보존해서.. 즉, 축소시에 데이터가 많이 날아가지 않음.)

이 각각의 방법을 자세히 살펴보도록 하자.



##### t-SNE( t-distributed Stochastic Neighbor Embedding)



T-distributed 는 t분포를 의미한다. 따라서 t분포에 대해서 알고 넘어가야할 필요가 있다.

정규분포(normal distribution) 에 대한 개념을 이해한다면 t 분포에 대해 이해를 더 수월하게 할 수 있다.

따라서 먼저 정규분포(normal distribution) 에 대해서 살펴보자.

정규분포를 결정하는 요인은 두 가지가 존재한다. 바로 평균(𝝁, mean) 과 표준편차(𝝈, standard deviation).

즉, 정규분포의 형태는 평균과 표준편차에 따라 결정되고 그때의 정규분포는 아래와 같다.



> N(𝝁,𝝈^2)



정규분포는 실험을 많이 할 수록 즉, 표본 크기가 커질수록 정규분포에 근사함을 알 수 있다.

따라서 우리는 가정을 하나 하게 되는데, 많은 연속된 값을 가지는 측정 값은 정규분포를 따른다는 가정을 하게 되고 실제로 키, 몸무게, 성적 및 자연 현상에서 나타나는 대부분 관측 결과들은 정규분포를 따른다.









##### Umap



umap 은 2가지 기술을 기반으로 한 차원 축소 알고리즘이다.



-  topological data analysis
- Manifold learning



기본 수학 지식으로는 다음과 같은 지식을 요구한다.



- Algebraic topology
- topological data analysis



다음으로 topological data analysis algorithm 의 기본 가정에 더 가깝게 하기 위해 **Riemannian Geometry** 를 사용한다.



=> 불행히도 이러한 방법은 새로운  issue 를 탄생시키지만 심화된 수학과 Fuzzy Logic 을 이용해 해결했다.









#### Find Dense Cluster of Documents



군집화에 기반한 밀집화의 목적은 유사성 공간에서 유사성이 높은 문서들을 찾는 것이다.



doc들이 다양한 밀집지역을 의미 공간에서 가진다는 점이 문제점이다.

추가적으로 doc들이 유사하지 않은 희소 지역이 존재한다.



이것들은 noise 로 여길 수 있다. => 눈에 띄는 주제가 없다.





HDBSCAN 이 doc 들의 밀집 지역을 찾는데 사용된다.

HDBSCAN 은 각각의 doc밀집 지역에 label 을 부여한다. 그리고 밀집지역에 속해있지 않은 doc벡터들 모두에 대해서 noise label을 붙인다.

여기서 dense area 로 임명받은 doc 벡터들은 토픽 벡터를 계산하는데 사용될 수 있다.

근데 noise label이 붙은 doc들은 토픽 벡터로 간주되지 않는다.



![스크린샷 2022-03-06 오후 7.33.02](/Users/jang-eunjun/Desktop/TIL/DataAnalysis/PaperReview/Top2Vec_Distributed Representations of Topics/스크린샷 2022-03-06 오후 7.33.02.png)



이 그림은 umap 차원 축소를 이용한 벡터들을 이용해서 HDBSCAN 군집화를 이용한 그림이다.

빨간색은 noised documnets 들로 분류가 되고

다른 색깔들이 dense area 로 임병받은 doc벡터들이라고 할 수 있다.



HDBSCAN 의 하이퍼 파라미터는 최소한의 군집의 크기이다.

15개로 지정. 크기가 그니까 관련없는 doc들도 포함하는 경우가 있었다고 한다.





### Calculate Topic Vector



#### Calculate Centroids in Original Dimensional Space



UMAP 과 HDBSCAN 을 사용한 거는 라벨을 붙이는 과정과 같다.

의미론적 임베딩 공간에서 각각의 doc 들은 noise label 이거나 밀집된 cluster 에 속하는 label로  분류된다.





밀집된 doc들의 각각의 군집에 대해 라벨을 붙이는 것을 통해서 토픽 벡터들은 계산될 수 있다.

doc 벡터들에 의해서 토픽 벡터가 계산될 수 있는 방법은 여러가지다.

가장 간단한 방법은 중심을 계산하는 것.(같은 밀집 군집에서 모든 doc 벡터들의 산술 평균을 구하는것)

다른 합리적인 방법으로는 기하학적 평균이나, HDBSCAN 을 통해서 만들어진 군집의 신뢰도로부터 나온 확률들을 사용한 방법이 있다.



세 가지 방법을 실험한 결과 모두 다 비슷한 토픽 벡터들을 만들어 냈고, 그 토픽 벡터 주변의 단어들도 비슷하게 도출됐다.

우리는 이게 높은 차원 공간의 희소성 때문이라고 추측했다.

따라서 논문 연구원들은 간단한 중심 계산 법을 사용했다.



![스크린샷 2022-03-06 오후 8.07.41](/Users/jang-eunjun/Desktop/TIL/DataAnalysis/PaperReview/Top2Vec_Distributed Representations of Topics/스크린샷 2022-03-06 오후 8.07.41.png)



docs들의 중심이 토픽 벡터로 계산됐다. (HDBSCAN 으로 인지된 문서의 밀도의 중심)

각각의 밀집된 doc들에서 토픽 벡터들이 만들어지는데, 따라서 그 밀집의 개수가 corpus 내에서의 눈에 띄는 토픽들의 개수로 간주될 수 있다.





#### Find Topic Words



의미론적 공간에서 모든 point 들은 그것의 가장 가까운 word 벡터들에 의해서 의미론적으로 가장 잘 설명된 토픽을 나타낸다. (주변 단어들에 의해서 의미론적으로 설명되는 토픽 = point) 따라서 토픽 벡터와 가장 가까운 단어는 그 토픽 벡터를 의미론적으로 가장 잘 표현한다고 할 수 있다.

그리고 각 word 벡터들과 토픽 벡터와의 거리는 얼마나 그 단어가 토픽과 유사한지를 나타낸다.

그리고 토픽 벡터와 가장 가까운 word 벡터들은 Dense 지역 내의 모든 docs들과 매우 유사한 단어들로 여겨질 수 있다.

이 단어들은 dense 지역의 docs 들의 공통 주제를 요약하는데 사용될 수 있다. (토픽 벡터와 가장 가까운 단어들을 이용해서..)



이 공통 단어들은 대부분ㄷ의 docs들에서 등장하고 그들은 종종 모든 docs들과 동일한 거리에 위치하는 지역에 존재한다.

결과적으로 토픽 벡터와 가장 가까운 단어들은 stop-words 로 지정되지 않을 것이기 때문에 top2vec 에서는 따로 stop-words 에 대한 작업을 할 필요가 없다.



### Topic Size and Hierarchical Topic Reduction



토픽 벡터와 doc 벡터들은 토픽의 사이즈가 계산 되게끔 만든다.

주제 벡터는 각 문서 벡터가 가장 가까운 주제 벡터에 속하도록 문서 벡터를 분할하는데 사용할 수 있다.

이는 각각의 document들을 특정 하나의 토픽으로 연관짓게 만든다. (그 특정 하나의 토픽은 document 와 가장 유사한 토픽임.)

##### 각각의 토픽의 크기는 그 토픽 벡터에 속하는 docs들의 숫자로 측정된다.



계층적으로 주제의 사이즈를 줄일 수 있다.

이건 희망하는 숫자의 토픽 개수가 도달할 때까지 반복적으로 가장 작은 토픽(사이즈가 가장 적은 토픽)을 그것과 가장 유사한 토픽에 합병하면서 진행된다.

가장 작은 토픽의 토픽 벡터와 그것의 가장 가까운 토픽벡터들 간의 가중 산술 평균을 취하면서 이루어지며 각각의 토픽 사이즈에 따라 가중치가 부여된다.

주제간 병합후에 주제의 사이즈도 다시 계산된다.



코퍼를 가장 대표하는 주제를 찾는 이점이 있다.



## Results



토픽 모델들을 가장 잘 평가하는 자연적인 방법은 그 docs들을 얼마나 잘 설명하는지에 대해 점수를 부여하는 것이다.

이 평가는 주제가 사용자에게 얼마나 유익한지를 평가한다.

우리는 그 모델의 토픽 단어들에 의해서 묘사될 때 그 docs 들에 관해 얻은 정보를 측정하는 "mutual information" 을 사용한는걸 주장한다.



전통적인 토픽 모델링 방법들은 토픽 공간을 이산화 시키고 docs들을 그것의 토픽들의 합성으로 묘사한다.

documents D로부터 산출된 토픽들의 집합 T를 평가하기 위해서,  주제 모델에 의해 주어진 주제들의 비율로 설명될 때, 전체 획득된 정보는 각각의 doc에 대해서 계산되어진다.



대조적으로 top2vec은 주제를 지속적으로 표현하고, 주제에 해당하는 문서를 해당 공간에 배치한다.

top2vec에 의해서 발견된 주제 벡터는 docs 그룹의 공통적인 주제 혹은 docs들의 개별 주제에 대한 평균을 나타낸다.

Docs D에서 생성된 top2vec 토픽 set에 대해서 평가하기 위해, docs(벡터)들은 하위 집합으로 분할되고 각 하위 집합은 가장 가까운 토픽 벡터가 동일한 doc 벡터들에 해당한다. => 따라서 각 doc들은 정확히 하나의 주제에 할당된다.

##### 주제 벡터에 가장 가까운 단어로 문서가 설명될 때, 각 문서의 하위 집합에 대해 얻은 총 정보가 측정된다.









### Topic Information Gain



=> 잘 모르겠음



### LDA, PLSA and Top2Vec Topic Information Gain



top2vec이 주제의 개수를 자동으로 찾는 탓에, LDA, PLSA 의 주제의 개수를 top2vec이 찾은 개수만큼 늘려가면서 비교한다.

세 모델을 비교하기 위해서 토픽의 개수도 동일하게 맞추고, 그 토픽 내의 단어들의 상위 개수도 맞췄다.



#### 20 New Groups Dataset



=>다 좋은 성능을 냈다.



## Discussion

#### 

top2vec

의미론적 공간은 연속적인 토픽 표현이다. 그리고 그것은 비슷한 docs들끼리 밀집된 지역으로부터 토픽 벡터, 토픽 사이즈 그리고 위계적인 토픽 축소를 계산하는 것을 허용한다.

또한 top2vec 모델은 의미론적 공간의 거리에 기반해서 단어들과 docs들 그리고 토픽들간의 비슷함을 비교해준다.

그리고 토픽 모델을 평가하는 새로운 평가법을 내놓는데, mutual information 을 사용해서 문서의 주제가 얼마나 유익한가를 계산한다.

토픽 정보 획득은 doc의 토픽 단어로 설명될 때(doc에 대해서 토픽 단어로 설명할 때), doc에 대해 얻은 정보의 양을 측정한다. 이렇게 하면 주제 단어의 품질이 측정되고 문서에 주제를 할당할 수 있다.

논문 연구의 결과는 top2vec이 다양한 크기의 주에와 상위 주제 단어 수에 대해서 전통적인 모델보다 더 유익하고 corpus 를 대표하는 주제를 일관되게 발견하는 것을 보여준다.





top2vec이 전통 모델에 비해서 가지는 가장 큰 장점은 토픽의 수를 자동으로 결정해준다는 점이다.

그리고 더 정보력이 있고 corpus 에 대해서 표현력있는 토픽을 찾는 것이다.

그리고 stop-word 같은 것들을 찾을 필요가 없기 때문에 모델을 굴리는데 편하고 쉽다.

전통적인 모델들의 bow 방식의 단점인, 단어의 의미를 파악하지 못한다는 점을 극복한다.(word2vec 사용하니까>)



전통적인 모델들은 오리지날 docs들의 단어 분포를 재현하는 토픽을 찾으려고 한다.
이는 정보력이 없는 단어들도 전통적인 모델에 포함하게 한다. (단어 빈도수가 쓸데없이 높은 거)



하지만 top2vec은 그것의 learning task 때문에 훨씬 더 정보력있다.

(단어가 나온 문서를 예측하는 task, 그리고 그건 doc벡터들이 doc에 대해 가장 많은 정보력을 가진 word 벡터와 가깝게 위치시킨다.)

블라블라 여튼 더 좋다.

































