# Top2Vec_Distributed Representations of Topics



HDBSCAN, 

Umap,

mutual information



# ABSTRACT



토픽 모델링은 잠재된 의미 구조 (주제) 를 documents 들의 collection 으로부터 찾는다.

지금까지 보통 LSA 와 LDA 를 사용해서 토픽 모델링을 진행했다.

널리 사용되고 사랑받는 기법들이긴 하지만 몇 가지 문제점들을 안고있다.

이 용법들은 최적의 성능을 내기 위해서 lemmatization, stop-word list, stemming 등등의 전처리를 진행한다.

또한 BOW(bag of words) 방식의 단어 representation 을 사용하기 때문에 표현이 단어의 순서나 의미를 제대로 담고있지 못하다.

그래서 단어나 문서의 Distributed representation 들이 단어나 문장의 의미를 남아낼 수 있기 때문에 인기 있었던 것이다.

논문의 연구원들은 공동 docs들과 단어 의미 임베딩을 이용해서 "토픽 벡터" 를 찾는 **top2vec** 을 제안한다.

top2vec은 사전 전처리 작업 즉, stemming, lemmatization, stop-word 등등을 할 필요가 없으며 자동으로 토픽들의 개수를 찾는다.

top2vec을 통해 도출된 토픽 벡터들은 의미론적 유사성을 나타내는 문서 및 단어 벡터와 함께 결합되어 있다.

논문 연구원들은 top2vec이 확률 기반의 토픽 모델들보다 더 유익하고(정보를 제공하고) 더 대표성이 있다고 주장한다.



## Introduction



대량의 text 들을 요약하고 조직하고 찾는건 nlp 에서 흔한 task 이다.

토픽 모델링은 규모가 큰 데이터 셋에서 사람에 의해서 읽혀질 수 없고 정렬되어질 수 없을 때 사용되는 모델 기법이다.



문서라고 불리는 많은 text들의 구성으로 이루어진 corpus 가 주어지면,(즉 corpus 가 주어지면)

토픽 모델은 문서 안에 있는 잠재된 의미론적 구조 또는 토픽을 찾는다.

그러면 토픽 모델링은 큰 규모의 docs의 높은 수준의 요약을 할 수 있다.

(관심있는 문서를 검색하고 비슷한 docs들끼리 그룹화한다.)



##### what is topic

토픽(topic) 이 무엇인지에 대한 이야기는 계속해서 진행 중이다. (분절된 가치들 science, politics 등등)

하지만 우리가 다루는 토픽은 위의 것들이 아니다. 왜냐하면 이러한 type 혹은 정의의 토픽들은 sub-topic들로 분화될 수 있기 때문이다.

추가적으로 "politics" 와 같은 토픽들은 다른 토픽들 예컨데 "health" 와 같은 토픽과 겹친다. 왜냐하면 그들은 공통된 sub-topic들을 공유하기 때문이다.

(politics 와 health 는 sub topic 으로 "health care" 를 공유한다.)



이러한 것들 중에 아무거나.. 중에서, 그들(토픽)의 combination 이나 variations 들은 weighted 된 단어들의 독특한 set으로 묘사될 수 있다.

논문은 토픽을 토픽에 대해서 표현하기 위해 사용된 weighted 된 단어들의 무한히 많은 조합들로 연속적이라고 가정한다.

ex) weighted word1 + weighted word2 + weighted word3 + weighted word4 + ... weighted word5 ...weighted word..i





또한 이것 뿐만 아니라 논문은 document 가 그 자체의 topic 을 가지고 있다고 가정한다.



> ##### 가정
>
> 1. 토픽은 weighted word 들의 연속체이다.
> 2. 어떤 document 는 그 자체의 토픽을 가지고 있다.



따라서 논문에서 말하는 토픽을 찾는다고 함은, 문서의 정보를  가장 잘 나타내는 단어들의 weighted sets을 찾는 것이다.



그후 논문 Introduction 에서는 관련된 작업들과 토픽의 분산 표현에 대해서 소개한다.

그리고 section 2에서는 top2vec 모델에 대해서 살펴보고 section3 에서는 토픽 정보 이득을 설명하고 논문의 실험을 요약한다. 그리고 section4에서 결론을 내린다.



##### Traditional Topic Modeling Methods



-LDA 와 PLSA

토픽 모델링과 관련된 작업인 전통적인 토픽 모델링들에 대해서 살펴보자.

전통적으로 corpus 에 대한 토픽 모델링은 LDA를 통해 이루어져왔다.

이 모델은 생성적인 확률 모델(generative probability model) 로 각각의 documents 들을 topic 들의 혼합으로 보며, 각각의 topic 들은 word들의 분포로 본다.

LDA 는 확률적인 잠재적 의미 분석인 PLSA 를 일반화한다, document-topic(특정 document d의 topic 분포) 과 topic-word(특정 토픽 k의 word 분포) 분포에 Dirichlet 분포를 추가해서 잠재적 의미 분석인 PLSA 를 일반화한다.



-LDA, PLSA 약점

LDA와 PLSA 는 연속된 토픽 공간을 t topics 들로 이산화하고 model documents 들을 t topics들의 혼합으로 본다.

##### 이 모델들은 토픽들의 숫자 t가 알려져있다고 가정한다. (LDA 를 만들 때 임의의 토픽의 숫자를 사용자가 직접 정해준다.)

토픽을 이산화 하는 것은 documents 와 words들의 관계를 모델링 하기 위해서 필수적이다.

이게 이 모델(LDA) 의 가장 큰 약점이다.

주제의 수 t 또는 그것을 추정하는 방법은 거의 알려지지 않았다. 특히 크고 친숙하지 않은 데이터셋에서는 더욱 그렇다.



-stop words 제거 (전처리 작업)

위와 같은 방법으로 만들어진 토픽들은 단어 확률 분포이다.

토픽에서 가장 높은 확률을 가지고 있는 단어들은 "the", "and" 와 같은 것들이다. (다른 언어도 마찬가지)

이런 common-word 들은 stop-words 라고 불린다.

Stop-words 들은 토픽을 해석 가능하게끔 하기 위해서 제거 되기도 한다. 그리고 유익한 토픽 단어들을 추출한다.

stop word를 찾아서 제거하는 일은 언어와 corpus 특이적이기 때문에 하찮은 일이 아니라 중요한 일이다.

(강아지에 관한 글이 있을 때, 강아지가 나온 빈도수가 많을 것이기 때문에 이를 stop-word로 간주하고 제거할 것이다. 왜냐하면 정보가 많이 담겨있다고 생각하지 않기 때문이다.)





LDA 와 LSA 는 입력값으로 bow 방식으로 표현된 단어 표현을 사용한다.

bow 방식의 표현은 단어의 의미를 무시한다.



> Canada - Canadian
>
> big - large



Canada 와 Canadian 은 의미상으로 유사함에도 불구하고 완전 다른 단어로 표현된다. (bow의 비슷함이 없다.)

Stemming과 lemmatizaion 전처리가 이런 문제를 해결하기 위해서 사용되는데, big 이나 large 같이 둘은 의미상으로 유사함에도 불구하고 stem단어를 공유하지 않았기 때문에 그 둘의 유사함을 인지하지 못한다.



확률적인 생산 모델(LDA, PLSA) 의 목적은 본래의 문서 단어 분포를 최소한의 오류로 재현 할 수 있는 토픽을 찾는 것이다. (특정 토픽의 단어 분포를 이용해서 doc의 n번째 단어를 생성한다. 이때 토픽도 문서 내의 토픽 분포에 의해서 결정된다.)

하지만, 모든 텍스트들의 큰 부분은 주제적으로 고려될 수 없을, 정보 가치가 없는 단어들을 포함한다. 문서 내 단어들을 정보가 있는지 없는지 구분해야 하는데, LSA, LDA 와 같은 모델들은 이런 구분을 할 수 없다. 왜냐하면 그들의 목적은 단순하게 문서의 단어 분포를 재현하는데 맞춰져 있기 때문이다.

따라서 토픽 내에서 높은 확률분포를 가지고 있는 단어들은 부득이하게 사용자가 직관적으로 주제 단어라고 생각하는 것과 일치할 수 없다.



##### Distributed Representation of Words and Documents



- word 와 documents 에 대한 vector representation.

- 분포 가설 (distributed hyphothesis) 비슷한 context 에서는 비슷한 의미 단어들이 사용된다.
- word2vec (전통적인 방법들과 비교해서 언어적인 업무들에서 최첨단의 방법)
- PMI(point wise mutual information) 방식을 이용한 GLOVE 방식 등.
- Document 에 대한 vector 를 얻기.



##### Distributed Representation of Topics



Semantic space(의미 공간) 에서의 distance 는 **의미론적 연관성** 을 표현한다.

단어의 의미론적 임베딩을 위한 많은 시도들이 있었고, word2vec이 그 중에 하나다.

doc2vec 모델은 같은 공간에서 함께 임베딩된 word와 document vector 들을 배울 수 있다.



함께 임베드된 document와 word vector들은 학습될 수 있다 word vector 의미론적으로  가까운 document vector처럼.

doc2vec 에서 공동으로 포함된 doc과 word 벡터는 doc 벡터가 의미론적으로 유사한 word 벡터와 가깝도록 생성된다.

이 속성을 통해서 word 벡터를 통해서 유사한 문서를 query 할 수 있게 된다. (doc 벡터가 유사한 word 벡터와 가깝게 학습이 되기 때문에.)

그리고 어떤 단어가 문서와 가장 유사한지 그리고 가장 대표적인지 파악할 수 있다.

##### 그래서 doc 벡터와 가장 유사한 word 벡터는 doc 토픽을 가장 대표할 가능성이 높다.



이렇게 document 와 word를 함께 임베딩하는 건 의미론적(semantic) 임베딩과 같다.

왜냐하면 임베딩된 공간의 거리는 document와 words 간의 의미론적 유사도를 측정하기 때문이다. 이를 쉽게 말하면 doc과 word를 동시 임베딩하게 되면 임베딩된 각각의 벡터간의 차이는 의미론적 유사도를 의미한다.



전통적인 BOW 기반의 topic modeling 과는 다르게, 의미론적(semantic) 임베딩은 word와 documents 같은 의미론적 유사도를 측정하는데 의미가 있다. (그 doc과 많이 유사한 단어는 토픽으로 간주될 수 있는건가?)

논문(연구원들)은 의미론적 공간은 그 스스로 연속적인 토픽 표현이라고 주장한다.

space에서의 점들은 토픽을 의미하고, 이 각각의 토픽 point 들은  각각 그것들의 가장 가까운 단어들에 의해서 요약된 토픽들이다. (nearest words)

(space 의 점들 주변은 그 점과 유사한 단어들의 분포되어 있는건가?)

##### doc의 밀집된 지역은 유사한 토픽들을 가진 많은 문서들로 여겨진다. => 이게 뭔말이지



연구원들은 이 가정을 document vectors 들의 밀집된 지역으로부터 계산된 분산 토픽 벡터인 top2vec을 주장하기 위해서 사용한다.

top2vec에서 발견되는 밀집된 지역의 숫자는 눈에 띄는 주제의 숫자로 가정된다.

(keyword = dense area,  dense area의 개수가 토픽의 숫자.. 기존의 LDA 처럼 하이퍼파라미터가 아니라 모델이 토픽의 개수를 정해준다.)



자. 이제 dense로부터 주제 벡터가 도출이 되는데, doc의 밀집 지역의 중심이 주제 벡터로 계산된다. 

dense area는 매우 유서한 문서들의 영역이다. 

또 dense area 의 중심 혹은 토픽 벡터는 그 dense area 를 가장 대표하는 평균 document 로 여겨질 수 있다.

(Leverage = 활용하다, 자주나오네)

각 토픽의 가장 대표적인 word를 찾기 위해서 의미론적 임베딩을 활용(leverage, 단어 복습^^)한다. by 토픽 벡터와 가장 가까운 word 벡터들을 찾음으로써 



top2vec 모델은 임베드된 토픽, doc, word 벡터들을 만든다. 그리고 그것들간의 거리는 의미론적 유사성을 나타낸다.

그 전의 LDA, pLSA와 다르게 토픽을 찾는데 있어서, top2vec 은 전처리 작업들(lemmatization, stop-words, stemming) 이 필요하지 않다. (다른 모델에 비한 이점)



이 토픽 벡터는 유사한 doc을 찾을 때 사용될 수 있으며 word는 유사한 토픽을 찾는데 사용될 수 있다.

word2vec 과 같이 설명되어지는 벡터의 똑같은 선형(나는 shape을 설명하고 싶은거라 생각했다.) 은 단어들, 문서들 그리고 토픽 벡터들 사이에서 사용될 수 있다.



토픽 벡터들은  각각의 **문서 벡터의 최근접 토픽 벡터** 를 기반으로 **토픽 사이즈** 들이 계산되어지는걸 허용한다.

또한 주제 벡터에서 주제 감소를 수행하여 유사한 토픽들을 계층적으로 그룹화하고 발견된 토픽들의 개수를 줄인다.



top2vec 모델들과 확률적인 생산 모델의 가장 큰 차이점은 각 모델이 주제를 어떻게 모델링 하는지이다.

LDA와 plsa는 최소한의 오류로 본래의 doc word 분포를 재현하는데 사용되는 단어의 분포로 토픽을 모델링한다.

이는 종종 토픽 안에서 높은 확률을 가지고 있는 주제에 적합하지 않은 단어들을 취한다.

왜냐하면 그들은 모든 text들의 큰 비중으로 이루어져 있기 때문이다.

대조적으로 의미론적 임베딩 top2vec의 토픽 벡터는 문서들 사이에서 공유되어지는 눈에띄는 토픽을 표현한다.

**토픽 벡터** 와 가장 가까운 주변 단어들은 토픽을 묘사하고 그 단어들은 가장 문서를 잘 나타낸다.

이는 동시 doc, word 학습 task 때문이다. 이것은 어떤 word가 doc을 가장 잘 나타내는지 예측하기 위한 공동 문서와 단어 포함 학습 작업 때문이다. 따라서 **토픽 벡터** 가 가장 유익한 단어에 가장 가깝다.



## Model Description



문서의 토픽을 추출하기 위해서 특정 속성들을 가지고 있는 함께 임베드된 doc 벡터와 word 벡터가 필요하다.

특히 우리는 doc 벡터와 word 벡터간의 거리를 임베딩 하는 게 필요하다. 그리고 word 벡터들은 의미론적 연관성을 나타낸다.

##### 의미론적으로 유사한 doc들은 의미론적 공간에서 서로 근접하게 위치된다. 그리고 당연히 의미론적으로 유사하지 않은 doc 들끼리는 멀게 위치한다.

##### 추가적으로 단어들은 그들이 가장 잘 표현할 수 있는 doc들에 근접하게 위치한다.

##### 특정 속성들을 가지면서 함께 임베드된 doc 벡터와 word 벡터들을 사용해서 토픽 벡터는 계산될 수 있다.



> 1. 의미론적으로 유사한 doc들끼리는 의미 유사성 공간에서 근접하게 나타난다. 유사하지 않은 doc들끼리는 멀게 나타난다.
> 2. word 벡터는 본인이 가장 잘 표현할 수 있는 doc에 가깝게 나타난다.
> 3. doc벡터, word 벡터를 이용해서 (물론 같이 embedding 작업을 거친) 토픽 벡터를 계산할 수 있다.



##### 논문 저자들은 윤곽이 그려진 속성들과 함께한 의미론적 공간은 연속적인 토픽의 표현이라고 주장한다.



![스크린샷 2022-03-06 오후 4.49.24](/Users/jang-eunjun/Desktop/TIL/DataAnalysis/PaperReview/Top2Vec_Distributed Representations of Topics/스크린샷 2022-03-06 오후 4.49.24.png)



> 보라색 point : doc
>
> 초록색 point : word
>
> 유사한 doc들끼리는 근접하게 나타나고 유사하지 않은 doc들끼리는 멀리 나타난다.
>
> word는 본인이 가장 잘 표현하는 doc들과 근접하게 나타난다.





여기서 같이 embedding 된 doc 벡터와 word 벡터를 얻기 위해서 논문 저자들은 doc2vec 을 사용한다.



doc2vec 의 두 가지 버전이 존재



1. the Paragraph Vector with Ditstributed Memory(DM)
   - context word 사용
   - context witndow 와 함께 target word 를 예측하기 위해서 doc vector 를 사용.
2. DIstributed Bag of Words(DBOW)
   - doc 안에서 context window 안에 있는 단어들을 예측하기 위해서 doc 벡터 사용



> DBOW 가 더 간단한 모델임에도 불구하고 성능이 더 좋게 나타났다.
>
> 따라서 논문은 doc2vec 의 버전 중 하나인 DBOW 모델을 사용한다.



doc2vec 의 DBOW 아키텍처는 word2vec 의 skip-gram model(window 내에서 주변 단어를 예측하는 형식) 과 굉장히 유사하다.

유일한 차이점은 doc2vec DBOW 는 window 내의 주변 단어들을 사용하는 대신에 window 내에서 주변 단어를 예측하는데 사용되어지는 doc 벡터를 사용한다.



##### word2vec 과 doc2vec



1. word2vec

   - CBOW

     - window 사이즈 지정

     - target word 주변의 window 사이즈 내의 단어들(맥락 단어)을 이용해서 target 단어를 예측한다.

     - 즉, input 단어로 맥락 단어들을 사용한다.

     - 일반적으로 input 단어에 대한 가중치 W의 각 행을 해당 단어의 분산 표현으로 사용한다.

     - 오차 역전파를 이용해서 입력 단어들에 대한 가중치들을 업데이트 한다.

     - cbow 학습 과정은 다음 사실을 전제한다.

       ex) we guzzle beer, we guzzle wine

       똑같은 맥락에서 drink 말고 guzzle 이 사용이 됐는데, 이럴 때 우리는 guzzle 과 drink 가 비슷한 단어라는 걸 알 수 있다.

       즉, 주변 맥락에 따라서 어떤 단어가 나올 수 있는데, 똑같은 맥락에서 어떤 단어들이복수 사용 가능하면, 그 단어들  끼리는 유사성이 있음을 알 수 있다.

   - SKIP-GRAM (논문은 사용)

     - CBOW 방식과 반대 방식을 이용한다.(맥락과 타깃을 역전시킨 모델)
     - 타깃 단어로부터 맥락 단어들을 예측한다.
     - 어떤 유사한 단어들끼리는 주변 맥락 단어들이 유사하게 등장할 것이다를 전제하고 학습한다.

2. doc2vec

   - paragraph 에 id 를 붙인다.
   - PV-DM, PV-DBOW 방식으로 나눔.
   - ![doc2vec 2](/Users/jang-eunjun/Desktop/TIL/DataAnalysis/PaperReview/Top2Vec_Distributed Representations of Topics/doc2vec 2.png)
   - ![doc2vec](/Users/jang-eunjun/Desktop/TIL/DataAnalysis/PaperReview/Top2Vec_Distributed Representations of Topics/doc2vec.png)
   - "the cat sat on the mat"
   - 이는 각 단어가 자신과 유사한 문서를 유인하는 동시에 자신과 다른 문서를 폐기하는 것으로 해석할 수 있다. 
   - 그 결과 문서가 문서를 가장 잘 설명하는 단어와 가장 가깝고 문서와 다른 단어와는 거리가 먼 의미 공간이 형성된다.
   - 비슷한 단어들로 같은 지역으로 끌어당겨질 것이기 때문에 이 공간에서는 비슷한 문서들이 서로 가까이 있을 것이다.
   - 서로 다른 문서들은 다른 단어들에 의해 의미 공간의 다른 영역으로 이끌리기 때문에 멀리 떨어져 있을 것이다.





##### 연구원들은 word2vec 과 doc2vec으로 생성된 의미론적 공간이 연속적인 토픽들의 표현이라고 주장한다.





d차원 공간의 point를 n개의 단어에 대한 확률 분포로 변환하는 방법.

이는 word2vec 에 의해서 만들어진 학습된 벡터 공간을 관찰하면 명확해질 수 있다.

이 모델은 학습되는 모든 단어 n을 위한 d차원의 context 단어 벡터들을 포함하는 행렬 W_n,d 를 학습한다.

연구원들은 이 d차원 임베딩 공간을 W_n,d 행렬에 의해서 정의되어진 연속적인 토픽 표현이라고 주장한다.



임베딩 공간으로부터 d차원 벡터에 적용될 때, 행렬 w_n,d 는 n차원 벡터를 생성하는 선형 변환으로 볼 수 있다.

d차원 공간으로 단어들이 표현되고 각 point 는 단어의 d차원 표현이라고 할 수 있으며 각각의 힘으로 측정된다.

행렬 "W Δ_n,d"는 내장 공간으로부터 d차원 벡터에 적용될 때 n차원 벡터를 생성하는 선형 변환으로 볼 수 있다.



하지만 모델이 실제 배우는 것은 d차원의 공간에서 전체 단어에 대해서 point 를 어떻게 확률 분포적으로 변화시키는지이다.

따라서 d 차원 내에서의 어떤 벡터든지 n word vocabulary 전체에 대해서 소프트 맥스 함수를 사용해서 확률 분포적으로 변환될 수 있다.

따라서 d차원의 point들은 다른 토픽들로 표현될 수 있다.



각각의 단어 벡터들은 각각의 단어의 가장 큰 확률을 가지는 d차원 공간의 토픽과 일치한다.

ex) cat 단어의 벡터 표현은 소프트맥스 함수로 넣었을 때, cat의 확률이 가장 큰 벡터 값이다.



일반적으로 d차원을 가지는 point(단어 vector 로 이해함) 는 가장 가까이 있는 word 벡터들을 사용해서 의미론적으로 가장 잘 설명할 수 있다. 왜냐하면 어휘 내의 n개의 단어들에 대한 해당 주제 분포에서 가장 높은 확률을 가지는 단어들이기 때문이다.

<hr>



### Find Number of Topics



의미론적 임베딩은 연속적인 토픽 표현을 학습하는데 이점이 있다.

Doc, word 는 의미적 공간에서 표현된다.

이 의미론적 공간에서는 doc 벡터는 doc의 토픽으로 표현된다고 할 수 있다.

그 doc 벡터와 가장 가까운  word 벡터들이 doc 토픽을 의미론적으로 가장 잘 표현한다고 할 수 있음.





문서들의 밀집된 공간은 꽤 많이 유사한 문서들의 공간으로 해석될 수 있따.

이 문서들의 밀집된 지역은 문서들의 공통 주제로 나타난다. 

이 doc벡터들이 doc의 토픽으로 나타날 수 있기 때문에, doc벡터들의 중심 혹은 평균이 계산될 수 있다. 이 중심은 doc들의 밀집 지역을 가장 대표하는 토픽 벡터다.

이 토픽 벡터와 가장 가까운 word 벡터들이 의미론적으로 토픽을 가장 잘 표현한 단어다.

##### Doc 벡터들의 밀집지역의 숫자는 눈에 띄는 토픽들의 숫자와 동일하다



#### Low Dimensional Document Embedding



차원 축소를 시켜서 더 효과적이고 정확하게 군집화를 시킬 수 있다.

- Umap -> 주요 차원 축소 방법 (다른 영역에서도 최적의 성능을 나타낸다고 나온다.)

- t-sne -> 다른 차원 축소 방법



논문에서는 umap 방법 선택 (전역적이고 지역적인 구조를 보존해서.. 즉, 축소시에 데이터가 많이 날아가지 않음.)

이 각각의 방법을 자세히 살펴보도록 하자.



<hr>



##### t-SNE( t-distributed Stochastic Neighbor Embedding)



##### 1. 정규분포

T-distributed 는 t분포를 의미한다. 따라서 t분포에 대해서 알고 넘어가야할 필요가 있다.

정규분포(normal distribution) 에 대한 개념을 이해한다면 t 분포에 대해 이해를 더 수월하게 할 수 있다.

따라서 먼저 정규분포(normal distribution) 에 대해서 살펴보자.

정규분포를 결정하는 요인은 두 가지가 존재한다. 바로 평균(𝝁, mean) 과 표준편차(𝝈, standard deviation).

즉, 정규분포의 형태는 평균과 표준편차에 따라 결정되고 그때의 정규분포는 아래와 같다.



> N(𝝁,𝝈^2)



실험을 많이 할 수록 즉, 표본 크기가 커질수록 정규분포에 근사함을 알 수 있다.

따라서 우리는 가정을 하나 하게 되는데, 많은 연속된 값을 가지는 데이터 값은 정규분포를 따른다는 가정을 하게 되고 실제로 키, 몸무게, 성적 및 자연 현상에서 나타나는 대부분 관측 결과들은 정규분포를 따른다.(대다수가 그렇다는거지 모든 자연 현상들이 그렇다는 것은 아니다.)



세상의 많은 분포는 이항 분포를 따른다.

이항 분포는 발생할 확률이 p인 사건을 n번 시행하여 사건 발생 횟수에 따른 확률을 나타낸 것. 이는 다음과 같이 나타낸다.



> B(n,p)



세상의 현상들은 독립적인 사건인 경우가 많다.

예를 들어서 동전을 n번 던졌을 때 앞면이 나올 확률은 이항 분포로 다음과 같이 표현된다.

> B(n, 1/2)



만일 100번을 던졌다면 분포는 B(100, 1/2) 이다. 주사위도 마찬가지다.

즉, 이항 분포르 따르는 독립 사건이 많으며, 만일 정규분포가 이항분포를 근사할 수 있으면 세상의 많은 사건들을 설명할 수 있게 된다.

실제로 n이 커지면(시행 횟수) 이항 분포는 정규분포에 근사(유사) 해진다.

결국 주사위를 던졌을 때, 정규분포 모양의 분포 형태가 등장한다면 이는 이항 분포의 n이 무수히 많다고 가정하였을 때의 정규분포를 나타낸 것이다.

이는 다음과 같이 나타낼 수 있다.



> N(np, np(1-p))



예를 들어 주사위를 600번 던졌을 때 1이 나올 확률은 다음과 같이 표현할 수 있다.



> N(600 X 1/6, 600 X 1/6 X 5/6)
>
> => N(100, 83.3)



이는 평균이 100이고 분산이 83.3인 정규분포를 뜻한다.



결론적으로 실제로 우리가 주사위를 던지는 행위는 이산적인 분포인 이항분포를 따르지만 그 횟수가 증가하면 이항분포는 정규분포와 근사해진다.

따라서 실험 횟수 n이 증가했을 때, 이항 분포가 점점 정규분포와 근사해지는 걸 확인할 수 있었던 것.



![정규분포](/Users/jang-eunjun/Desktop/TIL/DataAnalysis/PaperReview/Top2Vec_Distributed Representations of Topics/정규분포.png)



위 그림의 x축은 주사위를 던졌을 때 1이 나타나는 횟수, 확률 변수 X 라 하였을 때,

**X가 정규분포 N(100, 83.3) 을 따른다** 라고 표기한다.



> X ~ N(100,83.3)



##### 2. 표준정규분포

여기서 표준정규분포에 대해서 이야기 해야한다.

표준정규분포는 정규분포의 모양이 N(0, 1) 인 것을 뜻한다.

##### 이렇게 정규분포를 표준정규분포로 바꾸는 과정을 표준화라고 한다.



그럼 표준화는 어떻게 하는 것이며 왜 하는 것인가?

먼저, 서로 다른 세 정규분포가 존재한다고 가정하자.

우리는 주사위의 1이 나온 횟수가 115회 이상일 확률을 구하고 싶다고 가정하자.

위의 세 정규분포에서 115회 이후의 그래프 면적을 구하면 된다.

하지만 세 정규분포는 그 shape 이 다르기 때문에 우리가 구하고자 하는 값을 매번 계산해야 하는 번거로움이 존재한다.



##### 따라서 모든 계산을 표준정규분포에서 하기 위해서 각 분포를 모두 표준화 하기로 한다.

이렇게 표준화된 확률 변수를 Z 라고 부른다.

즉 우리는 확률 변수 X 를 표준화된 확률 변수인 Z로 표준화 하는 것이다.

표준화를 하는 방식은 각 확률 변수에 평균을 빼준 값을 표준 편차로 나누는 것이다.



> Z = X - 𝝁 / 𝝈



자, 지금까지 정규분포와 정규분포를 표준화하는 방법을 살펴봤으니, 표준화된 정규분포로부터 원하는 확률을 구하는 방법에 대해서 살펴보자.

먼저 위의 예시에서 나타났던 N(100, 83.3) 을 표준화하자.

그러면 위 그림의 x축에 있는 모든 확률 변수에 대해서 평균 100을 빼주고 표준 편차인 제곱근 83.3 으로 나눠준다. 결과적으로 평균이 100이고 분산이 83.3 인 정규분포는 평균이 0이고 표준편차가 1인 표준정규분포로 표준화 된다.



그러면 기존 정규분포에서 구하고 싶었던 1이 나온 횟수가 115회 이상인 확률을 표준정규분포에서 구하는 법을 살펴보자.

x= 115 에 대해서 표준화된 값을 살펴보면 다음과 같다.



> Z = 115 - 100 / 제곱근_83.3 = 1.64



![정규분포_표준화](/Users/jang-eunjun/Desktop/TIL/DataAnalysis/PaperReview/Top2Vec_Distributed Representations of Topics/정규분포_표준화.jpeg)



이제 우리는 표준정규분포에서 1.64 이상의 면적을 구하면 된다.

(표준정규분포 면적 표가 존재, 이를 참조)

따라서 표준화된 표준정규분포를 보고 손쉽게 확률 계산을 할 수 있기 때문에 정규분포를 표준화 하려고 하는 것.



##### 3. t분포(t-distribution)

이제 t-sne를 자세히 살펴보기 위해서 t 분포(t-distribution) 을 살펴볼 것.

정규분포는 적은 표본에 대해서 신뢰도가 낮아지는 문제점이 발생한다.

따라서 t분포는 **적은 표본이 편향되는 문제점을 완화하기 위해 등장한 분포**다.



t분포 또한 평균이 0인 그래프이며 정규분포와 동일하게 좌우 대칭이고 종모양이다.

##### 하지만 꼬리쪽의 두께가 정규분포와 다른데, t분포의 꼬리는 t분포의 자유도에 따라서 달라진다. (여기서 일단 자유도를 표본의 개수라고 생각)

자유도(표본의 크기)가 가장 작은 t분포의 꼬리가 가장 넓다.

자유도(표본의 크기)가 가장 큰 t분포의 꼬리가 가장 얇다.

(자유도(표본의 크기) 가 작을수록 두껍고 클수록 얇다.)



표본이 적을 때, 즉 자유도가 작을 때 신뢰도가 낮아지는 문제를 해결하기 위해서 예측 범위를 꼬리쪽으로 갈수록 높이려는 것. 

그러면 자유도(표본의 크기)가 커질수록 꼬리 부분의 확률이 작아지면서(얇아지면서)

표준정규분포와 그 모양이 비슷해진다.

(당연한 이야기로,표본크기가 커질수록 정규분포에 근사해진다.)

실제로 표본의 크기가 30이상이면  t분포는 정규분포와 거의 유사하다고 판단하며 그때는 정규분포를 사용한다.

표본의 크기가 120이상이 되면 표준정규분포와 거의 유사해진다.



> 표본의 크기가 적을 때, 표준정규분포를 사용하면 신뢰도가 떨어지므로, 표본이 적을 때 신뢰도를 높이면서 사용하기 위해 꼬리쪽에 대해 두껍게 만든 t분포를 사용한다.
>
> 표본의 크기가 30이상이면 정규분포와 t분포가 거의 흡사해서 표준정규분포를 사용한다.
>
> 또한 t분포는 정규분포와 다르게 확률을 구하려는 게 아니라, 신뢰구간, 신뢰도, 가설 검정을 위해 사용된다.



우리가 t분포에 대해서 알아야 할 것들은 다음과 같다.



1. 30보다 적은 표본 크기에 대해서는 정규 분포보다 t 분포를 이용하여 예측하는 것이 더 정확하다.
2. t분포의 자유도는 표본 크기와 관련 있으며, 자유도가 작아지면 꼬리 부분의 예측 범위를 넓히기 위해서 꼬리 부분이 두꺼워진다.
3. 적은 표본으로 모평균을 추정하고 모집단이 정규분포를 따르는 것을 예측할 수 있지만, 모표준편차를 모를 때 t분포를 사용한다.



##### 4. 본론 t-sne



데이터의 **차원을 축소**하거나 **의미있는 특징을 추출**.

혹은 데이터를 가시화 하기 위한 manifold learning (???) 의 일종.



t-sne의 기본적인 원리는 고차원의 데이터간 거리를 저차원으로 축소하였을 때도 똑같이 유지하는 것. (데이터간 거리를 그대로 유지)

> 여기서 말하는 거리는 데이터간 유클리드 거리가 아니라 확률로써 나타낸다.



고차원의 데이터 : x1,x2,x3,x4,x5,x6

저차원의 데이터 : y1,y2,y3,y4,y5,y6



기준점을 x1이라고 했을 때, x1과 나머지 데이터들 (x2,x3,x4,x5,x6) 간의 거리를 우리는 모종의 방법으로 측정할 수 있다.

이때, 고차원에서의 데이터들간의 거리를 저차원으로 압축했을 때도 비슷하게 유지하게끔 학습 하는 것.

즉, 저차원에서 y1을 기준점으로 두고 나머지 데이터들(y2,y3,y4,y5,y6) 간의 거리를 측정했을 때, 앞서 x1을 기준으로 했던 결과와 유사하게끔 학습을 하는 것.



학습을 진행할수록 저차원에 무작위로 분포된 y1, y2, y3, y4, y5, y6 들이 실제 고차원에서의 거리와 유사하게끔 배치가 된다.



![t-sne원리](/Users/jang-eunjun/Desktop/TIL/DataAnalysis/PaperReview/Top2Vec_Distributed Representations of Topics/t-sne원리.jpeg)





##### 고차원에서의 데이터간의 거리(유사도)

그러면 어떻게 데이터간의 거리(유사도)를 구할 수 있을까?

결론적으로 고차원의 원래의 데이터는 정규분포를

저차원의 임베딩 된 데이터는 t 분포를 이용하여 데이터간의(유사도) 를 구한다.



고차원의 총 n개의 데이터에 대해, 특정 두 개의 데이터 x_i, x_j 의 **정규분포에 관한 거리** p_ij는 아래와 같이 정의된다.

(여기서 **p_ij는 정규분포에 관한 거리** 라는 점에 주의!)

>  p_ij = p_i|j + p_j|i / 2n



![스크린샷 2022-03-08 오후 7.02.06](/Users/jang-eunjun/Desktop/TIL/DataAnalysis/PaperReview/Top2Vec_Distributed Representations of Topics/스크린샷 2022-03-08 오후 7.02.06.png)



위의 분자 p_i|j + p_j|i  는 뭘 의미할까

i를 기준으로 j를 바라본 유사도를 p_i|j

j를 기준으로 i를 바라본 유사도는 p_j|i



p_i|j 와 p_j|i 를 구하는 식은 다음과 같다.



![스크린샷 2022-03-08 오후 7.06.15](/Users/jang-eunjun/Desktop/TIL/DataAnalysis/PaperReview/Top2Vec_Distributed Representations of Topics/스크린샷 2022-03-08 오후 7.06.15.png)



![스크린샷 2022-03-08 오후 7.06.08](/Users/jang-eunjun/Desktop/TIL/DataAnalysis/PaperReview/Top2Vec_Distributed Representations of Topics/스크린샷 2022-03-08 오후 7.06.08.png)





issue 발생



xi를 기준으로 바라보나 xj를 기준으로 바라보나 두 점간의 유사도는 항상 같아야할 것 같은데, p_i|j 와 p_j|i 는 다른 값을 가지게 된다.

##### 왜냐하면 기준을 어디로 삼느냐에 따라 다른 데이터 간의 거리의 분포가 달라지기 때문이다.

밑의 그림이 이를 설명





![데이터간 유사도 측정](/Users/jang-eunjun/Desktop/TIL/DataAnalysis/PaperReview/Top2Vec_Distributed Representations of Topics/데이터간 유사도 측정.jpeg)





우리는 두 점간의 거리가 어느 기준을 잡더라도 동일하길 원하기 때문에 각 두 확률 p_i|j 와 p_j|i의 평균값을 최종적인 두 점의 유사도로 정의한다. 따라서 분모가  2n이 되는 것.



##### 저차원에서의 데이터간의 거리(유사도)



저차원에서의 데이터간 거리는 t 분포를 이용하여 구한다.

저차원에 임베딩 된 데이터에 대해 특정 두 개의 데이터 yi와 yj의 **t분포에 관한 거리** q_ij 는 아래와 같이 정의된다.



![스크린샷 2022-03-08 오후 7.12.48](/Users/jang-eunjun/Desktop/스크린샷 2022-03-08 오후 7.12.48.png)



정규분포의 거리와는 다르게 yi, yj 중 어떤 기준을 잡더라도 거리가 같기 때문에 위의 식 그대로 사용한다.

##### 정의의 분모에서 kk와 ll이 서로 다른 데이터의 쌍에 대해 계산한 값을 모두 더하기 때문. 그리고 기준에 따라 달라지는 분산에 해당하는 σ2σ2 부분이 사라졌기 때문 (??? 이해가 안된다.)



##### 고차원에서 정규분포를 사용하는 이유



정규분포를 이용하여 들어가게 되는 𝞼_i, 𝞼_j가 중요한 역할을 하기 때문이다.

pij 를 구하는 공식이 qij와 비슷하게 만들어보고 x1을 이상치 데이터라고 가정하자.



![스크린샷 2022-03-08 오후 7.21.25](/Users/jang-eunjun/Desktop/스크린샷 2022-03-08 오후 7.21.25.png)



xi가 이상치 데이터이기 때문에 |xi - xj|^ 2은 커지게 되고 전체적인 pij값은 작아지게 된다.

그러면 qij 가 받는 영향도 작아지고 학습이 잘 일어나지 않게 된다.

즉, qij를 구하듯이 pij 를 구하게 되면 이상치에 대해 치명적이기 때문에 pij를 구하기 위해서 p_j|i와 p_i|j 를 각각 구해 각 데이터들에 대한 정보를 모두 잘 수용할 수 있도록 하는 것.

 

##### 저차원에서 t분포를 사용하는 이유



역시 마찬가지로 qij를 정규분포 거리인 pij와 비슷하게 정의 된다고 가정해보면 다음 그림과 같다.

![스크린샷 2022-03-08 오후 7.26.17](/Users/jang-eunjun/Desktop/TIL/DataAnalysis/PaperReview/Top2Vec_Distributed Representations of Topics/스크린샷 2022-03-08 오후 7.26.17.png)

1. 이렇게 qij 를 정의 하면 yi와 yj 가 거의 유사해서 분자는 무한대에 가까워진다.

   이를 방지하기 위해서 1을 더하여 최소한 1 이상인 값이 나오도록 만든 것.

2. 또한 t분포의 모양은 꼬리가 두꺼운 모양인데, yi와 yj가 초기에 너무 가깝게 배치되었다고 가정했을 때, 학습 중 gradient 를 크게 가져올 수 있다. (두꺼우니까 확률을 크게 가져올 수 있어서???)

   덕분에 빠르게 학습할 수 있다.

3. 고차원에서 가까웠던 두 데이터를 저차원에서 상대적으로 더 가깝게끔, 고차원에서 멀었던 두 데이터는 저차원에서 상대적으로 더 멀게끔 학습이 가능하다.

   ![data간 거리](/Users/jang-eunjun/Desktop/TIL/DataAnalysis/PaperReview/Top2Vec_Distributed Representations of Topics/data간 거리.jpeg)

   빨간색이 t분포, 검정색이 정규분포. 각 데이터간의 거리(유사도)를 확률로써 판단. 예를 들어서 x1과 x2의 거리는 x1일때의 정규분포의 확률 값과 x2일 때의 정규분포의 확률 값은 거리를 의미한다. **즉 데이터간의 거리를 세로축인 확률로 비교하고 두 확률이 같은 지점에 해당하는 가로축(x)이 데이터가 분포한 위치가 된다. **

   두 확률이 같은 지점에 해당하는 가로축을 y2와 x2를 예시로 들 수 있다. 세로축을 기준으로 y2와 x2는 비록 다른 확률 분포를 사용하고 있지만 확률 값이 같고 그 확률이 같은 지점에 해당하는 가로축의 지점이 고차원 저차원에서의 x1와 y2에 비해 떨어진 지점이 된다.

   그 결과 저차원에서의 거리(y1과 y2 사이의 거리)가 고차원에서의 거리 (x1, x2 사이의 거리) 보다 가까운 것을 알 수 있으며, 반대로 멀리 떨어진 데이터인 x3과 y3는 저차원 사이의 거리가 고차원 사이의 거리보다 먼 것을 알 수 있다.(먼 데이터는 저차원이 더 멀고, 가까운 데이터는 저차원이 더 가깝다.)

   **즉 저차원에서 t분포를 사용하면 가까운 지점은 더 가깝게 먼 지점은 더 멀게 배치가 되는 것을 알 수 있다.** => 데이터의 특징을 더 분명하게 해주기 때문에 결과를 보기에 더 효과적으로 보이게끔 해준다.



##### t-sne 장점



1. PCA(주성분 분석) 과 같은 선형 변환은 데이터를 저차원으로 임베딩하여 가시화할 때 겹쳐서 뭉게지는 부분이 많이 생겼지만 비지도 방법인 t-sne를 사용하면 많이 개선이 된다.
2. 저차원 임베딩을 t분포 기반으로 하기 때문에 기존 SNE(??) 방법에서 저차원 임베딩을 정규분포를 사용했을 때 나타나는 특정 거리 이상부터는 학습에 반영이 되지 않는 문제점을 해결한다.
3. 다른 차원 축소 알고리즘에 비해서 하이퍼파라미터 영향이 적고 이상치에 둔감하다.



##### t-sne 단점



1. 시간이 오래 걸린다. 특정 기준 데이터를 정하고 나머지 데이터에 대해 확률로써 거리를 계산.
2. 또한 너무 높은 차원을 가진 데이터를 바로 2,3 차원으로 축소하는 것이 불가능하다.
3. 저차원으로 임베딩되어 가시화 된 결과에서 각 데이터간의 거리는 실제 데이터 간의 거리를 의미하지 않는다. 데이터들의 군집의 크기 역시 의미가 없다.
4. 저차원으로 임베딩 되는 과정에 어쩔 수 없이 정보 손실이 발생, 데이터 왜곡이 일어난다.
5. t-sne를 사용하면 맨 처음 랜덤으로 저차원 데이터가 분포가 되기 때문에 **매번 돌릴 때마다 결과** 가 다르게 나타난다.
6. 큰 데이터셋에 대해 t-sne를 사용하면 특징의 구분이 어렵고 명확해지지 않는다.



<hr>

##### Umap(Uniform Manifold Approximation and Projection)



UMAP 은 t-sne의 많은 문제점을 해결한다.

umap 의 컨셉도 서로 먼 데이터는 더 멀리 가까운 데이터는 더 가깝게 하려고 하는 t-sne방식과 유사하지만 그 수학원 원리는 전혀 다른 원리를 사용한다.



##### UMAP의 그래프를 형성하는 과정은?



고차원의 데이터 구조를 잘 형성하기 위해 "fuzzy simplicial complex" 를 구성한다.

(모호한 단순 복합체)


![umap_circle](/Users/jang-eunjun/Desktop/TIL/DataAnalysis/PaperReview/Top2Vec_Distributed Representations of Topics/umap_circle.jpeg)



윗줄은 extent 크기를 다르게 했고 밑에 줄은 n_nearest 크기를 다르게 했다.

extent 크기는 데이터의 주변의 범위 즉, 반지름 크기를 의미하고 값을 크게 하면 반지름의 크기가 증가하고 작게하면 반지름의 크기가 작아진다.

또한 n_nearest 는 값이 커질수록 각 데이터가 연결된 다른 데이터의 수가 커진다.

##### UMAP 에서는 데이터를 감싸고 있는 원이 만나는 두 데이터는 연결되기 때문에 각 데이터를 둘러싼 원의 크기는 매우 중요한 역할을 한다. 원이 너무 커지면 모든 데이터가 연결되고 원이 너무 작아지면 고립된 데이터가 많이 생겨나게 된다.

데이터의 크기는 extent와 각 데이터들의 n-nearest 데이터의 거리를 바탕으로 모종의 규칙을 적용하여 정해진다.

만일 모든 데이터에 대해 원이 커지게 되면 모든 데이터가 연결되는 문제가 발생하기 떄문에 **fuzzy 개념** 을 이용해서 이를 해결한다. **원의 반지름이 너무 커지게 되면 경계를 모호하게 하여 각 데이터간의 연결을 제한한다.** 또한 이러한 규칙 때문에 **특정 데이터가 하나도 연결이 안되는 경우가 발생하는 걸 방지하기 위해서 가장 가까운 두 데이터는 무조건 연결**하도록 한다.

위와 같은 규칙으로 데이터를 연결한 후, 가까운 데이터들은 연결된 정도가 강하게, 먼 데이터는 연결된 정도가 약하게 설정된다. (위 그림에서 연결된 edge 의 두께를 통해 확인할 수 있다.)

모호한 단순 복합체는 결국 weighted graph 라고 볼 수 있다.



##### Umap 의 수학적 이론



위에서는 umap 의 기본 원리에 대해 전체적으로 살펴보았다면 좀더 깊게 들어가보자.

토폴로지 조합을 나타낼 수 있는 Čech complex 를 기반으로 한다.

여기서 토폴로지(topology) 란 위상수학을 의미하며, 수학 학문 중 꽤 최근에 등장한 학문이다.

(토폴로지 => 위상 => 구조라고 생각하자)

##### Čech complex 에 도달하기 위해서 simplex 개념 사용.

기하학적으로 k-simplex 란 k+1 개의 점으로 이루어진 블록이라는 뜻.

ex) 0 simplex 는 한 개의 점, 1-simplex 는 2개의 점으로 이루어진 선, 2-simplex 는 삼각형 블록



우리는 여러 데이터를 이어 simplex 를 만들 수 있으며, 여러개의 simplex를 결합해서 전체 데이터가 분포된 토폴로지(구조) 를 알 수 있다. 이는 Čech complex 를 형성할 수 있게 되고 이를 지표삼아 데이터 구조가 어떠한지 판단할 수 있게 된다.



![simplex image](/Users/jang-eunjun/Desktop/TIL/DataAnalysis/PaperReview/Top2Vec_Distributed Representations of Topics/simplex image.png)



그렇다면 Čech complex 는 무엇인가?

##### Čech complex는 point cloud를 이용하여 기하학 정보(토폴로지 정보)를 표현하는 simplical complex(단순 복합체)다.

쉽게말하면 각 데이터의 원을 이용해서 데이터의 구조(기하학 정보)를 표현하는 단순 복합체이다.



위에서 "fuzzy simplical complex" 를 단순 복합체라고 해석될 수 있다고 했을 때, 여기서 의미하는 단순 복합체가 simplical complex 다.





![Čech complex](/Users/jang-eunjun/Desktop/TIL/DataAnalysis/PaperReview/Top2Vec_Distributed Representations of Topics/Čech complex.jpeg)



위의 그림의 가장 왼쪽 그림처럼 데이터가 분포되어 있을 때, 이 데이터의 토폴로지(구조) 를 어떻게 표현할 수 있을까?

𝞮 에 따라서 데이터의 토폴로지(구조)는 달라진다.

𝞮이 5일 경우와 8일 때 point cloud를 표현한 그림은 위와 같다.

각 데이터의 원(cloud)이 만나는 데이터를 연결한 것이 Čech complex 다.

따라서 Čech complex 은  𝞮 크기에 따라서 달라진다. (𝞮 크기가 커지면 cloud 가 커지게 되고 데이터간 연결될 가능성이 커지면서 Čech complex가 바뀌기 때문. )

마지막으로 Čech complex를 형성하는 것은 바로 위에서 언급한 k-simplex 들이다.

각 데이터를 연결하여 여러 simplex 를 만드는 edge의 두께는 원(cloud) 가 많이 겹치게 되면 두껍게, 덜 겹치면 얇게 표현된다. (원이 겹치는 부위를 이야기 하는 것 같다.)



Čech complex를 형성하기 위해 simplex 를 만드는 과정에서, cloud 의 크기는 중요하다.

너무 크게 되면 의미있는 토폴로지가 나타나지 않고, 너무 작게 되면 연결이 하나도 되지 않는 점이 많이 생기게 된다.

그래서 umap 의 원리에서 봤듯이 적절한 데이터의 토폴로지를 만들기 위하여 각 데이터의 **n-nearest 데이터 거리와 extent를 바탕으로 모종의 규칙을 통해서 데이터 별로 다 다른 크기의 원을 가지게끔 해주었다.**



하지만 모든 원의 크기를 다 다르게 한다고 하여도 cloud 의 크기가 너무 커지게 되면 모든 데이터가 연결되는 것은 불가피하다. 따라서 원의 반경에도 fuzzy 개념 도입, 즉 원의 반지름이 너무 커지게 되면 경계를 모호하게 하여 각 데이터간의 연결을 제한한다. (원이 커져서 멀리있는 원과 만나더라도 연결될 확률을 낮추는 개념을 도입) 

그리고 fuzzy 때문에 하나도 연결 안된 데이터가 남아있지 않도록 가장 가까이 있는 데이터는 fuzzy 와 상관없이 연결하게 한다.



그렇다면 이상적인 원의 크기는 무엇인가?



![decide size of a circle](/Users/jang-eunjun/Desktop/TIL/DataAnalysis/PaperReview/Top2Vec_Distributed Representations of Topics/decide size of a circle.jpeg)





1번 그림은 데이터가 일정한 간격으로 잘 분포가 되어 있어서 데이터마다 원의 크기를 다르게 해줄 필요가 없다.

2번 그림은 데이터의 분포가 각각 다르기 때문에 cloud의 크기를 데이터마다 다르게 설정한다.

3번 그림은 cloud 크기에 따른 issue 를 없애기 위해서 fuzzy 개념을 도입한 그림이다.

4번 그림은 이 세 가지의 것을 모두 고려해서 나타난 그림이다.



고차원에서 데이터의 토폴로지(구조) 를 파악하기 위해서 Čech complex를 형성하고, 이를 "fuzzy simplical complex" 라고 부른다.

이와 같은 수학적 원리로 고차원의 데이터의 토폴로지(global 구조) 를 형성한 후, 저차원으로 임베딩 된 데이터에서도 고차원의 토폴로지와 동일하게 유지가 되게끔 학습하는 방법이 UMAP 방법.



##### UMAP 이 빠른 이유



이 모델은 각 데이터에 대해 가까운 몇 개의 이웃 데이터에 대해서만 계산을 한다.

왜냐하면 fuzzy 이론 떄문에 데이터 중심에서 멀리 떨어질수록 결합확률이 0에 가까워진다.

각 데이터에 대해 가까운 몇 개의 이웃 데이터에 대해서만 계산을 수행하여도 토폴로지(gloabal 구조) 를 잘 학습한다. 즉 negative sampling 을 수행하여 모든 데이터 연결 고리에 대해 하지 않는다.

=> 멀리 떨어진 데이터를 다 버리는 게 아니라, 몇 개는 계산한다.

> (Negative sampling 예시: 예를 들어 모델이 queen이라는 단어에 대해 학습한다고 할 때, 모델은 queen과 유사한 단어와 유사하지 않은 단어들을 모두 학습하여 queen이란 단어의 의미를 파악할 것입니다. 이때 queen과 유사한 단어는 girl, woman과 같이 적은 반면에, queen과 관련이 없는 단어는 car, cookie, book, house, country, university 등 수많은 단어가 존재합니다. 우리는 유사한 단어에 대해서는 모두 학습을 하지만, 모델의 학습 시간을 줄이기 위해 수많은 유사하지 않은 모든 단어를 학습하지 않고 몇 개를 추려서 car, cookie, house 대해서만 학습을 하게 됩니다. 이렇게 유사하지 않은 수많은 단어 중 몇 개만 추려서 학습하는 것을 "negative sampling하여 학습한다"라고 표현합니다.) from blog



##### UMAP 의 단점



1. 하이퍼 파라미터의 영향을 많이 받는다.
2. t-sne의 문제점과 유사하며, 저차원으로 임베딩되어 가시화 된 결과는 실제 데이터 간의 거리를 의미하지 않는다.
3. 저차원으로 임베딩 되는 과정에 어쩔 수 없이 정보 손실이 발생하며 데이터 왜곡이 일어난다.

<hr>



#### Find Dense Cluster of Documents



군집화에 기반한 밀집화의 목적은 유사성 공간에서 유사성이 높은 문서들을 찾는 것이다.



doc들이 다양한 밀집지역을 의미 공간에서 가진다는 점이 문제점이다.

추가적으로 doc들이 유사하지 않은 희소 지역이 존재한다.



이것들은 noise 로 여길 수 있다. => 눈에 띄는 주제가 없다.





HDBSCAN 이 doc 들의 밀집 지역을 찾는데 사용된다.

HDBSCAN 은 각각의 doc밀집 지역에 label 을 부여한다. 그리고 밀집지역에 속해있지 않은 doc벡터들 모두에 대해서 noise label을 붙인다.

여기서 dense area 로 임명받은 doc 벡터들은 토픽 벡터를 계산하는데 사용될 수 있다.

근데 noise label이 붙은 doc들은 토픽 벡터로 간주되지 않는다.



![스크린샷 2022-03-06 오후 7.33.02](/Users/jang-eunjun/Desktop/TIL/DataAnalysis/PaperReview/Top2Vec_Distributed Representations of Topics/스크린샷 2022-03-06 오후 7.33.02.png)



이 그림은 umap 차원 축소를 이용한 벡터들을 이용해서 HDBSCAN 군집화를 이용한 그림이다.

빨간색은 noised documnets 들로 분류가 되고

다른 색깔들이 dense area 로 임병받은 doc벡터들이라고 할 수 있다.



HDBSCAN 의 하이퍼 파라미터는 최소한의 군집의 크기이다.

15개로 지정. 크기가 그니까 관련없는 doc들도 포함하는 경우가 있었다고 한다.





### Calculate Topic Vector



#### Calculate Centroids in Original Dimensional Space



UMAP 과 HDBSCAN 을 사용한 거는 라벨을 붙이는 과정과 같다.

의미론적 임베딩 공간에서 각각의 doc 들은 noise label 이거나 밀집된 cluster 에 속하는 label로  분류된다.





밀집된 doc들의 각각의 군집에 대해 라벨을 붙이는 것을 통해서 토픽 벡터들은 계산될 수 있다.

doc 벡터들에 의해서 토픽 벡터가 계산될 수 있는 방법은 여러가지다.

가장 간단한 방법은 중심을 계산하는 것.(같은 밀집 군집에서 모든 doc 벡터들의 산술 평균을 구하는것)

다른 합리적인 방법으로는 기하학적 평균이나, HDBSCAN 을 통해서 만들어진 군집의 신뢰도로부터 나온 확률들을 사용한 방법이 있다.



세 가지 방법을 실험한 결과 모두 다 비슷한 토픽 벡터들을 만들어 냈고, 그 토픽 벡터 주변의 단어들도 비슷하게 도출됐다.

우리는 이게 높은 차원 공간의 희소성 때문이라고 추측했다.

따라서 논문 연구원들은 간단한 중심 계산 법을 사용했다.



![스크린샷 2022-03-06 오후 8.07.41](/Users/jang-eunjun/Desktop/TIL/DataAnalysis/PaperReview/Top2Vec_Distributed Representations of Topics/스크린샷 2022-03-06 오후 8.07.41.png)



docs들의 중심이 토픽 벡터로 계산됐다. (HDBSCAN 으로 인지된 문서의 밀도의 중심)

각각의 밀집된 doc들에서 토픽 벡터들이 만들어지는데, 따라서 그 밀집의 개수가 corpus 내에서의 눈에 띄는 토픽들의 개수로 간주될 수 있다.





#### Find Topic Words



의미론적 공간에서 모든 point 들은 그것의 가장 가까운 word 벡터들에 의해서 의미론적으로 가장 잘 설명된 토픽을 나타낸다. (주변 단어들에 의해서 의미론적으로 설명되는 토픽 = point) 따라서 토픽 벡터와 가장 가까운 단어는 그 토픽 벡터를 의미론적으로 가장 잘 표현한다고 할 수 있다.

그리고 각 word 벡터들과 토픽 벡터와의 거리는 얼마나 그 단어가 토픽과 유사한지를 나타낸다.

그리고 토픽 벡터와 가장 가까운 word 벡터들은 Dense 지역 내의 모든 docs들과 매우 유사한 단어들로 여겨질 수 있다.

이 단어들은 dense 지역의 docs 들의 공통 주제를 요약하는데 사용될 수 있다. (토픽 벡터와 가장 가까운 단어들을 이용해서..)



이 공통 단어들은 대부분ㄷ의 docs들에서 등장하고 그들은 종종 모든 docs들과 동일한 거리에 위치하는 지역에 존재한다.

결과적으로 토픽 벡터와 가장 가까운 단어들은 stop-words 로 지정되지 않을 것이기 때문에 top2vec 에서는 따로 stop-words 에 대한 작업을 할 필요가 없다.



### Topic Size and Hierarchical Topic Reduction



토픽 벡터와 doc 벡터들은 토픽의 사이즈가 계산 되게끔 만든다.

주제 벡터는 각 문서 벡터가 가장 가까운 주제 벡터에 속하도록 문서 벡터를 분할하는데 사용할 수 있다.

이는 각각의 document들을 특정 하나의 토픽으로 연관짓게 만든다. (그 특정 하나의 토픽은 document 와 가장 유사한 토픽임.)

##### 각각의 토픽의 크기는 그 토픽 벡터에 속하는 docs들의 숫자로 측정된다.



계층적으로 주제의 사이즈를 줄일 수 있다.

이건 희망하는 숫자의 토픽 개수가 도달할 때까지 반복적으로 가장 작은 토픽(사이즈가 가장 적은 토픽)을 그것과 가장 유사한 토픽에 합병하면서 진행된다.

가장 작은 토픽의 토픽 벡터와 그것의 가장 가까운 토픽벡터들 간의 가중 산술 평균을 취하면서 이루어지며 각각의 토픽 사이즈에 따라 가중치가 부여된다.

주제간 병합후에 주제의 사이즈도 다시 계산된다.



코퍼를 가장 대표하는 주제를 찾는 이점이 있다.



## Results



토픽 모델들을 가장 잘 평가하는 자연적인 방법은 그 docs들을 얼마나 잘 설명하는지에 대해 점수를 부여하는 것이다.

이 평가는 주제가 사용자에게 얼마나 유익한지를 평가한다.

우리는 그 모델의 토픽 단어들에 의해서 묘사될 때 그 docs 들에 관해 얻은 정보를 측정하는 "mutual information" 을 사용한는걸 주장한다.



전통적인 토픽 모델링 방법들은 토픽 공간을 이산화 시키고 docs들을 그것의 토픽들의 합성으로 묘사한다.

documents D로부터 산출된 토픽들의 집합 T를 평가하기 위해서,  주제 모델에 의해 주어진 주제들의 비율로 설명될 때, 전체 획득된 정보는 각각의 doc에 대해서 계산되어진다.



대조적으로 top2vec은 주제를 지속적으로 표현하고, 주제에 해당하는 문서를 해당 공간에 배치한다.

top2vec에 의해서 발견된 주제 벡터는 docs 그룹의 공통적인 주제 혹은 docs들의 개별 주제에 대한 평균을 나타낸다.

Docs D에서 생성된 top2vec 토픽 set에 대해서 평가하기 위해, docs(벡터)들은 하위 집합으로 분할되고 각 하위 집합은 가장 가까운 토픽 벡터가 동일한 doc 벡터들에 해당한다. => 따라서 각 doc들은 정확히 하나의 주제에 할당된다.

##### 주제 벡터에 가장 가까운 단어로 문서가 설명될 때, 각 문서의 하위 집합에 대해 얻은 총 정보가 측정된다.









### Topic Information Gain



=> 잘 모르겠음



### LDA, PLSA and Top2Vec Topic Information Gain



top2vec이 주제의 개수를 자동으로 찾는 탓에, LDA, PLSA 의 주제의 개수를 top2vec이 찾은 개수만큼 늘려가면서 비교한다.

세 모델을 비교하기 위해서 토픽의 개수도 동일하게 맞추고, 그 토픽 내의 단어들의 상위 개수도 맞췄다.



#### 20 New Groups Dataset



=>다 좋은 성능을 냈다.



## Discussion

#### 

top2vec

의미론적 공간은 연속적인 토픽 표현이다. 그리고 그것은 비슷한 docs들끼리 밀집된 지역으로부터 토픽 벡터, 토픽 사이즈 그리고 위계적인 토픽 축소를 계산하는 것을 허용한다.

또한 top2vec 모델은 의미론적 공간의 거리에 기반해서 단어들과 docs들 그리고 토픽들간의 비슷함을 비교해준다.

그리고 토픽 모델을 평가하는 새로운 평가법을 내놓는데, mutual information 을 사용해서 문서의 주제가 얼마나 유익한가를 계산한다.

토픽 정보 획득은 doc의 토픽 단어로 설명될 때(doc에 대해서 토픽 단어로 설명할 때), doc에 대해 얻은 정보의 양을 측정한다. 이렇게 하면 주제 단어의 품질이 측정되고 문서에 주제를 할당할 수 있다.

논문 연구의 결과는 top2vec이 다양한 크기의 주에와 상위 주제 단어 수에 대해서 전통적인 모델보다 더 유익하고 corpus 를 대표하는 주제를 일관되게 발견하는 것을 보여준다.





top2vec이 전통 모델에 비해서 가지는 가장 큰 장점은 토픽의 수를 자동으로 결정해준다는 점이다.

그리고 더 정보력이 있고 corpus 에 대해서 표현력있는 토픽을 찾는 것이다.

그리고 stop-word 같은 것들을 찾을 필요가 없기 때문에 모델을 굴리는데 편하고 쉽다.

전통적인 모델들의 bow 방식의 단점인, 단어의 의미를 파악하지 못한다는 점을 극복한다.(word2vec 사용하니까>)



전통적인 모델들은 오리지날 docs들의 단어 분포를 재현하는 토픽을 찾으려고 한다.
이는 정보력이 없는 단어들도 전통적인 모델에 포함하게 한다. (단어 빈도수가 쓸데없이 높은 거)



하지만 top2vec은 그것의 learning task 때문에 훨씬 더 정보력있다.

(단어가 나온 문서를 예측하는 task, 그리고 그건 doc벡터들이 doc에 대해 가장 많은 정보력을 가진 word 벡터와 가깝게 위치시킨다.)

블라블라 여튼 더 좋다.

































