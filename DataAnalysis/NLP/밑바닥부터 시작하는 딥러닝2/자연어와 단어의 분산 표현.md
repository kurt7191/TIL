# 자연어와 단어의 분산 표현



## 자연어 처리란?



한국어와 영어 등 우리가 사용하는 언어를 자연어(Natural Language) 라고 부름.

자연어 처리 : Natural Language Proccessing(NLP)



"우리의 말을 컴퓨터에게 이해시키키 위한 기술(분야)"



### 단어의 의미



문장안에 여러 단어들이 포함되어 있음.

단어는 의미의 최소 단위.



(이번장)

***"컴퓨터에게 단어의 의미 이해시키기"  , 다른 말로 정확히 말하면 "컴퓨터가 단어의 의미를 잘 파악하는 표현 방법"***



구체적인 이번장과 다음장에서 알아볼 기술



> 1. 시소러스를 활용한 기법(이번)
> 2. 통계 기반 기법 (이번)
> 3. 추론 기반 기법(word2vec) (다음)



<hr>

## 시소러스



시소러스 방법(표현)을 이용해서 자연어 단어를 컴퓨터가 알아들을 수 있게 표현.



1. 시소러스
   - 유의어 사전
   - 기본적으로 뜻이 같은 단어나, 유사한 단어들이 하나의 그룹으로 분류됨.
2. 위계를 가지는 시소러스
   - object > motor vehicle >(car, go-kart, truck) -> car에서 분파되는 가지 (SUV, compact, hatch-back)





모든 단어에 대해서 유의어 집합을 만들고, 단어들의 관계를 그래프로 표현하여 단어 사이의 연결을 정의할 수 있다.

즉, 단어 네트워크(그래프) 를 통해서 단어간의 연결과 관계를 정의 내릴 수 있다.

이를 컴퓨터에게 가르치면 단어의 의미를 이해시켰다고 할 수 있다.



### wordNet



자연어 처리에서 가장 유명한 시소러스는 wordNet.

wordNet을 사용하면 유의어를 얻거나, 단어 네트워크를 이용해 단어간 유사도를 구할 수 있다.



### 시소러스의 문제점



1. 시대 변화에 따라서 단어의 의미가 새로 생기거나 변화한다.
   - 영어의 heavy 는 과거에 무겁다라는 의미만 있었고 현재는 어렵다라는 의미를 표현하기도 한다.
2. 사람을 쓰는 비용이 크다.
3. 단어의 미묘한 차이를 표현할 수 없다.
   - 빈티지와 레트로는 의미는 동일하지만 미묘한 표현 차이가 있다. 시소러스는 이를 표현할 수 없다.





시소러스의 이러한 문제점들을  해결하기 위해서 '통계 기반 기법'과 '신경망을 사용한 추론 기반 기법'을 알아볼 것.



1. 시소러스의 문제점을 타파하기 위한 두 가지 방법

- 통계 기반 기법
- 추론 기반 기법(신경망 이용)



이 두 기법은 대량의 텍스트 데이터로부터 "단어의 의미" 를 자동으로 추출한다.

이 덕에 사람이 순수 레이블링 하는 중노동에서 해방.(시소러스 해방)



<hr>



## 통계 기반 기법



말뭉치(Corpus) 사용

- 자연어 처리를 목적으로 수집한 대량의 텍스트 데이터
- 혹은 문서들의 집합이라고 표현



통계 기반 방법은 말뭉치로부터 자연어에 대한 지식을 자동으로 추출한다.

(말뭉치는 사람들이 적은 데이터이기 때문에, 자연어에 대한 정보가 자동으로 많이 담겨있다.)



>  *말뭉치 ->(추출) -> 자연어 지식 ※자동화*



### 파이썬으로 말뭉치 전처리하기



자연어 기본적인 전처리 : 텍스트 데이터에서 단어들을 추출하고, 그 단어들 하나하나에 ID를 부여하고 그 값들을 LIST로 변환하는 작업



*Text -> 단어1,단어2,단어3.... -> 단어1_id, 단어2_id, 단어3_id,..... -> [단어2_id, 단어3_id,.....]*



```python
#소문자로 전환

text = 'You say goodbye and I say hello'
text = text.lower()

#단어 단위로 나누기

words = text.split(' ')
words
```



단어 단위로 나누어져서 다루기 쉬워진 것은 맞지만 있는 그대로 사용하기란 불편하다.

따라서 각 단어에 ID 를 부여한다.

그리고 ID의 리스트로 이용할 수 있도록 리스트에 그 값들을 담아준다.



```python
#단어에 ID 붙이기

#각 단어에 id를 붙이고
#id가 value고 단어가 key 값인 딕셔너리를 만들고
#id가 key고 단어가 value인 딕셔너리를 만든다.

word_to_id = {}
id_to_word = {}

for word in words:
    if word not in word_to_id:
        new_id = len(word_to_id)
        word_to_id[word] = new_id
        id_to_word[new_id] = word
        
        
#word to id 딕셔너리를 array로 전환 (key : word, value는 id인 딕셔너리임)
import numpy as np
corpus = [word_to_id[w] for w in words]
corpus = np.array(corpus)
corpus
```



위의 과정을 한 번에 볼 수 있게 함수화 해보겠다.



```python
#text lower
#text split으로 단어만 추출
#단어 하나하나에 id부여하기
#부여한 단어id들을 배열로 담아두기

def preprocessing(text):
    text = text.lower()
    text = text.replace('.',' .')
    words = text.split(' ')
    
    word_to_id = {}
    id_to_word = {}
    
    for word in words:
        if word not in word_to_id:
            new_id = len(word_to_id)
            word_to_id[word] = new_id
            id_to_word[new_id] = word
            
    corpus = [word_to_id[word] for word in words]
    corpus = np.array(corpus)
    
    return corpus, word_to_id, id_to_word
    
```



이 함수를 이용해서 한 번에 return 값 받기



```python
text = 'You say goodbye and I say hello.'
corpus, word_to_id, id_to_word = preprocessing(text)
print(corpus)
print(word_to_id)
print(id_to_word)
```



### 분포 가설



단어를 벡터화 시키려는 노력은 계속 있어왔다.

중요한 기법의 거의 모두가 단 하나의 `간단한 아이디어` 에 뿌리를 두고 있다.



> 단어의 의미는 주변 단어에 의해 형성된다.



이를 `분포 가설(distribution hyphothesis)` 라고 부른다.



단어 자체에는 의미가 없고, 그 단어가 사용된 맥락(context)이 의미를 형성한다.

다시 의미를 이해하기 위해서 타이핑하면서 적어보면

단어 그 자체에는 의미가 없으며, 단어가 사용이된 맥락이 단어의 의미를 부여한다.

물론 의미가 같은 단어들은 같은 맥락에서 더 많이 등장.(선 맥락, x에 그 맥락에 따라 같은 단어 똑같이 더 많이 등장)



ex) I drink coffee, we drink beer



drink 가 사용이 됐는데, drink 단어 주변에는 음료가 등장하기 쉽다. 

즉, x 빈칸이 있고 그 주변에 액체가 있으면 drink 가 등장할 가능성이 커진다.



그리고



ex) we guzzle beer, we guzzle wine



똑같은 맥락에서 drink 말고 guzzle 이 사용이 됐는데, 이럴 시 우리는 guzzle 과 drink 가 비슷한 단어라는 걸 알 수 있다.

즉, 주변 맥락에 따라서 어떤 단어가 나올 수 있는데, 똑같은 맥락에서 어떤 단어들이복수 사용 가능하면, 그 단어들  끼리는 유사성이 있음을 알 수 있다.



> 같은 맥락에서 사용된 단어들은 서로 유사성이 있다.



앞으로 이 책에서는 맥락 이라고 하면 주변 단어들을 일컫는다.

다시, 맥락이란 특정 단어를 중심으로 둔 주변 단어들.



### 동시발생 행렬



이런 분포 가설을 기초해서 단어를 벡터화 하는 방법을 생각해보자.

(분포가설 : 맥락에 따라서 단어의 의미가 정해진다.)



어떤 단어에 집중했을 때, 그 주변에 어떤 단어가 몇 번이나 등장하는지 세어 집계하는 방법

집계해서 행렬을 만들면 이를 `co-occurrence-matrix` 라고 칭한다.



"You say goodbye and i say hello"

가 있다고 할 때,

You 주변 단어는 say 하나 뿐. 그리고 1 빈도를 기록

say 주변 단어는 you, goodbye, I, hello 이고 you : 1, goodbye : 1, I : 1, hello : 1 빈도 기록



```python
#동시행렬 만드는 자동화 함수

def create_co_matrix(corpus, vocab_size, window_size = 1):
    corpus_size = len(corpus)
    co_matrix = np.zeros((vocab_size, vocab_size), dtype=np.int32)
    
    for idx, word_id in enumerate(corpus):
        for i in range(1, window_size + 1):
            left_idx = idx - i
            right_idx = idx + i
            
            if left_idx >= 0:
                left_word_id = corpus[left_idx]
                co_matrix[word_id, left_word_id] += 1
                
            if right_idx < corpus_size:
                right_word_id = corpus[right_idx]
                co_matrix[word_id, right_word_id] += 1
                
    return co_matrix
    
```

