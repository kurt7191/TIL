# 다른 언어에 BERT 적용하기



BERT 를 영어가 아닌 다른 언어에 적용할 수 있을까?

M-BERT(multilingual - BERT) 는 영어 이외의 다른 언어에서도 표현을 계산한다.



- multilingual - BERT 이해하기
- multilingual - BERT 의 다국어 특징
- XLM
- XLM - R 이해하기
- 언어별 BERT



## M-BERT 이해하기



BERT는 본래 영어 텍스트만 사용 가능하다.

프랑스어나 스페인어는 어떻게 해야할까? M-BERT 사용한다.



M-BERT 로 영어를 포함한 다른 언어들의 표현을 얻을 수 있다.

본래의 BERT 는 영어 위키피디아와 토론토 책 말뭉치를 가지고 MSM, NSP 로 학습되었다.

M-BERT 는 영어 위키피디아뿐 아니라 104개 언어의 위키피디아 텍스트로 학습되었다.



어떤 언어는 특정 언어보다 자료의 수가 더 적을 수 있다. 예를 들어서 스와힐리어에 비해서 영어는 더 많은 자료를 가지고 있다.

만일 이 데이터 셋으로 학습을 시키면 과적합(Overfitting) 문제가 발생할 수 있다.

이 issue 를 해결하기 위해서 샘플링 방법을 사용해야 한다.

자료가 많은 언어를 언더 샘플링(undersampling) 하고 자료가 적은 언어를 오버샘플링(oversampling) 한다.



M-BERT 는 104개 언어의 위키피디아 텍스트를 사용해 학습했으므로 여러 언어의 구문 구조도 파악한다.

104개 언어에 걸쳐 모두 11만 개의 워드 피스로 구성된다.

BERT모델을 학습시킨 것과 동일한 방식으로 학습시켰다.



사전 학습된 M-BERT 모델은 구글에서 오픈 소스로 공개했다.

- BERT-base, multilingual- cased (대소문자 구분)
- BERT-base, multilingual- uncased(소문자만)



두 모델 모두 12개의 인코더 레이어, 768의 은닉 크기로 구성. 총 1억 1천만 개의 변수로 이루어져 있음.





### NLI 태스크 평가로 알아보는 M-BERT 평가하기



NLI 태스크에 M-BERT 를 파인 튜닝하고 평가를 해보자.

NLI 태스크는 전제와 가설을 비교해서 레이블을 판단하는 건데 레이블의 종류는 함의(진실), 모순(거짓), 미결정(중립) 이다.



NLI 태스크를 위한 데이터 셋



- 스탠포드 자연어 추론(일반적) - SNLI
- 교차 언어 자연어 추론 (M-BERT 에서 사용) - XNLI
- 멀티 장르 자연어 추론 - MultiNLI, SNLI 와 유사한 말뭉치로 이루어져 있다.



XNLI 데이터셋은 MultiNLI 를 확장한 데이터셋이다.



데이터셋 이해를 했으니 다음으로 M-BERT 평가법을 살펴보자.

사전 학습된 M-BERT 를 XNLI 데이터셋을 이용해서 NLI 태스크에 맞춰 파인 튜닝한다.



##### 제로샷(Zero - shot)



제로샷은 영어로 만들어진 학습셋으로 NLI 태스크에 맞춰 M-BERT 를 파인 튜닝한다.

그리고 테스트 셋을 이용해서 각 나라의 언어로 모델을 평가한다.

**모델을 영어에만 파인 튜닝하고 다른 언어로는 파인 튜닝하지 않았기 때문에 제로샷이라고 부른다.**

이 방법은 M-BERT 의 교차 언어 능력을 이해하는데 도움이 된다.



- 파인 튜닝 : 영어 학습셋으로만, NLI 태스크에 맞춰서
- 평가 : 모든 언어 테스트셋으로 평가



##### 번역 - 테스트



번역 - 테스트(TRANSLATE - TEST)

영어 학습 데이터셋을 이용해서 NLI 태스크에 맞춰서 BERT 모델을 파인 튜닝하고 영어로 번역된 테스트셋에서 평가한다.



- 파인 튜닝 : 영어 학습셋
- 평가 : 영어로 번역된 테스트셋



##### 번역 - 학습



학습셋을 본래 영어인데 다른 언어로 번역된 데이터 셋을 이용한다.

파인 튜닝에 번역된 학습셋을 사용하고 평가에는 모든 언어로 구성된 테스트셋을 이용한다.



- 파인 튜닝 : 영어에서 다른 언어로 번역된 학습셋
- 평가 : 모든 언어 테스트셋



##### 번역-학습-모두



학습셋을 영어에서 모든 다른 언어로 번역.

학습셋은 영어에서 모든 다른 언어로 번역한다.

학습셋 번역된 문장 쌍.

학습셋으로 M-BERT 파인 튜닝. 테스트셋에 포함된 모든 언어로 모형을 평가한다.



- 파인 튜닝 : 영어에서 모든 다른 언어로 번역된 학습셋
- 평가 : 모든 언어 테스트셋





ZERO 샷을 포함한 다양한 방법들이 다국어에서 성능이 좋다.

어떻게 그럴 수 있는지 살펴보자



## M-BERT 는 다국어 표현이 어떻게 가능한가?



### 어휘 중복 효과



M-BERT 의 다국어 지식 전이가 어휘 중복에 영향을 받는지 알아본다.



M-BERT 모델은 하나의 언어로 파인 튜닝하고 다른 언어에서 파인 튜닝된 M-BERT 모델을 사용할 수 있다. (ZERO샷)

그런데 M-BERT 의 ZERO샷 지식 전이는 어떻게 이루어질까?/



가설을 세워보자. ZERO샷 지식 전이가 가능한 이유가 언어 간에 중복되는 어휘 때문이라고 해보자.

그러면 M-BERT의 정확도는 어휘가 많이 중복되는 언어에 크게 영향을 받을 것이다.  (만일 영어와 독일어가 중복되는 어휘가 많다면 이 둘에 관한 M-BERT 모델의 정확도는 높은 것이다.)

실험을 통해서 살펴보자.

하나의 언어로 M-BERT를 파인튜닝하고 다른 언어로 평가한다.

파인 튜닝 언어로 워드피스 토큰을 E_train 으로 표시하고 평가 언어로 워드피스 토큰을  E_eval 로 표시한다.

파인 튜닝 및 평가 언어 간에 겹치는 워드피스 토큰을 다음과 같이 계산할 수 있다.



overlap = |E_train (교집합) E_eval | /  |E_train U E_eval|



먼저 하나의 언어로 파인 튜닝한 다음에 다른 16개 언어로 평가해서 각각의 F1 점수를 얻는다.

그러면 어휘 중복이 적은 경우에도 F1점수가 높게 나오는 걸 확인할 수 있는데 이를 통해서 어휘 중복이 많은 것과 무관하게 제로샷 성능이 잘 나온닥 이야기 할 수 있다.

따라서 가설을 틀린 것을 간주할 수 있다.

오히려 M-BERT 가 다른 언어와의 관계성을 고려해 일반화를 잘 하는 것이라고 말할 수 있다.

(모델이 단순 어휘를 암기하는 것이 아니라 다국어 표현을 더 깊게 학습한다는 것을 의미한다.)



### 스크립트에 대한 일반화



이제 M-BERT 가 서로 다른 언어에도 일반화해 적용할 수 있는지 살펴보자.

먼저 태스크를 정해보자. 태소 분석(Pos tagging) 태스크를 한다고 가정한다.

우르두어를 이용해 형태소 분석을 위해 M-BERT를 파인 튜닝한다 그 이후 다른 언어로 파인 튜닝된 M-BERT를 평가한다.