# BERT 파생 모델



## BERT의 파생 모델 I: ALBERT, RoBERTa, ELECTRA, SpanBERT





ALBERT : BERT 의 라이트 버전 (BERT 아키텍처의 변화가 거의 없는 편)

RoBERT : BERT 파생 버전 중에 가장 많이 쓰이는 방법 중 하나. 사전 학습 단계에서 몇 가지 차이가 존재.

ELECTRA : 다른 파생 모델들과 달리 생성기(generator) 와 판별기(discriminator) 를 사용한다.

또한 사전 학습 과정에 교체된 토큰 판별 태스크(replaced token detection task) 라는 태스크를 사용한다.



SpanBERT :  SpanBERT는 질문-응답, 관계 추출 등과 같은 태스크에 널리 사용되고 있다.



- ALBERT
- RoBERT
- ELECTRA
- SpanBERT



<hr>

### ALBERT



BERT 모델의 문제는 변수가 너무 많다는 점. BERT-base 모델은 변수가 1억 1천만 개의 변수로 구성되어 있다.

이 문제를 해결하기 위해서 ALBERT 도입했다.



밑의 두 가지 방식을 통해서 변수의 숫자를 줄였다.

그리고 학습 시간을 줄였다.



- 크로스 레이어 변수 공유
- 팩토라이즈 임베딩 레이어 변수화



#### 크로스 레이어 변수 공유



크로스 레이어 변수 공유는 BERT 모델의 변수를 줄이는 방법.

BERT는 인코더 여러 개를 중첩했었다. 예를 들어서 BERT-base 는 12개의 인코더를 중첩했다.

따라서 모델의 학습이 진행되면 모든 인코더의 변수들이 학습 과정을 거치게 된다.

하지만 크로스 레이어 변수 공유 방법은 첫 번째 인코더만 학습하고 그 학습한 값을 다른 모든 인코더 레이어와 공유한다.



크로스 레이어 변수는 결국 계층간에 변수를 공유하는 것과 같은데 이러한 방법은 여러 가지가 있다.



- All-shared :  첫 번째 인코더의 하위 레이어에 있는 모든 변수를 나머지 인코더와 공유
- Shared feedforward network : 첫 번째 인코더 레이어의 피드포워드 네트워크의 변수만 다른 인코더 레이어의 피드포워드 네트워크와 공유
- Shared attention : 첫 번째 인코더 레이어의 멀티 헤드 어텐션의 변수만 다른 인코더 레이어와 공유



지금까지 BERT의 수많은 변수량 문제를 해결하기 위한 변수 공유 문제를 살펴봤다.

다음으로는 변수 감소(parameter reduction) 에 대해서 살펴보자.



#### 팩토라이즈 임베딩 변수화



BERT를 다룰 때 워드피스 토크나이저를 사용했었다. (사전에 없는걸 ##으로 나눠서 있을 때까지 쪼개서 토큰화 했던 것, 그리고 그 사전은 워드 피스 방식으로 만든 것)

워드피스 토크나이저를 사용해서 토큰을 만들고 임베딩을 하게 되면 임베딩 크기는 은닉 레이어의 크기와 같다.



은닉 레이어 임베딩 크기가 70000 X 768 이라고 할 때, 워드피스 임베딩의 크기도 70000 X 768 이 된다.

따라서 은닉 레이어의 임베딩 크기 H를 늘리게 되면 워드피스 임베딩의 크기 E도 커진다.



위와 같이 워드피스 임베딩 크기를 은닉 레이어 임베딩 크기와 동일하게 설정하면 학습해야 할 변수 역시 늘어나는 걸 알 수 있다. 따라서 임베딩 행렬을 더 작은 행렬로 분해하는 방법인 "팩토라이즈 임베딩 변수화" 방법을 사용했다.

Embedding 행렬을 더 작은 행렬로 분해하는 방법이라고 할 수 있다.

즉

V X H (단어의 수 X 차원의 수 ) 를 하는 대신에, V X E (단어의 수 X 축소된 차원의 수 ) 로 투영하고, 그 다음에는 낮은 차원의 임베딩을 은닉 공간 H에 투영한다. (E X H)



V X H -> V X E -> E X H



과정을 정리하면 아래와 같다.



- 먼저 사전의 원-핫 인코딩한 벡터 V를 저차원의 워드피스 임베딩 공간 E로 투영한다. (V X E). 이때 워드피스 임베딩의 차원은 V X E = 30,000 X 128 이 된다.
- 그다음 워드피스 임베딩 공간 E를 은닉 레이어 H로 투영한다.(E X H). 이때 차원은 E X H = 128 X 768 이 된다.

즉, V X H 대신 V X E 와 E X H 로 분해하는 것.



지금까지 크로스 레이어 변수 공유 방법과 팩토라이즈 임베딩 변수화 방법을 통해서 BERT의 변수를 줄일 수 있다.



#### ALBERT 모델 학습



BERT 의 경우 MLM과 NSP 태스크를 통해 사전 학습을 진행했다.

ALBERT 모델도 BERT와 유사하지만 MLM은 사용하지만 NSP 태스크 대신 문장 순서 예측(SOP) 를 사용한다.



##### 문장 순서 예측



SOP는 이진 분류 형태의 태스크.

NSP 는 문장 순서 예측 태스크는 한 쌍의 문장이 isNext 또는 notNext 인지를 예측하는 형태로 학습이 이루어진다.

하지만 SOP는 주어진 한 쌍의 문장이 문장 순서가 바뀌었는지 여부를 예측하도록 모델 학습이 이루어진다.



A : She cooked pasta

B : It was delicious



문장 B가 문장 A의 뒤에 나오는 걸 알 수 있다. 이를 Positive 로 표시.



A : It was delicious

B : She cooked pasta



Negative 라고 표시.



결국 ,sop는 주어진 한 쌍의 문장이 Positive 인지 Negative인지를 판단하는 분류모델.

정리하면 ALBERT 는 BERT와 다르게 사전 학습 방법으로 MLM, SOP 태스크 사용.



#### ALBERT 와 BERT 비교



ALBERT 가 BERT에 비해서 모든 경우 변수가 적다.

ALBERT 도 사전학습된 모델을 사용할 수 있다.



### ALBERT 에서 임베딩 추출



기존 BERT의 방식과 동일



### RoBERTa



RoBERTa(Robustly Optimized BERT pre-training Approach) 는 BERT의 파생 모델 중 하나다.

RoBERTa 는 기본적으로 BERT와 동일하지만 사전 학습 시 다음의 항목을 변경했다.



- MLM 태스크에서 정적 마스킹이 아닌 동적 마스킹 방법을 적용했다.
- NSP 태스크를 제거하고 MLM 태스크만 학습에 사용했다.
- 배치 크기를 증가해 학습시켰다.
- 토크나이저로 BBPE 를 사용했다.(Byte-level BPE)



#### 정적 마스크 대신 동적 마스크 사용



BERT는 사전 학습을 할 때 마스크된 토큰을 예측하도록 학습한다.

마스킹은 데이터 전처리 단계에서 한 번만 수행되고, 에폭 별로 동일한 마스킹을 예측하도록 모델이 학습.

이게 정적 마스크이다.



RoBERTa 는 동적 마스크를 사용한다.

어떤 문장 A가 있을 때, 그 A문장을 10개로 복사한다.

각각의 문장에 대해서 15% MASKING 을 한다.

그러면 각각의 문장은 서로 다르게 MASKING 되어 있다.

에폭을 40으로 했을 때, 각 에폭별로 다르게 MASKING된 문장이 들어간다.

에폭 1,11,21,31은 문장 1(마스킹된) 이 들어가고

에폭 2, 22,32 는 문장 2가 들어간다. 그러면 에폭별로 다르게 MASKING된 문장이 들어간 것과 같다.



#### NSP 태스크 제거



NSP 중요도를 이해하기 위해서 몇 가지 실험 진행



- SEGMENT-PAIR + NSP : NSP사용해 BERT 학습 (입력 512이하의 토큰 쌍)
- SEMENT-PAIR + NSP : NSP 사용해 BERT 학습 (입력값은 한 문서의 연속된 부분 또는 다른 문서에서 추출한 문장을 쌍으로 구성)
- FULL SENTENCE : NSP 를 사용하지 않고 BERT를 학습 (입력값은 하나 이상의 문서에서 지속적으로 샘플링한 결과, 하나의 문서 마지막까지 샘플링을 한 이후에는 다음 문서에서 샘플링 작업을 이어간다.)
- DOC SENTENCES : NSP를 사용하지 않고 BERT를 학습. FULL SENTENCE 와 전체적으로 유사하나, 입력값은 하나의 문서에서만 샘플링한 결과만 입력. 즉, 하나의 문서 마지막까지 샘플링한 이후 다음 문서 내용을 사용하지 않음.



BERT는 실험 결과

FULL SENTENCE 와 DOC SENTENCE 에서 높은 성능을 보인다.



#### 더 많은 데이터로 학습



BERT에서 사용한 데이터셋 이외에 CC-News, Open WebText 와 Stories(크롤 데이터의 일부)를 추가로 사용했다.



#### 큰 배치 크기로 학습

BERT : 256개 배치로 100만 단계 동안 사전 학습을 진행.

RoBERTa : 8,000개 배치, 30만 단계 (800개 배치로 50만 단계 가능)



배치 크기를 키우면 학습 속도를 늘릴 수 있고 모델 성능 또한 향상시킬 수 있다.



#### BBPE 토크나이저 사용



BERT의 경우 워드 피스 토크나이저를 사용.

워드 피스 방식은 토큰을 결정하는 어휘 사전을 만들 때, 가능도를 기준으로 만들었다.

RoBERTa 의 경우 BBPE 토크나이저를 사용한다.



지금까지 내용을 정리하면, RoBERTa 모델은 BERT 모델과 다르게 MLM 을 통해서만 사전학습을 진행하지 NSP 태스크는 제외한다.

BERT와 다르게 큰 배치 크기를 사용한다 (모델의 성능과 학습 속도를 위해서)

그리고 BERT와 다르게 토크나이저 방식이 BBPE(Byte 기준) 방식이다.



### ELECTRA 이해하기

(다른 파생 모델과 달리 생성기와 판별기를 사용한다.)



사전 학습 방식이 BERT와 다르다.

BERT : MLM, NSP 태스크 진행

ELECTRA : 교체한 토큰 탐지(replaced token detection) 태스크 사용해 학습을 진행.



MLM방식으로 문장의 토큰을 MASKING 하는 방식과 달리 교체한 토큰 탐지는 마스킹 대상인 토큰을 다른 토큰으로 변경한 후 이 토큰이 본래의 토큰인지 교체한 토큰인지 판별하는 형태로 사전 학습을 진행한다.



MLM마스킹의 경우 사전 학습중에는 MLM 토큰을 사용하지만, 파인 튜닝 테스크에서는 MLM토큰을 사용하지 않기 때문에 사전 학습과 파인 튜닝 사이의 불일치 문제가 발생할 수 있다.

토큰을 MASKING으로 태깅하는 게 아니라 토큰간의 교체를 하게 되면 이러한 불일치 문제를 해결할 수 있다.



### 교체한 토큰 판별 태스크 이해하기



A문장의 토큰을 바꾸기전

토큰을 바꾼 후로 두 개를 만든다.

모델을 학습하고 입력 토큰들을 판별자에 넣어서 각 토큰이 바뀐건지 안바뀐건지 출력한다.



토큰을 교체하는 방법은 MLM방식을 사용한다.

먼저 문장을 토큰화 한다. 그리고 MLM 방식 그대로 마스킹 한다.

MASK 태그가 붙여진 상태로 BERT 모델에 집어 넣어서 MASK 태그가 어떤 단어일지 예측을 한다.

그 예측한 값으로 해당 태그 값을 교체한다.

(토큰에 대한 확률 분포를 결과로 제공하기 때문에 BERT 모델을 생성자라고 부른다.)



이제 교체한 토큰을 판별자에 입력해서 주어진 토큰이 본래의 토큰인지 교체된 토큰인지 판별하도록 모델을 학습한다.

여기서 판별자가 ELECTRA 모델이다.



#### ELECTRA의 생성자와 판별자 이해하기



생성자는 MLM태스크를 수행.











### 

