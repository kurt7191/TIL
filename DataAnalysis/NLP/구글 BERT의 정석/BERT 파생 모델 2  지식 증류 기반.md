# BERT 파생 모델 2 : 지식 증류 기반



저번 장에서 사전 학습된 BERT  모델을 사용하는 방법을 살펴봤다.

다운 스트림 태스크에서 사전 학습된 BERT를 파인 튜닝할 수 있었다.



Issue:

다만, 사전 학습된 BERT 는 계산 비용이 많이 들고, **매개변수가 많고** 추론에 시간이 오래 걸린다.



solution :

사전 학습된 대형  BERT (large BERT) 를 소형 BERT(small BERT) 로 지식을 이전하는 **지식 증류**를 사용할 수 있다.

(대형 BERT => 소형 BERT)



- 지식 증류 소개
- DistilBERT : BERT의 증류 버전
- TinyBERT 소개
- BERT 에서 신경망으로 지식 전달



## 지식 증류 소개



- 지식 증류(Knowledge distillation)
  - 대형 모델의 동작을 재현하기 위해서 소형 모델을 학습시키는 모델 압축 기술
  - 교사 - 학생 학습(teacher - student learning)
  - 사전 학습된 대형 모델 : 교사, 소형 모델 : 학생
- 지식 증류 예시
  - 교사 네트워크 : 사전 학습된 대형 모델
  - 사전 학습된 대형 모델을 언어 모델로 사용
  - 입력 문장을 입력하고 다음에 나올 단어를 예측하려고 한다.
    - 다음에 나올 단어는 softmax 가 반환한 확률 분포를 통해서 결정한다.
    - 가장 높은  확률을 나타내는 단어를 다음에 나올 단어로 결정
  - 확률이 가장 높은 단어 이외에도 다른 유용한 정보를 확률 분포에서 뽑아낼 수 없을까?
    - 가장 높은 확률을 기록하는 단어 이외에 상대적으로 높은 확률을 기록하는 단어들을 추출
    - 예를들어서 {0.606 : homework, 0.030 : cake, 0.223 : book, 0.135 : assignment, 0.004 : car} 가 있을 때, 확률이 가장 높은 단어는 homework 이다. 이때, homework 이외에 상대적으로 높은 확률을 보유하고 있는 단어들은 book과 assignment 이다. 이같은 것들은 암흑 지식(dark knowledge) 라고 부른다.
    - 우리는 암흑 단어를 학생 네트워크가 교사 네트워크로부터 배우길 원한다. (교사 네트워크(사전 학습된 대형 모델)가 학생 네트워크에게 암흑 단어를 가르친다.)
  - 성능이 좋은 모델은 정답에 가까운 레이블이 1에 가깝게 출력하고 그렇지 않은 단어들은 0에 가깝게 출력한다.
    - 따라서 암흑 지식을 추출하기 힘들다.
    - 템퍼러처(temperature) 와 소프트맥스(softmax) 함수를 사용 => 소프트맥스 템퍼러처 (출력 레이어에서 사용)
    - 소프트맥스 템프러처는 확률 분포를 평활화 하는데 사용된다.
    - 템퍼러처에 대한 소프트맥스 함수
      - p_i = exp(z_i / T / 시그마exp(z_i / T))
      - 여기서 T가 템퍼러처이다.
      - T = 1로 설정하면 일반 소프트맥스 함수와 같고 T의 크기를 늘리면 더 평활화 하는 것과 같다.(다른 클래스에 대한 더 많은 정보를 제공한다)
  - 암흑 지식을 얻기 위해서 소프트맥스 템프러처로 교사 네트워크를 사전 학습한다.
    - 그리고 지식 증류(교사가 학생에게 지식을 전달)를 통해서 암흑 지식을 학생에게 전달한다.





그렇다면 교사로부터 학생에게 어떻게 암흑 지식이 전달될까?

그리고 학생 네트워크는 어떻게 학습이 될까?



### 학생 네트워크 학습



문장의 다음 단어를 예측하는 사전 학습된 네트워크인 교사 네트워크.

이제 교사로부터 학생에게 지식이 전달되는 방법에 대해서 알아본다.

(교사 네트워크는 소프트맥스 템프러처로 사전 학습 되었고 학생 네트워크는 전혀 학습되지 않았다.)



- 소프트 타깃 : 사전 학습된 교사 네트워크가 입력 문장을 받아들이고 출력한 어휘 단어들의 확률 분포
- 소프트 예측 : 사전 학습되지 않은 학생 네트워크가 입력 문장을 받아들이고 (교사 네트워크가 받아들인 입력값) 출력한 어휘 단어들의 확률 분포



소프트 타깃과 소프트 예측 간의 교차 엔트로피 손실을 계산한다.

이 LOSS값을 최소화하기 위해서 역전파로 학생 네트워크를 학습시킨다.



- 증류 손실 : 소프트 타깃과 소프트 예측간의 교차 엔트로피 손실



여기서 교사 네트워크와 학생 네트워크는 동일학 템프러처(T) 값을 사용하며 1보다 큰 값을 사용했다. (암흑 지식을 추출하기 위해서)

(정리하면 증류 손실을 최소화하는 방향으로 학생 네트워크를 학습시킨다.)



여기서 !!!!

학생 네트워크는 증류 손실 이외에 **학생 손실**이라는 손실을 하나 더 사용한다.

학생 손실을 이해하기 위해서는 "소프트 타깃", "하드 타깃" 을 이해할 필요가 있다.



- 소프트 타깃 : 교사 네트워크에서 출력한 확률 분포를 직접 취하는 것
- 하드 타깃 :  확률이 가장 높은 곳을 1로 설정하고 나머지를 0으로 설정하는 것.



다음으로는 소프트 예측과 하드 예측을 알아볼 것.



- 소프트 예측 : 소프트맥스 템프러처 T가 1보다 큰 학생 네트워크에서 예측한 확률 분포 ( T > 1, 확률 분포)
- 하드 예측 : 소프트맥스 템프러처 T = 1 을 사용해 학생 네트워크에서 예측한 확률 분포 ( T = 1, 확률 분포, 일반적인 소프트맥스 함수 예측)



자 이제 **학생 손실**에 대해서 이야기 할 수 있다.

학생 손실은 기본적으로 하드 타깃과 하드 예측 간의 교차 엔트로피 손실이다.

정리하면, 



- 증류 손실 : 소프트 타깃 - 소프트 예측 (타깃은 교사가, 예측은 학생이)
- 학생 손실 : 하드 타깃 - 하드 예측 (타깃은 교사가, 예측은 학생이)



여기서 최종 손실 함수는 학생 손실과 증류 손실의 가중 합계.



L  = A X (학생 손실) + B X (증류 손실)



A,B는 학생, 증류 손실의 가중 평균을 계산하는데 사용되는 하이퍼파라미터다.

증류를 통해서 교사 네트워크에서 지식을 얻도록  **학생 네트워크를 학습**시키면서 학생 손실과 증류 손실의 가중 합계를 최소화 하는 방향으로도 **학생 네트워크를 학습**시킨다.



## DistilBERT : BERT의 지식 증류 버전



허깅페이스 연구원들은 BERT의 수많은 매개변수 문제 때문에 DistilBERT 를 소개한다. (DistilBERT 는 작고 빠르며 가벼운 BERT 모델)

지식 증류 DistilBERT는 사전 학습된 대형 BERT의 지식을 지식 증류를 통해 지식을 소규모 BERT에 이전하는 작업을 실시한다.

DistilBERT 즉, 소형 BERT 는 대형 BERT에 비해서 60% 빠르고 40% 더 작다.



#### 교사 - 학습 아키텍처



이제 교사 BERT, 학생 BERT 살펴보자.



- 교사 BERT : 사전 학습된 대규모 BERT, BERT-base 모델 사용할 것.



BERT -base 모델은 사전 학습을 MLM 태스크와 NSP 태스크를 통해서 진행한다.



BERT-base 모델은 MLM 작업을 실행하는데, 문장의 특정 부분을 MASKING 한다. 그리고 MASKING 된 입력값을 인코더에 집어넣어서 각 토큰의 표현 벡터를 얻는다. MASK 토큰도 표현 벡터를 얻을텐데, 그 벡터를 피드포워드와 소프트맥스에 집어넣는다. 그러면 어휘 사전의 모든 단어에 대해 마스크된 단어 확률 분포가 나온다. 이 확률 분포 안에는 암흑 지식(Dark Knowledge)이 포함되어 있다. 이 암흑 지식을 학생 BERT에 전달해야만 한다.



#### 학생 BERT



학생 BERT는 Layer 의 개수가 교사 BERT보다 적다.

연구원들은 레이어의 개수가 계산 효율성에 영향을 끼치고, 은닉 상태 차원은 효율성에 큰 영향을 끼치지 않는다고 생각했다.

따라서 레이어의 숫자만 줄이는 것에 집중했다.



즉, 은닉 상태의 차원수는 대형 BERT와 동일하지만 LAYER 의 개수가 더 적다.

그럼 학생 BERT를 어떻게 학습시킬 수 있을까?

이제 교사 BERT로부터 학생 BERT를 학습시키는 방법을 알아보자.



#### 학생 BERT(DistilBERT) 학습



학생 BERT를 학습시키는 Dataset 은 교사 BERT를 학습시켰던 동일한 Dataset 을 사용한다.

RoBERTa 와 마찬가지로 DistilBERT 또한 MLM 태스크만을 이용해서 학습을 진행한다. 추가로 동적 마스킹 방법을 사용하며 더 큰 배치 크기를 사용한다.



1. 입력 문장을 마스킹해서 교사 bert의 입력값으로 집어 넣는다. 동시에 학행 bert의 입력값으로 집어 넣는다.
2. 템프러처 소프트맥스를 통과해서 mask 단어에 대한 어휘 사전의 모든 단어들의 확률이 도출된다.(교사 bert, 학생 bert)
3. 소프트 타깃과 소프트 예측간의 교차 엔트로피 손실로 증류 손실을 계산한다.
4. 동시에 하드 타깃과 하드 예측간의 교차 엔트로피 손실로 학생 손실을 계산한다.
5. 증류 및 학생 손실과는 별도로 "코사인 임베딩 손실"도 계산한다. (교사와 학생 bert가 출력하는 벡터 사이의 거리 측정.)
6. 최종 손실 함수는 증류 손실 + mlm손실(학생 손실) + 코사인 임베딩 손실이다. 즉 세 손실의 합계이다.



위의 최종 손실을 최소화하는 방향으로 DistilBERT를 학습시킬 수 있다.



DistilBERT 는 BERT  -base 모델의 97%정도의 성능을 제공한다.

기존 BERT에 비해서 60% 빠르다.

사전 학습된 DistilBERT 도 제공한다. (허깅페이스)

원래 BERT에서처럼 사전 학습된 DistilBERT를 다운로드 하고 다운스트림 태스크에 맞춰서 파인 튜닝할 수 있다.



여기까지 DistilBERT 가 지식증류를 통해서 교사 BERT로부터 학생 BERT가 지식을 얻는 방법을 배웠다. (세 가지 손실 함수 합의 최소화)



## TinyBERT

#### 

교사의 출력 Layer에서 학생에게 지식을 전달하는 것(템프러처 소프트맥스의 확률분포, 암흑 지식) 이외에 교사 BERT의 다른 레이어로부터 지식을 전달할 수 있다.

TinyBERT 는 임베딩 및 여러 인코더 레이어에서 지식을 전달한다.



본래는 교사 BERT의 출력 레이어에서 생성된 로짓을 가져와서 학생 BERT의 출력에서도 동일한 로짓을 생성하도록 학생 BERT를 학습시켰다.

이것 이외에도 TinyBERT는 교사 BERT가 생성한 은닉 상태 및 어텐션 행렬을 기반으로 학생 BERT도 동일하게 은닉 상태와 어텐션 행렬을 생성하도록 학습시킨다.

또한 교사 BERT의 임베딩 레이어의 출력을 가져와서 학생 BERT의 임베딩 레이어도 동일하게 출력하게끔 학습을 시킨다.



이 이외에도 TinyBERT 는 사전 학습 및 파인 튜닝 단계에서 증류를 적용하는 2단계 학습 프레임워크를 사용한다.



### 교사-학생 아키텍처



#### 교사 BERT 이해하기



사전 학습된 BERT - base 모델을 교사 BERT로 사용.



#### 학생 BERT 이해하기



교사 BERT의 인코더 Layer의 개수는 학생 BERT의 인코더 Layer의 개수보다 크다. (N > M)

4개의 레이어가 있는 학생 BERT를 사용, 표현 벡터 크기를 312, 학생 BERT는 1,450개의 매개변수만 가지고 있다. (표현 벡터의 차원 수도 줄어들었다.)



이제 본격적으로 교사 BERT의 지식이 어떻게 학생 BERT로 이전되는지 살펴보자.



### TinyBERT 지식 증류



DistilBERT가 출력 레이어의 정보만 전달하는 것과 달리 TinyBERT 는 여러 레이어의 정보를 학생 bert에게 전달한다.

따라서 각각의 레이어에서 어떻게 지식을 전달한는지 살펴봐야 한다.



- 트랜스포머 레이어(인코더 레이어)
- 임베딩 레이어(입력 레이어)
- 예측 레이어(출력 레이어)



교사 bert의 각각의 레이어에 index를 부여한다. 임베딩 레이어는 0이고 그 다음 인코더 레이어는 1, 쭈욱 인덱스가 진행하다가 마지막 인코더 레이어는 N 그리고 예측 레이어는 N + 1 이다.



n = g(m)



교사 BERT의 n번째 레이어에서 학생의 m번째  레이어로 지식을 전달한다는 것을 의미한다.

즉, 학생의 m번째 레이어가 교사의 n번째 레이어에서 정보를 학습함을 의미한다.



















