# BERT시작하기 - 트랜스포머 입문

# 트랜스포머 소개



RNN과 LSTM 네트워크는 다음 단어 예측, 기계번역, 텍스트 생성에 사용된다.

그러나 RNN 기반의 모델들은 네트워크 장기 의존선 문제가 있다.

이러한 한계를 극복하고자 "Attention is all you Need" 라는 논문에서 트랜스포머 아키텍처를 제안한다.

트랜스포머 기반으로 BERT, GPT-3, T5 등과 같은 혁명적인 아키텍처가 발전하는 기반이 마련됐다.



트랜스포머는 RNN에서 사용한 순환 방식을 사용하지 않고 **순수하게 어텐션만 사용**한다.

트랜스포머는 **셀프 어텐션**이라는 특수한 형태의 어텐션을 사용한다.

(순수하게 어텐션만 사용, 셀프 어텐션 기술 사용)



트랜스포머의 인코더 디코더의 작동원리는 대략적으로 이해해보자.

먼저 영어를 프랑스어로 바꾸는 작업을 하고 싶다고 하자.

인코더에 영어 시퀀스를 입력값으로 주어 그 결과값을 디코더에 보낸다.

디코더는 인코더가 도출한 결과값을 입력값으로 받아들인다.

그리고 이를 이용해서 프랑스어를 도출한다.

자세한 원리를 이제 알아보도록 하자.



## 트랜스포머 인코더



트랜스포머는 N개의 인코더가 쌓인 형태다.

인코더의 결과값은 그 다음 인코더의 입력값으로 들어간다.

가장 마지막에 있는 인코더의 결과값이 입력값의(입력 시퀀스의) 최종 표현 결과가 된다.

(논문 Attention is all you Need는 인코더를 6개 쌓았다. 6개를 쌓을수도 있지만 2개, 3개 쌓아도 무관하다.)



그러면 인코더의 세부 구성 요소를 살펴보자.



1. 멀티 헤드 어텐션
2. 피드포워드 네트워크



트랜스포머 인코더의 내부 구성은 위와 같이 두 개로 이루어져있다. 입력값이 들어가면 멀티 헤드 어텐션을 거치게 되고 그 다음 피드포워드 네트워크를 거치게 된다.

이제, 두 가지 요소가 어떻게 작동하는지 알아보자. 먼저 멀티 헤드 어텐션이 어떻게 작동하는지 알아보려고 하는데, 멀티 헤드 어텐션이 어떻게 작동하는지 알기 위해서는 "셀프 어텐션"의 작동 원리를 알아야만 한다.

따라서 셀프 어텐션의 작동원리를 살펴보도록 하자.



정리 : 트랜스포머 인코더의 구성요소는 멀티 헤드 어텐션과 피드 포워드 어텐션이며, 각각의 작동 원리를 알아야 인코더의 작동 원리를 이해할 수 있다. 그리고 멀티 헤드 인텐션을 이해하기 위해서는 셀프 어텐션을 알아야만 한다.



### 셀프 어텐션의 작동 원리



"A dog ate the food because it was hungry"



위 문장에서 it에 대해 주목해보도록 하자. it은 dog를 지칭할수 있고, food를 지칭할 수 있다. 우리 인간은 이 문장을 보자마자 it이 dog임을 알아차릴 수 있다. 그러면 모델은 어떻게 it 이 dog이라는 것을 알 수 있을까?

모델이 이를 수행하기 위해서는 셀프 어텐션이 필요하다.



위의 예시 문장이 모델에 들어갔을 때, 모델은 가장 먼저 단어 A에 대한 표현을 계산하고 그 다음 dog 그 다음 ate 표현을 계산한다. 이때 각 단어의 표현은 문장 안에 있는 다른 단어의 표현과 연결해 단어가 문장 내에서 갖는 의미를 이해한다.

위의 예시의 단어들은

A, dog, ate, the, food , because, it, was, hungry 이다.

it 단어를 표현할 때, it이외의 모든 단어 표현들과 it단어를 연결지어서 문장내에서 it이 어떤 의미를 가지고 있는지 이해한다.

즉, it이라는 단어를 이해하기 위해서 it이외의 문장 내의 모든 단어들과 it을 연결짓는 작업을 시행하고 의미를 이해한다. (연결 짓고 it이 어떤 의미를 지니는지 이해한다.)



여기까지가 셀프 어텐션의 큰 개념.

더 작동원리를 자세히 살펴보도록 하자.



**먼저 입력 시퀀스 문장에 대해서 입력 행렬로 만드는 작업을 거친다.**

예를 들어서 I am good 이라는 문장이 있다고 가정하자.

문장 내 각각의 단어들에 대해서 임베딩 작업을 거쳐 벡터로 만들었다고 가정하자. 그러면 위의 문장은 세 개의 단어 벡터가 도출될 것이다. 이를 행순으로 이어 붙이면 행렬을 만들 수 있다.

즉, x1벡터를 0행으로, x2벡터를 1행으로 x3벡터를 2행으로 붙일 수 있다. 그러면 최종적인 입력 행렬의 shape은

(단어의 개수, 임베딩 벡터 차원) 이다.

만일 단어 임베딩 차원이 512 라고 한다면, 이 행렬의 shape은 (3, 512) 다.



**이제 입력행렬을 만들었으면 이 입력행렬로부터 쿼리 (Q) 행렬, 키(K) 행렬, 밸류(V) 행렬을 생성한다.**

(입력행렬 => 쿼리행렬, 키행렬, 밸류행렬, Q,K,V)

이 세 가지 행렬은 셀프 어텐션에 사용된다.



세 개의 Q,K,V 행렬을 만들기 위해서 세 개의 가중치 행렬 W_q, W_k, W_v 을 만든다.

이 가중치 행렬을 행렬 X에 곱해서 3개의 Q,K,V 행렬을 얻는다.

이때 가중치 행렬들은 초기에는 임의의 값을 가지며 학습을 거치면서 최적의 값을 가지게 된다.

최적의 가중치 행렬값을 가지게 되면 더욱더 정확한 Q,K,V 행렬을 만들 수 있다.



이때 Q,K,V 의 행렬은 입력벡터의 행개수와 똑같이 생성된다.

위의 예시로는 3개의 행이 똑같이 생성된다.

Q,K,V 각각의 행은 입력행렬의 각각의 행 벡터 단어들과 일치한다.

즉, Q행렬의 0번째 행은 입력행렬의 0번째 행 단어와 일치한다. 따라서 Q행렬의 0번째 행은 단어 I에 대한 쿼리 벡터다.

마찬가지로 K행렬의 0번째 행은 단어 I에 대한 키 벡터다. V행렬의 0번째 행도 단어 I에 대한 밸류 벡터다.



만일 Q,K,V 행렬의 차원의 수가 64라고 한다면 위의 이유 때문에 Q,K,V의 Shape은  (3,64) 이다.

그렇다면 이렇게 구한 Q,K,V 행렬은 어떻게 사용되는가? 그리고 이와 같은 방법의 장점은 무엇인가?



##### Q,K,V 행렬 사용



앞서, 셀프 어텐션은 문장 내 특정 단어에 대해서 표현하려고 할 때, 그 단어 이외의 문장 내 다른 모든 단어들과 연결하는 과정을 거친다는 것을 살펴봤다.

그 단어가 문장 내 다른 모든 단어들과 어떤 연관을 가지고 있는지 파악하면 그 단어에 대해서 훨씬 더 좋은 표현을 할 수 있기 때문이다.

그렇다면 특정단어와 문장 내 모든 단어들과 연결짓는 방법은 무엇인가?

**이때 Q,K,V 행렬을 사용해서 특정 단어와 문장 내에 있는 모든 단어를 연결할 수 있다.**



- 1단계
  - 쿼리 Q 행렬과 키 K 행렬의 내적 연산을 수행한다.(한 쿼리는 행렬곱을 하기 위해서 transpose 한다.)
  - 내적을 하게 되면 각 단어간의 유사도를 측정할 수 있다.
  - 예를 들어서 (3,4) 형태의 쿼리 행렬과 키 행렬이 있다고 가정해보자. 둘의 곱을 위해서 키 행렬은 전치 해주고 곱하면 다음과 같은 형태일 것이다. (3 X 4) * (4 X 3). 그럼 최종 형태는 3X3의 형태일 것이다. 
  - 쿼리 행렬의 행들은 각 문장 내 단어들을 의미하고, 전치된 키 행렬은 열이 이를 의미한다. 행렬곱의 계산 형식을 살펴보면 쿼리 행렬의 각 행은 키 행렬의 모든 열들과 곱해짐을 알 수 있다. 이 곱이 의미하는건 쿼리 행렬의 단어 벡터 한 개와 키 행렬의 단어 벡터 3개 각각이 곱해진것을 의미한다. 
  - 벡터간 곱은 두 벡터의 유사도를 의미한다. 따라서 쿼리 행렬과 키 행렬의 내적을 진행하면 위의 예시 I am good 에서는 I단어와 (i,am,good) 단어들간의 유사도, am 단어와 (i,am,good) 단어들간의 유사도, good 단어와 (i,am,good) 단어들의 유사도를 측정하는 것과 같다.
  - 결국 어떤 특정 단어가 문장 내의 다른 각각의 모든 단어들과 **얼마나 유사한지 파악**하는데 도움을 준다.(문장 내 다른 모든 단어들과 연결지었다고 볼 수 있다.)
- 2단계
  - 앞서 계산한 쿼리 Q행렬과 키 K행렬의 내적 QK^t 의 키 행렬의 벡터 차원의 제곱근으로 QK^t 행렬을 나눈다.
  - 위의 예에서 키 벡터의 차원은 64 였다. 따라서 키 벡터 차원의 제곱근은 8이다.
  - 위에서 구한 8을 QK^t 행렬에 나눠준다.
  - 이와 같은 방법을 적용하면 안정적인 경삿값(gradient) 를 얻을 수 있다.
- 3단계
  - 2단계의 결과값은 비정규화된 값이다.
  - 따라서 정규화 시켜주기 위해 Softmax 함수를 적용한다.
  - 전체 값의 합은 1이 되고 각각의 원소는 0에서 1사이의 값을 가진다.
  - 이러한 행렬을 스코어 행렬(score matrix) 라고 부른다.
  - 이 %를 활용해서 문장 내의 특정 단어가 다른 단어와 %로 관련있는지 확인할 수 있다.
- 4단계
  - 어텐션 Z 행렬을 계산하는 것.
  - 3단계에서 구한 softmax 를 적용한 행렬에 밸류 행렬 V를 곱하면 어텐션 Z 행렬을 구할 수 있다.
  - 즉 어떤 단어 x의 셀프 어텐션은  밸류 벡터값의 가중치 합으로 계산된다.
  - 여기서 가중치는 3단계에서 구한 soft맥스 행렬이다. 이 행렬에는 각 단어에 대한 가중치 정보가 담겨 있다.



셀프 어텐션은 쿼리와 키 벡터의 내적을 계산한 다음 차원의수 제곱근으로 나누기 때문에 **스케일 닷 프로덕트 어텐션** 이라고 부르기도 한다.

























