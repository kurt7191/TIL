# sentence-BERT 및 domain-BERT 살펴보기



sentence-BERT 는 BERT의 파생 모델로 주로 문장의 표현을 얻는데 사용된다.



- sentence-BERT로 문장 표현에 대해 배우기
- sentence-transformers 라이브러리 탐색
- 지식 증류로 다국어 임베딩 학습
- ClinicalBERT 및 BioBERT 와 같은 domain-BERT 살펴보기



<HR>

## sentence-BERT 로 문장 표현 배우기



sentence-BERT는 고정 길이 문장 표현을 얻는 데 사용된다.

사전 학습된 BERT 또는 파생 모델을 이용한다.

vanila-BERT 또는 파생 모델을 직접 사용해서 문장 표현을 얻을 수 있다.

하지만 vanila-BERT는 유사도를 계산할 때 시간이 많이 걸리는데, 계산 시간이 오래걸리는 단점을 개선하기 위해서 sentence-BERT가 사용될 수 있다.

sentence-BERT는 **문장 쌍 분류** 혹은 **두 문장간의 유사도 계산** 등에 널리 사용된다.



sentence-BERT의 작동 원리를 살펴보기 전에 사전 학습된 BERT를 이용해서 문장 표현을 계산하는 법을 살펴보자.



### 문장 표현 계산



사전 학습된 BERT를 이용해서 문장 표현을 계산해보자.

Paris is a beautiful city 를 먼저 토큰화 한다.



[cls, Paris, is, a, beautiful, city, sep]



앞서 BERT 모델에서 R_cls 토큰이 문장의 표현을 담고 있다고 했었다. 따라서 Paris is a beautiful city 문장의 표현을 R_cls 토큰의 표현으로 정할 수 있다.



issue :

사전 학습된 BERT 모델에서 출력된 어떤 문장의 표현은 BERT 모델이 파인 튜닝되기 전이기 때문에 토큰의 문장 표현이 정확하지 않을 수 있다.



solution : 

따라서 모든 토큰의 표현을 풀링해서 문장 표현을 계산할 수 있다.(평균 풀링, 최대 풀링)



- 평균 풀링을 하게 되면 문장 표현은 본질적으로 모든 단어의 의미를 가진다.
- 최대 풀링을 하게 되면 문장 표현은 본질적으로 중요한 토큰의 의미를 가진다.



사전 학습된 BERT 모델을 통해서 문장의 표현을 얻는 방법들을 살펴봤는데, 다음으로 sentence-BERT 모델이 어떻게 문장의 표현을 얻는지 살펴보자.



### sentence-BERT 이해하기

(파인튜닝에 샴 및 트리플렛 네트워크 아키텍처를 사용)

=> 문장 유사함을 두 개의 레이블로 분류



기본적으로 sentece-BERT 는 사전 학습된 BERT를 택해서 문장 표현을 얻도록 파인 튜닝된 모델이다.

이 모델의 특별한 점은 사전 학습된 모델을 파인 튜닝하기 위해서 **샴 및 트리플렛 네트워크 아키텍처를 사용**해서 더 빠르고 정확한 문장 표현을 얻게 된다.



##### 샴 네트워크 sentence-BERT

##### (문장 쌍 분류 태스크를 위한 sentence-BERT)

문장 쌍 태스크를 위해서 사전 학습된 BERT를 파인 튜닝하기 위해 샴 네트워크 아키텍처를 사용한다.

문장 쌍 분류 태스크는 두 문장A,B 가 있을 때 두 문장이 유사하면 1 레이블을 유사하지 않으면 0 레이블을 반납한다.



예를 들어서 다음과 같은 문장 A,B가 있다고 해보자.



A = I completed my assignment

B = I completed my homework



주어진 문장 쌍에 대해서 각각 토큰화 한다.



A  = [cls,I, completed, my, assignment,sep]

B = [cls,I, completed, my, homework, sep]



사전 학습된 BERT 모델에 A,B를 집어 넣어서 각각 토큰들에 대한 표현을 얻는다.

단, 샴 네트워크는 동일한 가중치를 공유하는 2개의 동일한 네트워크로 구성된다.

따라서 2개의 동일한 사전 학습된 BERT 모델을 사용해서 A,B 문장 각각 따로 표현을 얻는다.

그 과정을 보면 입력값이 BERT모델을 지나쳐서 표현을 얻게되고 그 표현값이 풀링(평균 풀링 혹은 최대 풀링) 을 통해서 문장의 표현을 출력한다.



A,B 두 개의 문장 표현이 나올텐데 이 두 문장 표현을 결합하고 가중치를 곱한 후 소프트맥스 분류기를 넣어서 문장 쌍 분류 태스크에 sentence-BERT를 이용할 수 있다.



u,v 가 두 문장의 표현이라고 한다면, 밑의 식으로 두 문장 표현을 결합한다.



(Wt((u,v,|u-v|)))



그리고 소프트맥스 함수와 분류기를 집어넣어서 두 문장의 유사성이 0인지 1인지 분류한다.



Softmax(Wt((u,v,|u-v|)))



이때 교차 엔트로피 손실(cross-entropy-loss) 를 최소화하도록 Wt를 업데이트해 네트워크를 학습한다.



##### (문장 쌍 회귀 태스크를 위한 sentence-BERT)

=> 문장 간 유사도 수치 측정



문장 쌍 회귀 태스크의 목표는 주어진 두 문장 사이의 의미 유사도를 예측하는 것이다.

위의 문장 쌍 분류 태스크를 위한 sentence-BERT의 과정과 유사하다

동일한 사전학습된 BERT 모델을 사용해서 두 문장에 대한 표현을 각각 얻는다.

각각 문장의 표현을 u,v 라고 할 때, 이 두 문장에 코사인 유사도와 같은 유사도 척도를 사용해 두 문장 간의 유사도를 계산한다.



이때 평균제곱손실(mse) 를 최소화하게끔 모델의 가중치를 업데이트해 주어진 네트워크를 학습한다.

이런 식으로 문장 쌍 회귀 태스크에 sentence-BERT를 사용할 수 있다.



##### 트리플렛 네트워크 sentence-BERT



이제 sentence-BERT가 샴 네트워크 아키텍처를 사용한 게 아니라 트리플렛 네트워크 아키텍처를 사용하는 법을 알아보자.



- 기준 문장 : Play the game
- 긍정 문장 : He is playing the game
- 부정 문장 : Don't play the game



태스크 : 기준 문장과 긍정 문장의 유사도는 높아야 하고, 기준 문장과 부정 문장 사이의 유사도는 낮아야 한다.

문장이 세 개이기 때문에 sentenc-BERT는 트리플렛 네트워크 아키텍처를 사용한다.



먼저 3개의 사전 학습된 BERT를 이용해서 각각의 문장에 대한 표현을 얻는다. (S_p, S_a, S_n) => 각각 긍정, 기준, 부정 문장에 대한 표현을 얻는다.

그리고 세 개의 문장 표현을 이용해서 다음과 같은 트리플렛 목적 함수를 최소화한다.

max(||s_a - s_p|| - ||s_a - s_n|| + 입실론, 0)



|| .|| 는 거리 메트릭을 나타낸다. (유클리디안 거리), 입실론은 마진을 나타낸다.

기준 문장 s_a에서 긍정 문장 표현 s_p는 기준 문장 표현 s_a에서 부정 문장 표현 s_a보다 적어도 입실론 만큼은 가깝다.



트리플렛 손실 함수를 최소화하기 위해서 (긍정, 기준, 부정) 문장의 각각의 거리를 측정하고, 손실함수에 그 값을 집어넣어서 손실 값을 출력한다. 이때 손실 함수의 값이 최소가 되게끔 네트워크를 학습한다.



지금까지 sentence-BERT 의 두 가지 네트워크 (샴 네트워크, 트리플렛 네트워크) 를 살펴봤다.

다음을 sentence-transformers 라이브러리를 탐색하자.



## sentence - transformers 라이브러리 탐색

