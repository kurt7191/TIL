# -*- coding: utf-8 -*-
"""허깅페이스 트랜스포머.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1aUI0Bi162MRyNGHF3y3T_IXj7xPjqjgd
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install transformers==3.5.1

import torch
from transformers import BertModel, BertTokenizer

model = BertModel.from_pretrained('bert-base-uncased')

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

"""# 입력 전처리 하기"""

sentence = 'I love Paris'

tokens = tokenizer.tokenize(sentence)
print(tokens)

tokens = ['[CLS]'] + tokens + ['[SEP]']

print(tokens)

tokens = tokens + ['[PAD]'] + ['[PAD]']
print(tokens)

attention_mask = [1 if i!= '[PAD]' else 0 for i in tokens]

attention_mask

token_ids = tokenizer.convert_tokens_to_ids(tokens)

token_ids

token_ids = torch.tensor(token_ids).unsqueeze(0)
attention_mask = torch.tensor(attention_mask).unsqueeze(0)

"""# 임베딩 추출하기"""

hidden_rep, cls_head = model(token_ids, attention_mask = attention_mask, return_dict = False)

hidden_rep.shape

cls_head.shape



"""# 임베딩 추출하기(여러 인코더 레이어로부터)"""

from transformers import BertModel, BertTokenizer
import torch

model = BertModel.from_pretrained('bert-base-uncased',
                                output_hidden_states = True,)
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

"""## 입력 전처리"""

sentence = 'I love Paris'
tokens = tokenizer.tokenize(sentence)
tokens = ['[CLS]'] + tokens + ['[CLS]']
print(tokens)

tokens += ['[PAD]'] + ['[PAD]']
print(tokens)

tokens_id = tokenizer.convert_tokens_to_ids(tokens)

tokens_id

attention_mask = [1 if i != '[PAD]' else 0 for i in tokens]
print(attention_mask)

token_ids = torch.tensor(token_ids).unsqueeze(0)
attention_mask = torch.tensor(attention_mask).unsqueeze(0)

"""## 임베딩 가져오기"""

last_hidden_state, pooler_output, hidden_states = model(token_ids,
                                                        attention_mask = attention_mask,
                                                        return_dict = False)









