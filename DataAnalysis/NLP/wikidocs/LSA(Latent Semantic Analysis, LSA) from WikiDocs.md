

# LSA(Latent Semantic Analysis, LSA) from WikiDocs



잠재 의미 분석 (Latent Semantic Analysis) 이하 LSA는 LDA 이전의 토픽 모델 중 하나다.

LDA 는 LSA를 보완해서 나온 모델이기 때문에, LDA 를 들어가기 전에 LSA 를 알아두는 게 좋다.



BoW(bag or words) 의 DTM 이나 TF-IDF 는 단어의 빈도수를 기반으로 단어를 수치화 하면 단어의 의미를 고려하지 못한다는 단점이 존재한다.

이를 위한 대안으로 DTM 의 숨어있는(Latent) 단어의 의미를 끄집어 내는 LSA 방법이 존재한다. LSI(Latent Semantic Indexing) 이라고 부르기도 한다.



LSA 를 이해하기 위해서는 선형대수학의 특이값 분해(Singular Value Decomposition, 이하 SVD) 를 이해할 필요가 있다.

(wikidocs 는 svd 의 선형대수학 구체적인 과정을 설명하지 않고 svd 가 갖고 있는 의미를 이해하는데 초점을 두어 설명했다.)



ㄴ> LSA 는 dtm 에서 latent 된 의미를 끄집어내는 기법. for this, 선형대수학 svd 를 알아야 할 필요가 있다.

타이포라

<hr>

## 특이값 분해 (SIngular Value Decomposition ,SVD)



SVD란 A 가 m x n 행렬일 때, 3개의 행렬의 곱으로 분해(decomposition) 하는 것을 말한다.



![스크린샷 2022-03-02 오후 2.09.51](/Users/jang-eunjun/Desktop/TIL/DataAnalysis/NLP/스크린샷 2022-03-02 오후 2.09.51.png)



각 3개의 행렬은 다음과 같은 조건을 만족한다.



![스크린샷 2022-03-02 오후 2.17.29](/Users/jang-eunjun/Desktop/TIL/DataAnalysis/NLP/스크린샷 2022-03-02 오후 2.17.29.png)



> **선형대수  용어 정리**
>
> 1) 전치 행렬(Transposed Matrix)
>    - 원래의 행렬에서 행과 열을 바꾼 행렬
> 2) 단위 행렬(Identify Matrix)
>    - 주대각선의 원소가 모두 1, 나머지 원소는 모두 0인 "정사각" 행렬, 줄여서 대문자 I 로 표현
> 3) 역행렬(Inverse Matrix)
>    - 행렬 A 와 어떤 행렬을 곱했을 때, 그 결과가 단위 행렬(I) 가 나온다면, 어떤 행렬은 A의 역행렬 이라고 부른다.
> 4) 직교 행렬(Orthogonal Matrix)
>    - 실수 n x n 행렬 A에 대해서 , A에 transposed A 를 곱했을 때(A X A^T), 단위 행렬 I 가 나오면서 그 역(A^T X A) 도 단위 행렬 I를 만족하면 행렬 A를 직교 행렬이라고 부른다., 
> 5) 대각 행렬(Diagonal matrix)
>    - 주대각선을 제외한 곳의 원소가 모두 0인 행렬을 말한다. (정사각행렬 뿐만 아니라 직사각 행렬도 대각 행렬이 될 수 있다.)
>    - SVD 를 통해서 나온 대각행렬 ∑의 주대각원소를 행렬 A의 특이값(singular value) 라고 부른다.
>    - 즉 , svd를 통해서 도출된 대각행렬의 주대각원소를 특이값이라고 부른다.
>    - 주대각원소는 내림차순으로 정리되어 있다.



## 절단된 SVD(Truncated SVD)



위에서 설명한(그림) svd 방식은 full svd 라고 부른다.

LSA 의 경우 full svd 에서 나온 3개의 행렬에서 일부 벡터들을 삭제시킨 절단된 svd(truncated svd) 를 사용한다.



![스크린샷 2022-03-02 오후 2.30.12](/Users/jang-eunjun/Desktop/TIL/DataAnalysis/NLP/스크린샷 2022-03-02 오후 2.30.12.png)



절단된 svd 는 대각행렬 ∑에서 상위값 t개만 남게된다.(그림 참조)

절단된 svd는 값의 손실이 일어나기 때문에 A행렬을 복구할 수 없다.

또한  U 행렬과 V 행렬의 t열까지만 남기고 자른다. (그림처럼, V는 transpose 되어 있다는 걸 참조)

여기서 t는 우리가 찾고자 하는 토픽의 수를 반영한 하이퍼파라미터값이다. (t개를 잘라낸다는 것은 중요한 토픽은 남기고 불필요한 토픽을 제거한다는 의미다. )

t를 크게 잡으면 기존의 행렬 A로부터 다양한 의미를 가져갈 수 있다. 하지만 작게 잡아야만 노이즈를 제거할 수 있다.

즉, 크게 잡으면 다양한 의미를 가져갈 수 있는 장점이 있지만 노이즈가 커지고, 작게 잡으면 노이즈는 적어지지면 다양한 의미를 끄집어 낼 수 없다.



여기서 축소된 U의 shape은 (문서의 개수 x 토픽의 수) 를 의미한다. 즉, 4개의 문서 각각은 2개의 차원으로 표현하고 있다.

또한 축소된 V의 shape 은 (토픽의 개수 x 단어의 개수) 를 의미한다. VT의 각 열은 **잠재 의미** 를 표현하기 위해 수치화된 각각의 단어 벡터



이렇게 일부 벡터를 삭제하는 것을 데이터의 차원을 줄인다고 한다.

차원을 줄이면서 얻는 효능은 다음과 같다.



- Full svd보다 직관적으로 비용 계산이 낮아진다.

- 상대적으로 중요하지 않은 정보를 삭제한다.
  - 자연어 처리 분야에서는 **설명력이 낮은 정보를 삭제**하고 **설명력이 높은 정보를 남긴다**는 의미를 가진다. (쓸데없는거 제거하고 괜찮은거 남긴다.)
  - 즉, 쓸모없는걸 제거하고 중요한 것만 남기기 때문에, 기존의 행렬에서는 드러나지 않았던 심층적인 의미를 확인할 수 있게 해준다.



##### 정리

- LSA 를 보완해서 나온게 LDA 다.
- truncated SVD 를 사용하면 중요하지 않은 벡터는 삭제되고 중요한 내용만 남게 되니 기존의 행렬에서는 발견하지 못했던 심층적인 의미를 발견할 수 있다.



## 잠재 의미 분석(Latent Semantic Analysis, LSA)



기존의 DTM, TF-IDF 행렬은 단어의 의미를 전혀 고려하지 못한다는 단점을 가진다.

LSA 는 DTM, TF-IDF 에 절단된 svd 를 사용하여 차원을 축소시키고, 단어들의 잠재적인 의미를 끌어낸다는 아이디어를 가진다.

즉, 기존의 dtm, tf-idf 는 단어의 의미를 잘 담아내지 못하고 있는데, 절단된 svd를 통해서 기존의 행렬에서는 발견하지 못했던 심층적인 의미를 발견한다.



아래와 같은 그림의 DTM 이 존재한다고 할 때, 이 행렬을 svd 를 통해서 행렬 분해한다.



![스크린샷 2022-03-02 오후 2.51.56](/Users/jang-eunjun/Desktop/TIL/DataAnalysis/NLP/스크린샷 2022-03-02 오후 2.51.56.png)



행렬 A를 svd 해서 A_PRIME 행렬을 만들었다면 A행렬과 행렬의 shape 은 동일하지만, A_PRIME 행렬은 불필요한 내용이 제거된 상태로 문서의 잠재의미만 남은 상태다. (문서에 대한 벡터)

















