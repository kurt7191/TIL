# LDA(Latent Dirichlet Allocation) from wiki docs





## LDA



1. 문서들은 토픽들의 혼합으로 구성
2. 토픽들은 확률 분포에 기반하여 단어들을 생성 (토픽  make 단어)



데이터가 주어지면, LDA는 문서가 생성되던 과정을 역추적한다.



<hr>

### LDA 간략한 진행 과정



문서1 : 저는 사과랑 바나나를 먹어요
문서2 : 우리는 귀여운 강아지가 좋아요
문서3 : 저의 깜찍하고 귀여운 강아지가 바나나를 먹어요



=> 토픽의 개수 k개를 사용자가 사전에 지정



**<각 문서의 토픽 분포>**
문서1 : 토픽 A 100%
문서2 : 토픽 B 100%
문서3 : 토픽 B 60%, 토픽 A 40%

**<각 토픽의 단어 분포>**
토픽A : **사과 20%, 바나나 40%, 먹어요 40%**, 귀여운 0%, 강아지 0%, 깜찍하고 0%, 좋아요 0%
토픽B : 사과 0%, 바나나 0%, 먹어요 0%, **귀여운 33%, 강아지 33%, 깜찍하고 16%, 좋아요 16%**



<hr>

## LDA의 가정



문서의 집합으로부터 어떤 토픽이 존재하는지 알아내기 위한 알고리즘.

LDA는 빈도수 기반의 표현 방법인 BOW의 DTM과 TF-IDF 를 사용하기 때문에 단어의 순서는 신경쓰지 않는다.

(bert를 통해서 단어의 순서를 고려한 벡터를 집어넣으면 어떻게 될까?)



LDA는 문서가 작성될 때마다 문서의 작성자가 아래와 같은 생각을 하면서 글을 작성했다고 가정한다.



1. 문서에 사용할 단어의 개수 N을 정한다. 
   - (5개로 정해야징!)
2. 문서에 사용할 토픽의 혼합을 확률 분포에 기반하여 결정한다.
   - 토픽이 2개라고 하면 a토픽은 60 %, b 토픽은 40%
3. 문서에 사용할 각 단어를 아래와 같이 정한다.
   - 토픽 분포에서 토픽 T를 확률적으로 고른다. (a 토픽을 60%의 확률로, b토픽을 40%의 확률로 정한다.)
   - 선택한 토픽 T에서 단어의 출현 확률 분포에 기반해 문서에 사용할 단어를 고른다.
4. 이러한 과정을 통해 문서가 작성되었다는 가정 하에 LDA는 토픽을 뽑아내기 위하여 위 과정을 역으로 추적하는 역공학(reverse engneering) 을 수행한다.



## LDA의 수행하기



1. 사용자는 알고리즘에게 토픽의 개수 k를 알려준다.

   - 토픽의 개수(k)를 정해주는 것은 컴퓨터의 몫이 아니라 사용자의 몫이다.
   - k개의 토픽이 M개의 전체 문서에 걸쳐 분포되어 있다고 가정.

2. 모든 단어를 k개 중 하나의 토픽에 할당한다.

   - 모든 문서의 단어들에 대해서 k개의 토픽중 하나의 토픽에 랜덤으로 할당한다.
   - 이 작업이 끝나면 각 문서는 토픽을 가진다.
   - 토픽은 단어 분포를 가진다.
   - 랜덤으로 할당했기 때문에 이 결과들은 전부 틀린 상황

3. 이제 모든 문서의 모든 단어에 대해서 아래의 사항을 반복 진행한다.

   - 어떤 문서의 각 단어 w는 자신은 잘못된 토픽에 할당되어져 있지만, 다른 단어들은 전부 올바른 토픽에 할당되어져 있는 상태라고 가정.

   - 잘못 할당됐다는 가정하에 w는 아래의 두 가지 기준에 따라 토픽이 재할당.

     - p(topic | document) : 문서 d의 단어들 중 토픽 t에 해당하는 단어들의 비율.

       ![lda3](/Users/jang-eunjun/Desktop/TIL/DataAnalysis/NLP/wikidocs/lda3.png)

     - p(word | topic) : 각 토픽들 t에서 해당 단어 w의 분포

       ![lda2](/Users/jang-eunjun/Desktop/TIL/DataAnalysis/NLP/wikidocs/lda2.png)

       



## 잠재 디리클레 할당과 잠재 의미 분석의 차이



LSA : DTM 차원 축소하여 축소 차원에서 근접 단어들을 토픽으로 묶는다.

LDA : 단어가 특정 토픽에 존재할 확률과 문서에 특정 토픽이 존재할 확률을 결합확률로 추정하여 토픽을 추출한다.

















