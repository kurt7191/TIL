# 할리스 커피 크롤링



```python
from bs4 import BeautifulSoup
import requests
import datetime
result = []
```



크롤링을 위한 import 들을 해준다.



```python
for page in range(1,58):
    url = 'https://www.hollys.co.kr/store/korea/korStore2.do?pageNo=%d&sido=&gugun=&store=' % page
    header = {"User-Agent":"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.114 Safari/537.36"}
    res = requests.get(url, headers = header)
    html = BeautifulSoup(res.text, 'lxml')
    # print(html)
    tag_tbody = html.find('tbody')
    # print(tag_tbody)

    # print(tag_tbody.find_all('tr'))
    for store in tag_tbody.find_all('tr'):
    #     print(store)
        if len(store)<=3 :
            break
        store_td = store.find_all('td')
        store_name = store_td[1].string
        store_sido = store_td[0].string
        store_address = store_td[3].string
        store_phone = store_td[5].string
        result.append([store_name]+[store_sido]+[store_address]+[store_phone])



```



할리스 커피 홈페이지에서 매장 정보 페이지에 들어간다.

406 에러가 뜨기 때문에 user-agent를 복사해서 headers에 넣어준다.



페이지를 확인해보니 tbody에 정보들이 적혀있다. 따라서 tbody를 find해준다.

또 자세히 살펴보면 tbody 안에 tr의 td로 정보가 적혀있다.

따라서 각 tr별 td의 정보를 확인해야 한다.

tr을 for문을 통해서 하나하나 확인한다.

tr안에서 모든 td를 확인하기 위해서 find_all 을 해준다. 

        store_name = store_td[1].string
        store_sido = store_td[0].string
        store_address = store_td[3].string
        store_phone = store_td[5].string



각 td별로 담긴 정보가 다른데 정보의 종류 별로 변수를 설정하여 위와 같이 저장해준다.

*(위는 위 코드의  일부분)*



그리고 result 에 정보를 담는다.



하지만 페이지를 살펴보니 정보가 58페이지까지 존재한다.

따라서 url 부분을 만져주며 pageNo 을 58을 할 수 있게 for문을 돌려준다.

string 이니 % 을 통해서 넣어준다.



### 파이썬 파일 작성하여 크롤링하기



```python
from bs4 import BeautifulSoup
import requests
import pandas as pd
import datetime

```



먼저 필요한 모듈들을 import 해준다.



```python
def hollys_store(result):
    for page in range(1,58):
        url = 'https://www.hollys.co.kr/store/korea/korStore2.do?pageNo=%d&sido=&gugun=&store=' % page
        header = {"User-Agent":"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.114 Safari/537.36"}
        res = requests.get(url, headers = header)
        html = BeautifulSoup(res.text, 'lxml')
        tag_tbody = html.find('tbody')

    for store in tag_tbody.find_all('tr'):
        if len(store)<=3 :
            break
        store_td = store.find_all('td')
        store_name = store_td[1].string
        store_sido = store_td[0].string
        store_address = store_td[3].string
        store_phone = store_td[5].string
        result.append([store_name]+[store_sido]+[store_address]+[store_phone])
        
def main():
    result = []
    hollys_store(result)
    hollys_tbl = pd.DataFrame(result, columns = ['store','gido_gu','address','phone'])
    # 2021-06-30
    hollys_tbl.to_csv('./data_crowling/hollys.csv', encoding = 'cp949', mode = 'w', index = True)
    print('hollys 데이터 저장 완료')
    del result[:]

if __name__ == '__main__':
    main()
    
```



위의 코드들을 함수화 시켜준다.



이 함수를 시행하면 지정된 장소에 csv 파일이 만들어진 것을 확인할 수 있다.

