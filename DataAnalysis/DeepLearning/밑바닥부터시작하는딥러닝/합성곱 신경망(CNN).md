# 합성곱 신경망(CNN)



기존 신경망의 계층 구조(일반적인)

5층을 세워보자.



(Affine -> Relu) => (Affine -> Relu) => (Affine -> Relu) => (Affine -> Relu) => (Affine -> Softmax)  ------>



CNN 신경망의 계층 구조(일반적인)



(Conv -> Relu -> Pooling) => (Conv -> Relu -> Pooling) => (Conv -> Relu) => (Affine -> Relu) -> (Affine-> Softma)



<hr>

## 합성곱 계층



##### 완전연결 계층의 문제점 (Affine 계층이 문제점)



Affine 계층의 문제점은 데이터의 공간적 형상에 담긴 정보를 무시한다는 점이다.

즉, 데이터의 형상이 무시된다는 것.



예를 들어서 MNIIST 데이터는 본래 이미지 하나당 (가로, 세로, 색상) 의 3차원 데이터였다.

하지만 Affine 계층 경우에는 이 데이터를 평탄화 시켜서 1차원으로 축소하여 입력을 받는다.

mnist 가로 세로는 각각 28을 기록하는데, 28 * 28 의 784를 입력의 형상으로 받는다.



이렇게 공간적 형상을 없애고 1차원으로만 데이터를 받으면, 공간에 담긴 정보를 무시할 수 밖에 없다.

픽셀간 가까운 거리는 밀접한 관련이 있다거니와, 픽셀간 거리가 멀으면 전혀 관련이 없는 정보라던가 등등의 정보가 없어지는 것,



> CNN 은 이를 해결



CNN 은 이를 해결한다. 이미지의 입력을 이미지 형상 그대로 받아들인다.

그리고 마찬가지로 이미지 형상 그래도 다음 계층에 데이터를 넘긴다. 이런 이유로 cnn 이 이미지 형상 데이터를 이해해는데 좋은 도구라는 것.



cnn에서는 합성곱 계층의 입출력 데이터를 `특정맵(feature map)` 이라고 칭한다.

합성곱 계층의 입력 데이터를 `입력 특징 맵` , 합성곱 계층의 출력 데이터를 `출력 특징 맵` 이라고 칭한다.



<hr>

## 합성곱 연산



입력 데이터 * 필터(커널)



데이터와 필터간 계산은 단일 곱셈 누산, 데이터와 필터간 대응하는 원소끼리 곱하고 그 값들을 모두 더함.



완전연결 신경망(Affine) 은 가중치 매개변수와 편향이 존재했다.

cnn에서 가중치 매개변수 역할을 하는 건, 필터의 매개변수 값이다.



편향도 존재하는데 편향은 항상 1 * 1의 형식만을 가진다. 만일 3이면 2 * 2의 필터를 거친 데이터 값의 각 원소에 3을 더한다.



##### 패딩



입력데이터 테두리에 특정 값을 채워 넣어서 입력데이터의 크기를 부풀리는 것.



##### 스트라이드



필터를 움직이는 거리를 스트라이드라고 한다. 1이면 한 칸씩 움직이면서 곱을 계산하고, 2면 두 칸씩 움직이면서 곱을 계산한다.



패딩을 하면 출력값은 커졌고, 스트라이드를 하면 출력값은 작아졌다. 이를 수식화 해볼까?



> 입력 크기를 (H,W)
>
> 필터 크기를 (FH, FW)
>
> 출력 크기를 (OH, OW)
>
> 패딩을 P
>
> 스트라이드 S



식은



OH = H + 2P - FH / S + 1

OW =  W + 2P - FW /S + 1



이 계산을 할 때 주의해야할 점은 OH, OW 가 정수로 떨어져야 한다는 점.

만일 출력크기가 정수가 아니면, 반올림을 해준다거나 하는 경우도 있다.



<HR>

## 3차원 데이터의 합성곱 연산



지금까지는 2차원 데이터의 합성곱 연산을 살펴봤다.



채널까지 고려한 3차원 이미지 데이터를 생각해보자



행열 크기의 이미지가 있고 이 이미지들이 채널 방향으로 N개 있을 것.

그러면 필터도 채널의 개수 N개 만큼 있어야 한다.

그리고 각각에 대응하는 데이터와 필터를 곱해주고 다른 채널에서도 똑같이 곱해준다. 그리고 도출된 값들을 위치에 맞게 더해준다.



주의점은 데이터의 채널 수와 필터의 개수가 동일해야 한다는 점.



근데 여기까지 하면 출력 특징맵(출력값) 은 한 개의 맵만 나오게 된다. 이 출력맵으르 여러 개 뽑으려면 어떻게 해야 하나? 바로 필터를 여러 개 주면 된다. 만일 필터를 N개 주개 되면 N개의 출력맵이 나오게 된다.

따라서 필터의 가중치 데이터는 4차원 데이터다.

`출력층 채널 수, 입력 채널 수, 높이 너비` 순이다.

예를 들어서 채널수 3, 크기 5*5 인 필터를 20개 배치하여 출력맵을 20개 출력하고 싶다면

필터 가중치 데이터는 (20, 3,5,5) 이다.



##### 편향 더하기



완전연결 신경망과 같이 CNN 도 편향을 더한다.

이 경우 편향은 출력데이터의 채널수는 동일하고 (1,1) 의 형상을 가진다.

즉 채널의 개수는 동일하면서 1,1 을 가진 (FN,1,1) 을 가진다.

따라서 출력 데이터의 형식 (FN,OH, OW) 에서 모든 대응 채널들의 원소들에 편향이 더해진다.

<HR>

## 배치 처리



완전연결 신경망에서 배치 처리해서 효율을 높였다.

CNN 에서도 배치 처리 할 수 있는데, 흐르는 데이터에 데이터 차원을 늘려주면 가능하다.

본래 3차원 데이터는 (채널, 가로, 세로) 로 되어 있는데, (데이터수, 채널, 가로, 세로) 로 바꿔준다.



필터를 거쳐서 나온 합성곱 맵도 본래는 (FN, OH,OW) 인데 (N, FN,OH,OW) 로 차원을 늘려준다.

출력데이터도 마찬가지



여기서 의미는 신겨망에 4차원 데이터가 하나 흐를 때, 데이터 N개에 대한 합성곱 연산이 한번에 이루어 진다는 의미. 즉 N회분의 처리를 한번에 하는 것.



<HR>

## 풀링 계층



풀링의 크기와 스트라이드 크기는 통일하는게 일반적

예를 들어서 풀링으 크기가 2 * 2 면 스트라이드 크기를 2로 설정하는게 일반적.



풀링 계층은 학습할 것이 없고 해당된 구역에 평균이나 최댓값을 구하는 것뿐임.



풀링의 계산은 또한 채널도 그대로 출력한다. 채널마다 독립적으로 계산하기 때문임.



데이터가 조금 변해도 풀링의 결과는 잘 변하지 않는다.



<HR>



## 층 깊이에 따른 추출 정보 변화





합성곱 층을 여러 개 두었더니 그 출력 값이 점점 추상화 되었다.



## 대표적인 CNN



##### LeNet



합성곱 계층과 풀링 계층을 반복하고 그 출력값을 완전연결 계층과 연결해서 출력



##### AlexNet



활성화 함수로 ReLU 사용

LRN 이라는 구소적 정규화 실시

드롭아웃 사용





















