# 학습 관련 기술들



## SGD



우리가 지금까지 매개변수의 기울기를 활용하여 최적의 매개변수를 도출했다.

지금까지의 방법을 확률적 경사 방법(SGD) 라고 부른다.



SGD는 탐색경로가 비효율적이라는 단점을 가진다.

지그재그로 탐색을 하면서 최적의 기울기로 향하는데 시간이 많이 걸린다는 단점이 있다.



따라서 더 영리한 대체 기술들이 필요하다.



<HR>

## 모멘텀



모멘텀(Momentum) 은 운동량을 뜻하는 단어로 물리와 관련이 있음.

변화를 주려는 방향과 기존의 방향을 고수하려는 힘과 관련된 공식.

기울기를 탐색하는 과정을 그래프로 그리면 공이 그릇에서 굴러가는 그림이 도출된다.



SGD보다 지그재그 움직임이 줄어든다.



<HR>

## AdaGrad



신경망에서 최적의 매개변수 값을 찾아갈 때 중요한 점으느 학습률인데, 너무 커서도 안되고 너무 작아서도 안된다.

AdaGrad 같은 경우, 학습률 조정을 해주는 기능을 가진다. 초반에는 크게 학습하다가 나중에너느 조금씩 작게 학습하는 방식을 취한다.



<hr>

## Adam



모멘텀과 AdaGrad를 합성한 방법을 말한다.





어떤 공식을 사용할지는 문제와 개인의 선택에 달렸다.



<hr>

## 가중치의 초깃값



초반 임의의 가중치 매개변수값을 지정해야 하는데, 이 과정은 매우 매우 중요하다. 초반 가중치 매개변수 값들을 랜덤으로 부여를 하는데, 이때 이 값들의 표준편차가 어떻게 되는지에 따라서 신경층의 활성화값이 다르게 도출된다.



##### 표준편차 1의 가중치 매개변수의 활성화 값 분포



만일 표준편차를 1로 하고 임의의 가중치 매개변수 값을 부여했다고 가정해보자. 그리고 순전파 과정을 거치면서 은닉층의 활성화 값이 어떤 값을 도출하는지 분포를 살펴보면 0과 1에 몰려서 분포해 있음을 알 수 있다.

이때 최적의 가중치를 찾기 위해서 오차역전파를 하면서 그 값을 미분하게 된다면 점점 깊이가 깊어질수록 기울기 값이 작아지다가 사라진다. 즉 `기울기 소실` 문제가 발생



##### 표준편차 0.01의 가중치 매개변수의 활성화 값 분포



표준편차를 0.01 로 하면 활성화 값이 중간에 몰려서 나온다.

이 경우에는 신경망에서 여러 노드를 설정한 이유를 없앤다. 왜냐하면 모든 노드에서 값이 비슷하게 나오기 때문이다.



##### Xavier 초깃값



Xavier 라는 사람이 최적의 가중치 매개변수 최적의 표준편차를 찾으려고 했다.

그는 앞 계층의 노드가 1/n 에 루트를 씌우는 방식으로 표준편차를 정했다.

만일 앞 계층의 노드의 수가 100개면 루트 1/100 이다.



활성화값 분포를 보면 넓게 잘 퍼져있다.



##### He 초깃값



앞서 Xavier 는 활성화 함수가 tanh 이거나 sigmoid 일 때 사용이 적합하다.

현재로서는 ReLU 활성화 함수일시 He 초깃값이 최적화 되어있다.



Xavier 값에 2배의 계수를 해주면 된다. 즉 루트 2/n.



<hr>

## 배치 정규화



앞서 우리는 가중치 매개변수의 초기 임의값들의 분포를 어떻게 설정하냐에  따라서 활성화값 분포가 다르게 나타나고, 각기 다른 이슈가 발생함을 살펴봤다.



그렇다면 각 층이 활성화를 적당히 퍼뜨리도록 강제하면 어떨까? 이 아이디어가 바로 배치 정규화(Batch Normalization).



배치 정규화는 말 그대로 미니 배치를 단위로 정규화 시킨다.

데이터가 평균이 0 분산이 1이 되도록 재배치한다.



배치 정규화를 사용하면 학습의 시간이 단축이 되고, 가중치 초깃값에 크게 좌지우지 되지 않는다.





<hr>

## 오버피팅 방지



## 드랍아웃



dropout(0.2) 와 같이 층을 하나 넣어준다.

노드를 너무 많이 사용하면 오버피팅 되는 현상이 생기는데, 이 노드들을 x% 꺼주고 1 - x% 켜서 학습함을 의미한다.

단, 이 때 시험 때, 각 뉴런의 출력값에 훈련 때 삭제 안한 비율을 곱하여 출력한다.











