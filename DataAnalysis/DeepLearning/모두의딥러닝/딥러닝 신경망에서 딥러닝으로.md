# 신경망의 이해



신경 말단에서 자극 -> 시냅스에서 화학 물질이 나와 전위 변화 -> 전위가 임계 값을 넘으면 다음 뉴런으로 신호를 전달, 아니면 신호 전달 x

신경망을 이루는 가장 작은 단위 `퍼셉트론`



<hr>

## 퍼셉트론과 그 용어



y = ax + b 의 a 는 기울기 b는 절편



딥러닝 언어 =>



> a : 가중치(weight)
>
> b : 바이어스(bias)
>
> 가중합(weighted sum) : 입력값과 가중치의 곱을 모두 더하고 바이어스를 더한 것
>
> 활성화 함수 : 가중합의 결과를 놓고 1또는 0으로 판단하는 것





## 퍼셉트론의 과제 , XOR 문제



앞서 배운 선형 회귀나 로지스틱 함수는 선을 긋는 과정이라고 할 수 있다. 하지만 2차원 평면에서 아무리 선을 그어도 해결할 수 없는 경우가 존재한다.



논리학에서 배운 진리표

AND 진리표, OR 진리표, XOR 진리표 존재

AND 진리표는 하나라도 0이 존재하면 0 출력, 둘 다 1이여야 1출력

OR 진리표는 하나라도 1이 존재하면 1 출력 둘 다 0출력이면 0출력

`XOR 진리표` : 하나만 1이여야 1출력 나머진 0



AND, OR, XOR 을 모두 그래프로 표현하면 XOR 만 선으로 데이터들을 구분할 수 없다.



`XOR 문제`로 인해서 퍼셉트론이 진행되지 못한다.



즉 퍼셉트론 안에 활성화 함수가 있을 때, 로지스틱 회귀나 선형회귀는 새로운 선을 그어서 데이터를 구분하거나 예측하는 것이라고 할 수 있다. 근데 XOR 문제처럼 데이터가 존재할 때, 데이터들을 선을 그어서 구분할 수 없는 문제가 발생한다.



이를 `다층 퍼셉트론` 으로 해결할 수 있다.



<HR>

## 다층 퍼셉트론



XOR문제를 해결하기 위해선 좌표 평면 자체에 변화를 주어야만 한다.

즉, 공간에 왜곡을 주어야만 한다.

좌표 평면에 변화를 주는 방법은 은닉층(hidden layer) 을 만드는 방식이 있다.

즉, 다층 퍼셉트론은 은닉층을 포함한 퍼셉트론을 의미한다.



다층 퍼셉트론으로 인해서 공간에 왜곡을 주면 입력값을 하나의 직선으로 잘 구분할 수 있게 된다.



<hr>

## 다층 퍼셉트론의 설계





입력값이 들어가면 각각의 입력값이 은닉측으로 향하는 가중치 존재,

은닉층의 각각의 은닉노드는 각각의 입력값과 각 가중치가 곱해진 값을 모두 더하고 각 노드의 바이어스를 더한 이후 시그모이드 함수를 거친 결과값을 가진다.

각각의 은닉노드의 결과값이 또다시 가중치를 받고 모두 더해진 다음 바이어스를 더한 값이 최종 값으로 도출된다.



이제 각각의 가중치와 바이어스를 구하는 방법이다(a, b)

-> 앞서 선형회귀와 로지스틱 회귀 경우 경사 하강법을 이용했다.

-> 이 가중치와 바이어스를 구하는 방법은 이후에



이렇게 다층 퍼셉트론을 이용해서 문제를 해결해 가는 과정은 우리의 신경망과 닮았다.

따라서 이를 `신경망` 이라고 부른다.



## 오차 역전파



선형 회귀와, 시그모이드 함수는 가중치와 바이어스 값을 경사 하강법을 이용해서 구했다.

오차 역전파는 경사 하강법의 확장 개념이다.



선형 회귀와 로지스틱 함수의 경사 하강법과 같이 오차를 구한뒤 오차가 최소인 지점으로 기울기와 절편을 이동하는 방법을 사용한다는 점은 동일하다. 

하지만 선형 회귀나 로지스틱 함수는 `단일 퍼셉트론`일 경우였다. 은닉층이 들어온 이상, 계산이 더 어려워졌다.



입력층과 은닉층의 기울기를 거쳐서 만들어진 결과값을 실제값과 비교해 오차를 구하고, 오차에 대해 미분을 했을 때 0이 되는 지점으로 향하게끔 기울기들을 수정을 해나간다. 이 과정은 출력층에서부터 입력층으로 흘러가므로 back propagation 으로 불리는 것





## 신경망에서 딥러닝으로



퍼셉트론 -> XOR 문제 -> 다층 퍼셉트론 -> 오차 역전파 -> 신경망



신경망의 문제 : `기울기 소실`



경사 하강법, 오차 역전파를 통해서 기울기를 수정할 때 필요한건 오차를 미분했을 때 나오는 기울기. 이 기울기가 0이 되게끔 계속 수정하는 것. 

근데 시그모이드 함수는 미분을 할 수록 점점 0에 가까워지는 특성이 있다. 따라서 계속 미분을 하다보면 0이 되어 기울기가 소실되는 문제가 발생한다.

이 때 대체로 사용하는 함수가 RELU 함수,

RELU 함수는 0보다 작으면 0으로 0보다 크면 그 값 그대로 사용하는 함수.



### 속도와 정확도 문제를 해결하는 고급 경사 하강법



경사 하강법은 전체 데이터를 사용하므로 시간이 많이 걸린다는 단점이 존재.



`확률적 경사 하강법` : 

`확률적 경사 하강법` 이 이를 해결

확률적 경가 하강법은 전체 데이터를 사용하지 않고 일부 랜덤한 데이터만을 사용한다.



`모멘텀` :

모멘텀은 가속도라는 의미.

경사 하강법과 마찬가지로 매번 기울기를 구하지만 이전에 구했던 기울기의 수정값과 방향을 고려해서 기울기를 수정하는 방법.

수정방향이 양수 한번 음수 한번으로 이루어져서 지그재그로 진행됐던 경사하강법에서 완화된 지그재그로 변형된다.





