# 신경망 학습



## 데이터에서 학습하는 신경망



여기서 말하는 `학습`  : 가중치의 최적값을 자동으로 찾는 것

손실함수를 통해서 최적의 가중치 값을 찾는다.

손실함수의 값을 가급적 작게 만드는 기법으로 함수의 기울기를 활용하는 경사법 소개.



<hr>

1. 처음부터 데이터가 없이, 알고리즘을 무(無)에서 개발.
2. 머신러닝을 통해서 개발
   - 이미지에서 특징 추출
   - 특징의 패턴을 기계학습으로 학습
   - 이미지 데이터를 벡터로 변환 -> 벡터를 이용해서 svm,knn 가용 가능.
     - 하지만 문제에 따라서 그 특징을 다르게 찾아야 한다. 숫자와 강아지 사진의 경우 다른 특징을 생각해야 한다.
   - 결국 기계학습도 사람의 개입이 들어간다.
3. 신경망(딥러닝)
   - 이미지를 있는 그대로 학습한다.
   - 머신러닝에서는 특징을 사람이 설계했지만, 신경망은 이미지에 포함된 중요한 특징까지도 기계가 스스로 학습.
   - 이러한 의미로 딥러닝을 `종단간 기계학습` 이라고 부름, 처음부터 끝까지 라는 의미



<hr>

## 훈련 데이터와 시험 데이터



훈련데이터와 테스트 데이터로 나뉨

과적합 피하기 위함.

모두의 딥러닝이나 파이썬 머신러닝 완벽 가이드 기록에 그 이유가 있음



<hr>

## 손실 함수



신경망은 하나의 `척도` 를 가지고 최고의 매개변수를 탐구한다.

그 척도는 바로 손실 함수(loss function) 이다.



#### 오차제곱합(Mean Squared Error)



손실 함수의 종류도 여러 개인데, 그 중에 가장 많이 쓰이는 함수는 오차제곱합.



(모두의 딥러닝 참고)



### 교차 엔트로피 오차



또 다른 손실함수로 교차 엔트로피 오차가 존재한다. (cross entropy error)



식은 다음과 같다.



-시그마(tk * loge^yk)

(수식 찾기 프로그램 쓰기 귀찮다 ㅈㅅ)



여기서 tk는 실제 정답값을 의미하고 yk 는 예측값을 의미한다.

k는 데이터의 차원 수를 의미한다.

즉, tk는 정답벡터(10개) 중에서 k번째 값을 의미한다. yk는 예측벡터(10개) 값들 중에서 k번째 값을 의미한다.

이때 정답값은 원핫 코딩으로 이루어져 있기에 값이 1이다. 정답이 아닌 값이 k = 2라고 한다면, t2 = 0이다



따라서 교차 엔트로피 오차 함수는 정답일때의 출력이 전체 값을 정하게 된다.

(출력은 정답만으로 이루어져 있다. (원핫 인코딩 때문에 정답이 아니면 0이기 때문에))



다시 설명하면 예를 들어서

밑에와 같이 

t = [0,0,1,0,0,0,0,0,0,0]

y = [0.1,0.05,0.6,0.0,0.05,0.1,0.0,0.1,0.0,0.0]

이렇게 있을 때 정답을 기준으로 교차 엔트로피 오차 함수에 변수를 대입한다.

위는 인덱스 2가 정답이므로 예측값에서 인덱스 2를 찾는다.

그럼 `-시그마log0.6` 이 오차 값이다. 이 경우는 0.51이다.

그럼 반대로 신경망이 오판을 했을 때의 오차를 보자.

y = [0.1,0.05,0.1,0.0,0.05,0.1,0.0,0.6,0.0,0.0] 이라고 신경망이 예측했으면,

식은 `-시그마log0.1` 이 된다. 이 경우는 2.3이다.



로그 함수의 형태를 보면 x가 0에 가까워질수록 y는 작아지고 x가 1에 가까워질수록 y는 0이된다.

교차 엔트로피도 마찬가지로, 정답에 해당하는 출력이 커질수록 0에 다가가고 출력이 작아질수록 오차는 커진다.



### 미니배치 학습



오늘은 여기까지 다음 기회에.





