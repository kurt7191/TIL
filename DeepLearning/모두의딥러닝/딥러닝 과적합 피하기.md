# 딥러닝 과적합 피하기



EDA



```python
import pandas as pd

df = pd.read_csv('C:\\Users\\user\\Desktop\\080228\\deeplearning\\dataset\\sonar.csv',
                        header = None,
                       )
print(df.info())

df[60].unique()

```



모델 생성 및 학습



```python
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from sklearn.preprocessing import LabelEncoder
import pandas as pd
import numpy as np
import tensorflow as tf

df = pd.read_csv('C:\\Users\\user\\Desktop\\080228\\deeplearning\\dataset\\sonar.csv',
                        header = None,
                       )

dataset = df.values
X = dataset[:,0:60].astype(float)
Y_obj = dataset[:,60]

#문자열 변환

encoder = LabelEncoder()
Y = encoder.fit_transform(Y_obj)

#모델 구축

model = Sequential()
model.add(Dense(24, input_dim = 60, activation =  'relu'))
model.add(Dense(10, activation = 'relu'))
model.add(Dense(1, activation = 'sigmoid'))

#모델 컴파일

model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])

#모델 학습

model.fit(X, Y, epochs = 200, batch_size = 5)

#결과

print('Accuracy : {:.4f}'.format(model.evaluate(X, Y)[1]))
```



Accuracy : 1.000



100 정확도를 자랑한단건데 그럴 리가 없다. 과적합 문제가 발생한 것





##### 과적합 이해하기



과적합은 너무 학습데이터에 맞추어 예측을 했기 때문에 새로운 데이터가 들어오면 제대로 예측할 수 없는 문제를 말함.



딥러닝 층이 너무 많거나, 테스트 데이터 셋과 학습 데이터 셋이 같을 때 생기기도 한다.



학습 데이터 셋과 테스트 데이터 셋이 존재할 때, 학습데이터로 너무 학습하거나 은닉층의 숫자를 늘려서 학습을 한 경우, 학습 데이터 한에서는 정확도가 높게 나오지만 테스트 데이터의 경우 성능이 좋아지다가 나빠지는 현상이 생긴다.



따라서 테스트 데이터의 성능이 최고로 좋아지고 성능이 나빠지기 직전에 끊어야 한다.(은닉층의 수, 학습 데이터 학습 등)



사이킷런으로 학습 데이터와 테스트 데이터를 나눠서 성능을 평가할 수 있다.

이는 머신러닝의 과정과 유사하다.

주의 : 머신러닝은 `predict()` 함수를 이용해서 결과를 pred 변수에 저장하고 그 결과값을 실제 정답인 y_test 와 비교해서 metrics 를 했는데, 여기서는 model.evaluate() 에 X_test 를 넣고 바로 y_test 를 집어넣어서 평가를 진행한 걸 알 수 있다.



```python
df = pd.read_csv('C:\\Users\\user\\Desktop\\080228\\deeplearning\\dataset\\sonar.csv',
                        header = None,
                       )

dataset = df.values
X = dataset[:,0:60].astype(float)
Y_obj = dataset[:,60]

#문자열 변환

encoder = LabelEncoder()
Y = encoder.fit_transform(Y_obj)


from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X,Y, test_size = 0.3, random_state = 0)

#모델 구축

model = Sequential()
model.add(Dense(24, input_dim = 60, activation =  'relu'))
model.add(Dense(10, activation = 'relu'))
model.add(Dense(1, activation = 'sigmoid'))

#모델 컴파일

model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])

#모델 학습

model.fit(X_train, y_train, epochs = 130, batch_size = 5)

#결과

print('Accuracy : {:.4f}'.format(model.evaluate(X_test, y_test)[1]))

```



Accuracy : 0.7937



<hr>

## 번외 모델 저장하기



```python
from tensorflow.keras.models import load_model

model.save('my_model.h5')
model = load_model('my_model.h5')
```







<hr>



